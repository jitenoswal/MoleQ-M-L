{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b2e8667-747b-4f86-b8f7-3d37cd792fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saumya/.conda/envs/penny_skl/lib/python3.12/site-packages/pennylane/__init__.py:209: RuntimeWarning: PennyLane is not yet compatible with JAX versions > 0.6.2. You have version 0.8.0 installed. Please downgrade JAX to 0.6.2 to avoid runtime errors using python -m pip install jax~=0.6.0 jaxlib~=0.6.0\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in Jupyter notebook. Call main() directly with parameters:\n",
      "  results = main(n_runs=2, n_epochs=50, output_dir='lih_results', data_dir='eqnn_force_field_data_LiH')\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Comparison of Graph Equivariant vs Non-Equivariant QML for LiH Energy/Force Prediction\n",
    "\n",
    "This script runs both methods on LiH molecular data and compares their performance.\n",
    "All results are saved to an output directory for later analysis.\n",
    "\n",
    "Methods compared:\n",
    "1. Graph Equivariant QML - Uses SO(3) equivariant encoding with Heisenberg observable\n",
    "2. Non-Equivariant QML - Simple QNN with basic rotations\n",
    "\n",
    "Usage:\n",
    "    python run_comparison.py --n_runs 3 --n_epochs 100 --output_dir results\n",
    "\"\"\"\n",
    "\n",
    "import pennylane as qml\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "\n",
    "import jax\n",
    "jax.config.update('jax_platform_name', 'cpu')\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "from jax import numpy as jnp\n",
    "\n",
    "from jax.example_libraries import optimizers\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.optimize import curve_fit\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =============================================================================\n",
    "# GRAPH EQUIVARIANT QML MODEL\n",
    "# =============================================================================\n",
    "\n",
    "class EquivariantQML:\n",
    "    \"\"\"\n",
    "    Graph Equivariant Quantum Machine Learning model for molecular properties.\n",
    "    Uses SO(3) equivariant encoding with Heisenberg Hamiltonian observable.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_qubits=3, depth=6, blocks=2, seed=42):\n",
    "        self.num_qubits = num_qubits\n",
    "        self.depth = depth\n",
    "        self.blocks = blocks\n",
    "        self.seed = seed\n",
    "        \n",
    "        # Pauli matrices\n",
    "        self.X = np.array([[0, 1], [1, 0]])\n",
    "        self.Y = np.array([[0, -1.0j], [1.0j, 0]])\n",
    "        self.Z = np.array([[1, 0], [0, -1]])\n",
    "        \n",
    "        self.sigmas = jnp.array(np.array([self.X, self.Y, self.Z]))\n",
    "        self.sigmas_sigmas = jnp.array(np.array([\n",
    "            np.kron(self.X, self.X),\n",
    "            np.kron(self.Y, self.Y),\n",
    "            np.kron(self.Z, self.Z)\n",
    "        ]))\n",
    "        \n",
    "        # Create device and circuit\n",
    "        self.dev = qml.device(\"default.qubit\", wires=num_qubits)\n",
    "        self._create_circuit()\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self._init_params()\n",
    "    \n",
    "    def _create_circuit(self):\n",
    "        \"\"\"Create the equivariant quantum circuit.\"\"\"\n",
    "        num_qubits = self.num_qubits\n",
    "        depth = self.depth\n",
    "        blocks = self.blocks\n",
    "        sigmas = self.sigmas\n",
    "        sigmas_sigmas = self.sigmas_sigmas\n",
    "        \n",
    "        # Heisenberg observable\n",
    "        Heisenberg = [\n",
    "            qml.PauliX(0) @ qml.PauliX(1),\n",
    "            qml.PauliY(0) @ qml.PauliY(1),\n",
    "            qml.PauliZ(0) @ qml.PauliZ(1),\n",
    "        ]\n",
    "        self.Observable = qml.Hamiltonian(np.ones((3)), Heisenberg)\n",
    "        \n",
    "        def singlet(wires):\n",
    "            qml.Hadamard(wires=wires[0])\n",
    "            qml.PauliZ(wires=wires[0])\n",
    "            qml.PauliX(wires=wires[1])\n",
    "            qml.CNOT(wires=wires)\n",
    "        \n",
    "        def equivariant_encoding(alpha, data, wires):\n",
    "            hamiltonian = jnp.einsum(\"i,ijk\", data, sigmas)\n",
    "            U = jax.scipy.linalg.expm(-1.0j * alpha * hamiltonian / 2)\n",
    "            qml.QubitUnitary(U, wires=wires, id=\"E\")\n",
    "        \n",
    "        def trainable_layer(weight, wires):\n",
    "            hamiltonian = jnp.einsum(\"ijk->jk\", sigmas_sigmas)\n",
    "            U = jax.scipy.linalg.expm(-1.0j * weight * hamiltonian)\n",
    "            qml.QubitUnitary(U, wires=wires, id=\"U\")\n",
    "        \n",
    "        @qml.qnode(self.dev, interface=\"jax\")\n",
    "        def circuit(data, params):\n",
    "            weights = params[\"params\"][\"weights\"]\n",
    "            alphas = params[\"params\"][\"alphas\"]\n",
    "            \n",
    "            # Initial entangled state\n",
    "            if num_qubits >= 2:\n",
    "                singlet(wires=[0, 1])\n",
    "            if num_qubits >= 3:\n",
    "                qml.CNOT(wires=[1, 2])\n",
    "            \n",
    "            # Initial encoding\n",
    "            for i in range(num_qubits):\n",
    "                equivariant_encoding(alphas[i, 0], jnp.asarray(data, dtype=complex)[i % 1, ...], wires=[i])\n",
    "            \n",
    "            # Layers\n",
    "            for d in range(depth):\n",
    "                qml.Barrier()\n",
    "                for b in range(blocks):\n",
    "                    for i in range(0, num_qubits - 1, 2):\n",
    "                        trainable_layer(weights[i, d + 1, b], wires=[i, (i + 1) % num_qubits])\n",
    "                    for i in range(1, num_qubits, 2):\n",
    "                        trainable_layer(weights[i, d + 1, b], wires=[i, (i + 1) % num_qubits])\n",
    "                \n",
    "                for i in range(num_qubits):\n",
    "                    equivariant_encoding(alphas[i, d + 1], jnp.asarray(data, dtype=complex)[i % 1, ...], wires=[i])\n",
    "            \n",
    "            return qml.expval(self.Observable)\n",
    "        \n",
    "        self.circuit = circuit\n",
    "        self.vec_circuit = jax.vmap(circuit, (0, None), 0)\n",
    "    \n",
    "    def _init_params(self):\n",
    "        \"\"\"Initialize trainable parameters.\"\"\"\n",
    "        np.random.seed(self.seed)\n",
    "        limit = np.sqrt(1.0 / (self.num_qubits * self.depth))\n",
    "        weights = np.random.uniform(-limit, limit, (self.num_qubits, self.depth + 1, self.blocks))\n",
    "        \n",
    "        np.random.seed(self.seed + 1)\n",
    "        alphas = np.random.uniform(0.3, 0.8, (self.num_qubits, self.depth + 1))\n",
    "        \n",
    "        self.params = {\n",
    "            \"params\": {\n",
    "                \"weights\": jnp.array(weights),\n",
    "                \"alphas\": jnp.array(alphas),\n",
    "                \"epsilon\": None\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def get_params(self):\n",
    "        return self.params\n",
    "    \n",
    "    def set_params(self, params):\n",
    "        self.params = params\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# NON-EQUIVARIANT QML MODEL\n",
    "# =============================================================================\n",
    "\n",
    "class NonEquivariantQML:\n",
    "    \"\"\"\n",
    "    Simple non-equivariant QNN for molecular properties.\n",
    "    Uses basic rotations without symmetry preservation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_qubits=4, depth=3, seed=42):\n",
    "        self.num_qubits = num_qubits\n",
    "        self.depth = depth\n",
    "        self.seed = seed\n",
    "        \n",
    "        self.dev = qml.device(\"default.qubit\", wires=num_qubits)\n",
    "        self._create_circuit()\n",
    "        self._init_params()\n",
    "    \n",
    "    def _create_circuit(self):\n",
    "        \"\"\"Create the simple QNN circuit.\"\"\"\n",
    "        num_qubits = self.num_qubits\n",
    "        depth = self.depth\n",
    "        \n",
    "        @qml.qnode(self.dev, interface=\"jax\", diff_method=\"backprop\")\n",
    "        def circuit(positions, params):\n",
    "            weights = params[\"weights\"]\n",
    "            \n",
    "            # Single feature: bond length\n",
    "            dist = jnp.linalg.norm(positions[1] - positions[0])\n",
    "            \n",
    "            # Initialize\n",
    "            for i in range(num_qubits):\n",
    "                qml.RY(0.5, wires=i)\n",
    "            \n",
    "            # Simple layers\n",
    "            for layer in range(depth):\n",
    "                for i in range(num_qubits):\n",
    "                    qml.RY(weights[layer, i, 0] * dist, wires=i)\n",
    "                \n",
    "                for i in range(num_qubits - 1):\n",
    "                    qml.CNOT(wires=[i, i + 1])\n",
    "                \n",
    "                for i in range(num_qubits):\n",
    "                    qml.RZ(weights[layer, i, 1], wires=i)\n",
    "                    qml.RY(weights[layer, i, 2], wires=i)\n",
    "            \n",
    "            return qml.expval(qml.PauliZ(0) + qml.PauliZ(1) + qml.PauliZ(2) + qml.PauliZ(3))\n",
    "        \n",
    "        self.circuit = circuit\n",
    "        self.vec_circuit = jax.vmap(circuit, (0, None), 0)\n",
    "    \n",
    "    def _init_params(self):\n",
    "        \"\"\"Initialize parameters.\"\"\"\n",
    "        np.random.seed(self.seed)\n",
    "        weights = np.random.normal(0, 0.1, (self.depth, self.num_qubits, 3))\n",
    "        self.params = {\"weights\": jnp.array(weights)}\n",
    "    \n",
    "    def get_params(self):\n",
    "        return self.params\n",
    "    \n",
    "    def set_params(self, params):\n",
    "        self.params = params\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# TRAINING FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def train_equivariant(model, data_train, E_train, F_train, data_test, E_test, F_test,\n",
    "                      n_epochs=200, lr=0.01, lambda_E=1.5, lambda_F=2.0):\n",
    "    \"\"\"Train the equivariant model.\"\"\"\n",
    "    \n",
    "    def energy_single(coords, params):\n",
    "        return model.circuit(coords, params)\n",
    "    \n",
    "    def force_single(coords, params):\n",
    "        grad_fn = jax.grad(energy_single, argnums=0)\n",
    "        return -grad_fn(coords, params)\n",
    "    \n",
    "    vec_force = jax.vmap(force_single, (0, None), 0)\n",
    "    \n",
    "    @jax.jit\n",
    "    def mse_loss(predictions, targets):\n",
    "        return jnp.mean((predictions - targets) ** 2)\n",
    "    \n",
    "    @jax.jit\n",
    "    def cost(params, data, E_target, F_target):\n",
    "        E_pred = model.vec_circuit(data, params)\n",
    "        E_loss = mse_loss(E_pred, E_target)\n",
    "        \n",
    "        F_pred = vec_force(data, params)\n",
    "        F_loss = mse_loss(F_pred, F_target)\n",
    "        \n",
    "        total_loss = lambda_E * E_loss + lambda_F * F_loss\n",
    "        return total_loss, (E_loss, F_loss)\n",
    "    \n",
    "    opt_init, opt_update, get_params = optimizers.adam(lr)\n",
    "    opt_state = opt_init(model.params)\n",
    "    \n",
    "    history = {\"epoch\": [], \"train_loss\": [], \"test_E_loss\": [], \"test_F_loss\": []}\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        (loss, (E_loss, F_loss)), grads = jax.value_and_grad(cost, argnums=0, has_aux=True)(\n",
    "            get_params(opt_state), data_train, E_train, F_train\n",
    "        )\n",
    "        opt_state = opt_update(epoch, grads, opt_state)\n",
    "        \n",
    "        if (epoch + 1) % max(1, n_epochs // 20) == 0:\n",
    "            test_params = get_params(opt_state)\n",
    "            E_pred_test = np.array(model.vec_circuit(data_test, test_params))\n",
    "            F_pred_test = np.array(vec_force(data_test, test_params))\n",
    "            \n",
    "            E_test_loss = np.mean((E_pred_test - np.array(E_test)) ** 2)\n",
    "            F_test_loss = np.mean((F_pred_test - np.array(F_test)) ** 2)\n",
    "            \n",
    "            history[\"epoch\"].append(epoch + 1)\n",
    "            history[\"train_loss\"].append(float(loss))\n",
    "            history[\"test_E_loss\"].append(float(E_test_loss))\n",
    "            history[\"test_F_loss\"].append(float(F_test_loss))\n",
    "    \n",
    "    model.set_params(get_params(opt_state))\n",
    "    return history\n",
    "\n",
    "\n",
    "def train_non_equivariant(model, pos_train, E_train, F_train, pos_test, E_test, F_test,\n",
    "                          n_epochs=200, lr=0.01, lambda_E=2.0, lambda_F=1.0):\n",
    "    \"\"\"Train the non-equivariant model.\"\"\"\n",
    "    \n",
    "    def energy_single(coords, params):\n",
    "        return model.circuit(coords, params)\n",
    "    \n",
    "    def force_single(coords, params):\n",
    "        grad_fn = jax.grad(energy_single, argnums=0)\n",
    "        return -grad_fn(coords, params)\n",
    "    \n",
    "    vec_force = jax.vmap(force_single, (0, None), 0)\n",
    "    \n",
    "    @jax.jit\n",
    "    def combined_loss(params, positions, E_target, F_target):\n",
    "        E_pred = model.vec_circuit(positions, params)\n",
    "        E_loss = jnp.mean((E_pred - E_target) ** 2)\n",
    "        \n",
    "        F_pred_full = vec_force(positions, params)\n",
    "        F_pred_z = F_pred_full[:, 1, 2]\n",
    "        F_loss = jnp.mean((F_pred_z - F_target) ** 2)\n",
    "        \n",
    "        E_loss = jnp.where(jnp.isnan(E_loss), 1.0, E_loss)\n",
    "        F_loss = jnp.where(jnp.isnan(F_loss), 1.0, F_loss)\n",
    "        \n",
    "        total_loss = lambda_E * E_loss + lambda_F * F_loss\n",
    "        return total_loss, (E_loss, F_loss)\n",
    "    \n",
    "    opt_init, opt_update, get_params = optimizers.adam(lr)\n",
    "    opt_state = opt_init(model.params)\n",
    "    \n",
    "    history = {\"epoch\": [], \"train_loss\": [], \"test_E_loss\": [], \"test_F_loss\": []}\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        (loss, (E_loss, F_loss)), grads = jax.value_and_grad(combined_loss, has_aux=True)(\n",
    "            get_params(opt_state), pos_train, E_train, F_train\n",
    "        )\n",
    "        \n",
    "        grad_norm = jnp.sqrt(sum(jnp.sum(jnp.square(g)) for g in jax.tree.leaves(grads) if g is not None))\n",
    "        if grad_norm > 10.0:\n",
    "            grads = jax.tree.map(lambda g: g * (10.0 / grad_norm) if g is not None else g, grads)\n",
    "        \n",
    "        opt_state = opt_update(epoch, grads, opt_state)\n",
    "        \n",
    "        if (epoch + 1) % max(1, n_epochs // 20) == 0:\n",
    "            test_params = get_params(opt_state)\n",
    "            E_pred_test = np.array(model.vec_circuit(pos_test, test_params))\n",
    "            F_pred_test = np.array(vec_force(pos_test, test_params))[:, 1, 2]\n",
    "            \n",
    "            E_test_loss = np.mean((E_pred_test - np.array(E_test)) ** 2)\n",
    "            F_test_loss = np.mean((F_pred_test - np.array(F_test)) ** 2)\n",
    "            \n",
    "            history[\"epoch\"].append(epoch + 1)\n",
    "            history[\"train_loss\"].append(float(loss))\n",
    "            history[\"test_E_loss\"].append(float(E_test_loss))\n",
    "            history[\"test_F_loss\"].append(float(F_test_loss))\n",
    "    \n",
    "    model.set_params(get_params(opt_state))\n",
    "    return history\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# DATA LOADING\n",
    "# =============================================================================\n",
    "\n",
    "def load_lih_data(data_dir=\"eqnn_force_field_data_LiH\"):\n",
    "    \"\"\"Load LiH molecular data.\"\"\"\n",
    "    energy = np.load(os.path.join(data_dir, \"Energy.npy\"))\n",
    "    forces = np.load(os.path.join(data_dir, \"Forces.npy\"))\n",
    "    positions = np.load(os.path.join(data_dir, \"Positions.npy\"))\n",
    "    \n",
    "    return energy, forces, positions\n",
    "\n",
    "\n",
    "def prepare_data(energy, forces, positions, test_split=0.2, seed=42):\n",
    "    \"\"\"Prepare and scale data for training.\"\"\"\n",
    "    shape = positions.shape\n",
    "    \n",
    "    # Scale energy\n",
    "    energy_scaler = MinMaxScaler((-1, 1))\n",
    "    if energy.ndim == 1:\n",
    "        energy = energy.reshape(-1, 1)\n",
    "    energy_scaled = energy_scaler.fit_transform(energy).flatten()\n",
    "    \n",
    "    # Center molecule positions\n",
    "    n_atoms_total = positions.shape[1]\n",
    "    positions_centered = np.zeros((shape[0], n_atoms_total - 1, 3))\n",
    "    positions_centered[:, 0, :] = positions[:, 1, :] - positions[:, 0, :]\n",
    "    \n",
    "    # Scale forces (z-component of H atom)\n",
    "    forces_H = forces[:, 1:, :]\n",
    "    force_scaler = MinMaxScaler((-1, 1))\n",
    "    forces_z_only = forces_H[:, 0, 2].reshape(-1, 1)\n",
    "    forces_z_scaled = force_scaler.fit_transform(forces_z_only).flatten()\n",
    "    \n",
    "    forces_scaled = np.zeros_like(forces_H)\n",
    "    forces_scaled[:, 0, 2] = forces_z_scaled\n",
    "    \n",
    "    # Train/test split\n",
    "    np.random.seed(seed)\n",
    "    n_samples = shape[0]\n",
    "    indices_train = np.random.choice(np.arange(n_samples), size=int((1-test_split) * n_samples), replace=False)\n",
    "    indices_test = np.setdiff1d(np.arange(n_samples), indices_train)\n",
    "    \n",
    "    data = {\n",
    "        \"energy_scaler\": energy_scaler,\n",
    "        \"force_scaler\": force_scaler,\n",
    "        \"energy_scaled\": energy_scaled,\n",
    "        \"forces_scaled\": forces_scaled,\n",
    "        \"positions_centered\": positions_centered,\n",
    "        \"positions_raw\": positions,\n",
    "        \"forces_H\": forces_H,\n",
    "        \"indices_train\": indices_train,\n",
    "        \"indices_test\": indices_test,\n",
    "    }\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# EVALUATION\n",
    "# =============================================================================\n",
    "\n",
    "def evaluate_model(model, data, model_type=\"equivariant\"):\n",
    "    \"\"\"Evaluate model and compute metrics.\"\"\"\n",
    "    \n",
    "    positions_centered = data[\"positions_centered\"]\n",
    "    positions_raw = data[\"positions_raw\"]\n",
    "    energy_scaled = data[\"energy_scaled\"]\n",
    "    forces_z_scaled = data[\"forces_scaled\"][:, 0, 2]\n",
    "    energy_scaler = data[\"energy_scaler\"]\n",
    "    force_scaler = data[\"force_scaler\"]\n",
    "    indices_train = data[\"indices_train\"]\n",
    "    indices_test = data[\"indices_test\"]\n",
    "    forces_H = data[\"forces_H\"]\n",
    "    \n",
    "    if model_type == \"equivariant\":\n",
    "        # Get predictions\n",
    "        E_pred_scaled = np.array(model.vec_circuit(jnp.array(positions_centered), model.params))\n",
    "        \n",
    "        def energy_single(coords, params):\n",
    "            return model.circuit(coords, params)\n",
    "        def force_single(coords, params):\n",
    "            return -jax.grad(energy_single, argnums=0)(coords, params)\n",
    "        vec_force = jax.vmap(force_single, (0, None), 0)\n",
    "        \n",
    "        F_pred_scaled = np.array(vec_force(jnp.array(positions_centered), model.params))\n",
    "        F_pred_z_scaled = F_pred_scaled[:, 0, 2]\n",
    "    else:\n",
    "        E_pred_scaled = np.array(model.vec_circuit(jnp.array(positions_raw), model.params))\n",
    "        \n",
    "        def energy_single(coords, params):\n",
    "            return model.circuit(coords, params)\n",
    "        def force_single(coords, params):\n",
    "            return -jax.grad(energy_single, argnums=0)(coords, params)\n",
    "        vec_force = jax.vmap(force_single, (0, None), 0)\n",
    "        \n",
    "        F_pred_all = np.array(vec_force(jnp.array(positions_raw), model.params))\n",
    "        F_pred_z_scaled = F_pred_all[:, 1, 2]\n",
    "    \n",
    "    # Post-correction for energy\n",
    "    def corr_E(E, a, b, c):\n",
    "        return a * E**2 + b * E + c\n",
    "    \n",
    "    try:\n",
    "        popt_E, _ = curve_fit(corr_E, E_pred_scaled[indices_train], energy_scaled[indices_train])\n",
    "        E_pred_corrected = corr_E(E_pred_scaled, *popt_E)\n",
    "    except:\n",
    "        E_pred_corrected = E_pred_scaled\n",
    "    \n",
    "    # Post-correction for force\n",
    "    try:\n",
    "        lr_model = LinearRegression()\n",
    "        lr_model.fit(F_pred_z_scaled[indices_train].reshape(-1, 1), forces_z_scaled[indices_train])\n",
    "        F_pred_corrected = lr_model.predict(F_pred_z_scaled.reshape(-1, 1)).flatten()\n",
    "    except:\n",
    "        F_pred_corrected = F_pred_z_scaled\n",
    "    \n",
    "    # Inverse transform\n",
    "    E_pred_original = energy_scaler.inverse_transform(E_pred_corrected.reshape(-1, 1)).flatten()\n",
    "    F_pred_original = force_scaler.inverse_transform(F_pred_corrected.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    E_true_original = energy_scaler.inverse_transform(energy_scaled.reshape(-1, 1)).flatten()\n",
    "    F_true_original = forces_H[:, 0, 2]\n",
    "    \n",
    "    # Compute metrics on test set\n",
    "    E_mae = np.mean(np.abs(E_pred_original[indices_test] - E_true_original[indices_test]))\n",
    "    E_rmse = np.sqrt(np.mean((E_pred_original[indices_test] - E_true_original[indices_test]) ** 2))\n",
    "    E_r2 = 1 - np.sum((E_pred_original[indices_test] - E_true_original[indices_test])**2) / \\\n",
    "               np.sum((E_true_original[indices_test] - E_true_original[indices_test].mean())**2)\n",
    "    \n",
    "    F_mae = np.mean(np.abs(F_pred_original[indices_test] - F_true_original[indices_test]))\n",
    "    F_rmse = np.sqrt(np.mean((F_pred_original[indices_test] - F_true_original[indices_test]) ** 2))\n",
    "    F_r2 = 1 - np.sum((F_pred_original[indices_test] - F_true_original[indices_test])**2) / \\\n",
    "               np.sum((F_true_original[indices_test] - F_true_original[indices_test].mean())**2)\n",
    "    \n",
    "    metrics = {\n",
    "        \"E_mae_Ha\": float(E_mae),\n",
    "        \"E_mae_eV\": float(E_mae * 27.2114),\n",
    "        \"E_rmse_Ha\": float(E_rmse),\n",
    "        \"E_rmse_eV\": float(E_rmse * 27.2114),\n",
    "        \"E_r2\": float(E_r2),\n",
    "        \"F_mae\": float(F_mae),\n",
    "        \"F_rmse\": float(F_rmse),\n",
    "        \"F_r2\": float(F_r2),\n",
    "    }\n",
    "    \n",
    "    predictions = {\n",
    "        \"E_pred\": E_pred_original.tolist(),\n",
    "        \"E_true\": E_true_original.tolist(),\n",
    "        \"F_pred\": F_pred_original.tolist(),\n",
    "        \"F_true\": F_true_original.tolist(),\n",
    "        \"indices_test\": indices_test.tolist(),\n",
    "    }\n",
    "    \n",
    "    return metrics, predictions\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN COMPARISON\n",
    "# =============================================================================\n",
    "\n",
    "def run_comparison(n_runs=3, n_epochs=100, output_dir=\"lih_comparison_results\", data_dir=\"eqnn_force_field_data_LiH\"):\n",
    "    \"\"\"Run comparison between equivariant and non-equivariant models.\"\"\"\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"LiH Energy/Force Prediction: Equivariant vs Non-Equivariant QML\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Load data\n",
    "    print(\"\\nLoading data...\")\n",
    "    try:\n",
    "        energy, forces, positions = load_lih_data(data_dir)\n",
    "        print(f\"  Loaded {len(energy)} samples\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  ERROR: Data not found in {data_dir}\")\n",
    "        print(\"  Please ensure the LiH data files are available\")\n",
    "        return None\n",
    "    \n",
    "    # Prepare data\n",
    "    data = prepare_data(energy, forces, positions)\n",
    "    print(f\"  Train: {len(data['indices_train'])}, Test: {len(data['indices_test'])}\")\n",
    "    \n",
    "    # Results storage\n",
    "    all_results = {\n",
    "        \"config\": {\n",
    "            \"n_runs\": n_runs,\n",
    "            \"n_epochs\": n_epochs,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "        },\n",
    "        \"equivariant\": {\"runs\": [], \"metrics_summary\": {}},\n",
    "        \"non_equivariant\": {\"runs\": [], \"metrics_summary\": {}},\n",
    "    }\n",
    "    \n",
    "    # Prepare training data\n",
    "    E_train = data[\"energy_scaled\"][data[\"indices_train\"]]\n",
    "    E_test = data[\"energy_scaled\"][data[\"indices_test\"]]\n",
    "    \n",
    "    # For equivariant model\n",
    "    data_train_eq = jnp.array(data[\"positions_centered\"][data[\"indices_train\"]])\n",
    "    data_test_eq = jnp.array(data[\"positions_centered\"][data[\"indices_test\"]])\n",
    "    F_train_eq = data[\"forces_scaled\"][data[\"indices_train\"]]\n",
    "    F_test_eq = data[\"forces_scaled\"][data[\"indices_test\"]]\n",
    "    \n",
    "    # For non-equivariant model\n",
    "    pos_train_neq = jnp.array(data[\"positions_raw\"][data[\"indices_train\"]])\n",
    "    pos_test_neq = jnp.array(data[\"positions_raw\"][data[\"indices_test\"]])\n",
    "    F_train_neq = data[\"forces_scaled\"][data[\"indices_train\"], 0, 2]\n",
    "    F_test_neq = data[\"forces_scaled\"][data[\"indices_test\"], 0, 2]\n",
    "    \n",
    "    # Run experiments\n",
    "    for run in range(n_runs):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"RUN {run+1}/{n_runs}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        run_seed = 42 + run * 100\n",
    "        \n",
    "        # --- Equivariant Model ---\n",
    "        print(f\"\\n[Equivariant Model]\")\n",
    "        eq_model = EquivariantQML(num_qubits=3, depth=6, blocks=2, seed=run_seed)\n",
    "        \n",
    "        print(f\"  Training for {n_epochs} epochs...\")\n",
    "        eq_history = train_equivariant(\n",
    "            eq_model, data_train_eq, E_train, F_train_eq,\n",
    "            data_test_eq, E_test, F_test_eq, n_epochs=n_epochs\n",
    "        )\n",
    "        \n",
    "        print(f\"  Evaluating...\")\n",
    "        eq_metrics, eq_predictions = evaluate_model(eq_model, data, \"equivariant\")\n",
    "        \n",
    "        print(f\"  Energy: MAE={eq_metrics['E_mae_Ha']:.6f} Ha, R²={eq_metrics['E_r2']:.4f}\")\n",
    "        print(f\"  Force:  MAE={eq_metrics['F_mae']:.4f} eV/Å, R²={eq_metrics['F_r2']:.4f}\")\n",
    "        \n",
    "        all_results[\"equivariant\"][\"runs\"].append({\n",
    "            \"run_id\": run,\n",
    "            \"seed\": run_seed,\n",
    "            \"history\": eq_history,\n",
    "            \"metrics\": eq_metrics,\n",
    "            \"predictions\": eq_predictions,\n",
    "        })\n",
    "        \n",
    "        # --- Non-Equivariant Model ---\n",
    "        print(f\"\\n[Non-Equivariant Model]\")\n",
    "        neq_model = NonEquivariantQML(num_qubits=4, depth=3, seed=run_seed)\n",
    "        \n",
    "        print(f\"  Training for {n_epochs} epochs...\")\n",
    "        neq_history = train_non_equivariant(\n",
    "            neq_model, pos_train_neq, E_train, F_train_neq,\n",
    "            pos_test_neq, E_test, F_test_neq, n_epochs=n_epochs\n",
    "        )\n",
    "        \n",
    "        print(f\"  Evaluating...\")\n",
    "        neq_metrics, neq_predictions = evaluate_model(neq_model, data, \"non_equivariant\")\n",
    "        \n",
    "        print(f\"  Energy: MAE={neq_metrics['E_mae_Ha']:.6f} Ha, R²={neq_metrics['E_r2']:.4f}\")\n",
    "        print(f\"  Force:  MAE={neq_metrics['F_mae']:.4f} eV/Å, R²={neq_metrics['F_r2']:.4f}\")\n",
    "        \n",
    "        all_results[\"non_equivariant\"][\"runs\"].append({\n",
    "            \"run_id\": run,\n",
    "            \"seed\": run_seed,\n",
    "            \"history\": neq_history,\n",
    "            \"metrics\": neq_metrics,\n",
    "            \"predictions\": neq_predictions,\n",
    "        })\n",
    "    \n",
    "    # Compute summary statistics\n",
    "    for model_type in [\"equivariant\", \"non_equivariant\"]:\n",
    "        metrics_list = [r[\"metrics\"] for r in all_results[model_type][\"runs\"]]\n",
    "        \n",
    "        summary = {}\n",
    "        for key in metrics_list[0].keys():\n",
    "            values = [m[key] for m in metrics_list]\n",
    "            summary[key] = {\n",
    "                \"mean\": float(np.mean(values)),\n",
    "                \"std\": float(np.std(values)),\n",
    "                \"min\": float(np.min(values)),\n",
    "                \"max\": float(np.max(values)),\n",
    "                \"values\": values,\n",
    "            }\n",
    "        all_results[model_type][\"metrics_summary\"] = summary\n",
    "    \n",
    "    # Save results\n",
    "    results_path = os.path.join(output_dir, \"results.json\")\n",
    "    with open(results_path, 'w') as f:\n",
    "        json.dump(all_results, f, indent=2)\n",
    "    print(f\"\\nResults saved to: {results_path}\")\n",
    "    \n",
    "    # Save numpy arrays for easy loading\n",
    "    np.savez(os.path.join(output_dir, \"metrics.npz\"),\n",
    "             eq_E_r2=[r[\"metrics\"][\"E_r2\"] for r in all_results[\"equivariant\"][\"runs\"]],\n",
    "             eq_F_r2=[r[\"metrics\"][\"F_r2\"] for r in all_results[\"equivariant\"][\"runs\"]],\n",
    "             eq_E_mae=[r[\"metrics\"][\"E_mae_Ha\"] for r in all_results[\"equivariant\"][\"runs\"]],\n",
    "             eq_F_mae=[r[\"metrics\"][\"F_mae\"] for r in all_results[\"equivariant\"][\"runs\"]],\n",
    "             neq_E_r2=[r[\"metrics\"][\"E_r2\"] for r in all_results[\"non_equivariant\"][\"runs\"]],\n",
    "             neq_F_r2=[r[\"metrics\"][\"F_r2\"] for r in all_results[\"non_equivariant\"][\"runs\"]],\n",
    "             neq_E_mae=[r[\"metrics\"][\"E_mae_Ha\"] for r in all_results[\"non_equivariant\"][\"runs\"]],\n",
    "             neq_F_mae=[r[\"metrics\"][\"F_mae\"] for r in all_results[\"non_equivariant\"][\"runs\"]])\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\n{'Metric':<20} {'Equivariant':<25} {'Non-Equivariant':<25}\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    for metric in [\"E_r2\", \"E_mae_Ha\", \"F_r2\", \"F_mae\"]:\n",
    "        eq_mean = all_results[\"equivariant\"][\"metrics_summary\"][metric][\"mean\"]\n",
    "        eq_std = all_results[\"equivariant\"][\"metrics_summary\"][metric][\"std\"]\n",
    "        neq_mean = all_results[\"non_equivariant\"][\"metrics_summary\"][metric][\"mean\"]\n",
    "        neq_std = all_results[\"non_equivariant\"][\"metrics_summary\"][metric][\"std\"]\n",
    "        \n",
    "        print(f\"{metric:<20} {eq_mean:.4f} ± {eq_std:.4f}       {neq_mean:.4f} ± {neq_std:.4f}\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN\n",
    "# =============================================================================\n",
    "\n",
    "def main(n_runs=2, n_epochs=50, output_dir=\"lih_comparison_results\", data_dir=\"eqnn_force_field_data_LiH\"):\n",
    "    \"\"\"\n",
    "    Main function - can be called directly from Jupyter or command line.\n",
    "    \n",
    "    Args:\n",
    "        n_runs: Number of runs for each model\n",
    "        n_epochs: Training epochs per run\n",
    "        output_dir: Directory to save results\n",
    "        data_dir: Directory containing LiH data (.npy files)\n",
    "    \n",
    "    Returns:\n",
    "        results dictionary\n",
    "    \"\"\"\n",
    "    return run_comparison(\n",
    "        n_runs=n_runs,\n",
    "        n_epochs=n_epochs,\n",
    "        output_dir=output_dir,\n",
    "        data_dir=data_dir\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    \n",
    "    # Check if running in Jupyter\n",
    "    if 'ipykernel' in sys.modules:\n",
    "        # Running in Jupyter - use defaults or call main() directly\n",
    "        print(\"Running in Jupyter notebook. Call main() directly with parameters:\")\n",
    "        print(\"  results = main(n_runs=2, n_epochs=50, output_dir='lih_results', data_dir='eqnn_force_field_data_LiH')\")\n",
    "    else:\n",
    "        # Running as script - use argparse\n",
    "        parser = argparse.ArgumentParser(description=\"Compare Equivariant vs Non-Equivariant QML on LiH\")\n",
    "        parser.add_argument(\"--n_runs\", type=int, default=2, help=\"Number of runs\")\n",
    "        parser.add_argument(\"--n_epochs\", type=int, default=50, help=\"Training epochs per run\")\n",
    "        parser.add_argument(\"--output_dir\", type=str, default=\"lih_comparison_results\", help=\"Output directory\")\n",
    "        parser.add_argument(\"--data_dir\", type=str, default=\"eqnn_force_field_data_LiH\", help=\"Data directory\")\n",
    "        \n",
    "        args = parser.parse_args()\n",
    "        \n",
    "        results = main(\n",
    "            n_runs=args.n_runs,\n",
    "            n_epochs=args.n_epochs,\n",
    "            output_dir=args.output_dir,\n",
    "            data_dir=args.data_dir\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "831e9223-4615-4835-83c5-91863217e7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:2025-11-26 00:54:19,575:jax._src.xla_bridge:850: An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LiH Energy/Force Prediction: Equivariant vs Non-Equivariant QML\n",
      "======================================================================\n",
      "\n",
      "Loading data...\n",
      "  Loaded 2400 samples\n",
      "  Train: 1920, Test: 480\n",
      "\n",
      "======================================================================\n",
      "RUN 1/1\n",
      "======================================================================\n",
      "\n",
      "[Equivariant Model]\n",
      "  Training for 200 epochs...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m results = \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_runs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlih_results\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43meqnn_force_field_data_LiH\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 681\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m(n_runs, n_epochs, output_dir, data_dir)\u001b[39m\n\u001b[32m    668\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmain\u001b[39m(n_runs=\u001b[32m2\u001b[39m, n_epochs=\u001b[32m50\u001b[39m, output_dir=\u001b[33m\"\u001b[39m\u001b[33mlih_comparison_results\u001b[39m\u001b[33m\"\u001b[39m, data_dir=\u001b[33m\"\u001b[39m\u001b[33meqnn_force_field_data_LiH\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    669\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    670\u001b[39m \u001b[33;03m    Main function - can be called directly from Jupyter or command line.\u001b[39;00m\n\u001b[32m    671\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    679\u001b[39m \u001b[33;03m        results dictionary\u001b[39;00m\n\u001b[32m    680\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m681\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_comparison\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    682\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_runs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_runs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    683\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    684\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 568\u001b[39m, in \u001b[36mrun_comparison\u001b[39m\u001b[34m(n_runs, n_epochs, output_dir, data_dir)\u001b[39m\n\u001b[32m    565\u001b[39m eq_model = EquivariantQML(num_qubits=\u001b[32m3\u001b[39m, depth=\u001b[32m6\u001b[39m, blocks=\u001b[32m2\u001b[39m, seed=run_seed)\n\u001b[32m    567\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Training for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m epochs...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m568\u001b[39m eq_history = \u001b[43mtrain_equivariant\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    569\u001b[39m \u001b[43m    \u001b[49m\u001b[43meq_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_train_eq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mE_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF_train_eq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    570\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_test_eq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mE_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF_test_eq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_epochs\u001b[49m\n\u001b[32m    571\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Evaluating...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    574\u001b[39m eq_metrics, eq_predictions = evaluate_model(eq_model, data, \u001b[33m\"\u001b[39m\u001b[33mequivariant\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 261\u001b[39m, in \u001b[36mtrain_equivariant\u001b[39m\u001b[34m(model, data_train, E_train, F_train, data_test, E_test, F_test, n_epochs, lr, lambda_E, lambda_F)\u001b[39m\n\u001b[32m    258\u001b[39m history = {\u001b[33m\"\u001b[39m\u001b[33mepoch\u001b[39m\u001b[33m\"\u001b[39m: [], \u001b[33m\"\u001b[39m\u001b[33mtrain_loss\u001b[39m\u001b[33m\"\u001b[39m: [], \u001b[33m\"\u001b[39m\u001b[33mtest_E_loss\u001b[39m\u001b[33m\"\u001b[39m: [], \u001b[33m\"\u001b[39m\u001b[33mtest_F_loss\u001b[39m\u001b[33m\"\u001b[39m: []}\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m     (loss, (E_loss, F_loss)), grads = \u001b[43mjax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalue_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margnums\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mget_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt_state\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mE_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF_train\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    264\u001b[39m     opt_state = opt_update(epoch, grads, opt_state)\n\u001b[32m    266\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (epoch + \u001b[32m1\u001b[39m) % \u001b[38;5;28mmax\u001b[39m(\u001b[32m1\u001b[39m, n_epochs // \u001b[32m20\u001b[39m) == \u001b[32m0\u001b[39m:\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/penny_skl/lib/python3.12/site-packages/jax/_src/api.py:502\u001b[39m, in \u001b[36mvalue_and_grad.<locals>.value_and_grad_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    500\u001b[39m   ans, vjp_py = _vjp(f_partial, *dyn_args)\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m502\u001b[39m   ans, vjp_py, aux = \u001b[43m_vjp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf_partial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mdyn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    503\u001b[39m _check_scalar(ans)\n\u001b[32m    504\u001b[39m tree_map(partial(_check_output_dtype_grad, holomorphic), ans)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/penny_skl/lib/python3.12/site-packages/jax/_src/api.py:2194\u001b[39m, in \u001b[36m_vjp\u001b[39m\u001b[34m(fun, has_aux, *primals)\u001b[39m\n\u001b[32m   2192\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2193\u001b[39m   flat_fun, out_aux_trees = flatten_fun_nokwargs2(fun, in_tree)\n\u001b[32m-> \u001b[39m\u001b[32m2194\u001b[39m   out_primals, vjp, aux = \u001b[43mad\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvjp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflat_fun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprimals_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   2195\u001b[39m   out_tree, aux_tree = out_aux_trees()\n\u001b[32m   2196\u001b[39m out_primal_avals = \u001b[38;5;28mmap\u001b[39m(shaped_abstractify, out_primals)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/penny_skl/lib/python3.12/site-packages/jax/_src/interpreters/ad.py:315\u001b[39m, in \u001b[36mvjp\u001b[39m\u001b[34m(traceable, primals, has_aux)\u001b[39m\n\u001b[32m    313\u001b[39m   out_primals, pvals, jaxpr, consts = linearize(traceable, *primals)\n\u001b[32m    314\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m315\u001b[39m   out_primals, pvals, jaxpr, consts, aux = \u001b[43mlinearize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraceable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mprimals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34munbound_vjp\u001b[39m(pvals, jaxpr, consts, *cts):\n\u001b[32m    318\u001b[39m   cts = \u001b[38;5;28mtuple\u001b[39m(ct \u001b[38;5;28;01mfor\u001b[39;00m ct, pval \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(cts, pvals) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pval.is_known())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/penny_skl/lib/python3.12/site-packages/jax/_src/interpreters/ad.py:287\u001b[39m, in \u001b[36mlinearize\u001b[39m\u001b[34m(traceable, *primals, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m has_aux = kwargs.pop(\u001b[33m'\u001b[39m\u001b[33mhas_aux\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    286\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.use_direct_linearize.value:\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdirect_linearize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraceable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprimals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_aux:\n\u001b[32m    289\u001b[39m   jvpfun = jvp(traceable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/penny_skl/lib/python3.12/site-packages/jax/_src/interpreters/ad.py:255\u001b[39m, in \u001b[36mdirect_linearize\u001b[39m\u001b[34m(traceable, primals, kwargs, has_aux, tag)\u001b[39m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m (core.set_current_trace(linearize_trace),\n\u001b[32m    253\u001b[39m       source_info_util.transform_name_stack(\u001b[33m'\u001b[39m\u001b[33mjvp\u001b[39m\u001b[33m'\u001b[39m)):\n\u001b[32m    254\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m has_aux:\n\u001b[32m--> \u001b[39m\u001b[32m255\u001b[39m     ans, aux = \u001b[43mtraceable\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtracers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m     aux_primals = [x.primal\n\u001b[32m    257\u001b[39m                    \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, LinearizeTracer)\n\u001b[32m    258\u001b[39m                    \u001b[38;5;129;01mand\u001b[39;00m x._trace.tag \u001b[38;5;129;01mis\u001b[39;00m linearize_trace.tag\n\u001b[32m    259\u001b[39m                    \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m aux]\n\u001b[32m    260\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/penny_skl/lib/python3.12/site-packages/jax/_src/linear_util.py:212\u001b[39m, in \u001b[36mWrappedFun.call_wrapped\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_wrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m    211\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls the transformed function\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mf_transformed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/penny_skl/lib/python3.12/site-packages/jax/_src/api_util.py:106\u001b[39m, in \u001b[36mflatten_fun_nokwargs2\u001b[39m\u001b[34m(f, store, in_tree, *args_flat)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;129m@lu\u001b[39m.transformation_with_aux2\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mflatten_fun_nokwargs2\u001b[39m(f, store, in_tree, *args_flat):\n\u001b[32m    105\u001b[39m   py_args = tree_unflatten(in_tree, args_flat)\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m   pair = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpy_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pair, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pair) != \u001b[32m2\u001b[39m:\n\u001b[32m    108\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mexpected function with aux output to return a two-element \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    109\u001b[39m                     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtuple, but got type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m with value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpair\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/penny_skl/lib/python3.12/site-packages/jax/_src/api_util.py:292\u001b[39m, in \u001b[36m_argnums_partial\u001b[39m\u001b[34m(_fun, _dyn_argnums, _fixed_args, *dyn_args, **kwargs)\u001b[39m\n\u001b[32m    290\u001b[39m args = [\u001b[38;5;28mnext\u001b[39m(fixed_args_).val \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m sentinel \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(fixed_args_, sentinel) \u001b[38;5;129;01mis\u001b[39;00m sentinel\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/penny_skl/lib/python3.12/site-packages/jax/_src/linear_util.py:421\u001b[39m, in \u001b[36m_get_result_paths_thunk\u001b[39m\u001b[34m(_fun, _store, *args, **kwargs)\u001b[39m\n\u001b[32m    419\u001b[39m \u001b[38;5;129m@transformation_with_aux2\u001b[39m\n\u001b[32m    420\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_result_paths_thunk\u001b[39m(_fun: Callable, _store: Store, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m421\u001b[39m   ans = \u001b[43m_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    422\u001b[39m   result_paths = \u001b[38;5;28mtuple\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mresult\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_clean_keystr_arg_names(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m path, _ \u001b[38;5;129;01min\u001b[39;00m generate_key_paths(ans))\n\u001b[32m    423\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m _store:\n\u001b[32m    424\u001b[39m     \u001b[38;5;66;03m# In some instances a lu.WrappedFun is called multiple times, e.g.,\u001b[39;00m\n\u001b[32m    425\u001b[39m     \u001b[38;5;66;03m# the bwd function in a custom_vjp\u001b[39;00m\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/penny_skl/lib/python3.12/site-packages/jax/_src/pjit.py:263\u001b[39m, in \u001b[36m_cpp_pjit.<locals>.cache_miss\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    258\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.no_tracing.value:\n\u001b[32m    259\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mre-tracing function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjit_info.fun_sourceinfo\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    260\u001b[39m                      \u001b[33m\"\u001b[39m\u001b[33m`jit`, but \u001b[39m\u001b[33m'\u001b[39m\u001b[33mno_tracing\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is set\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    262\u001b[39m (outs, out_flat, out_tree, args_flat, jaxpr,\n\u001b[32m--> \u001b[39m\u001b[32m263\u001b[39m  executable, pgle_profiler, const_args) = \u001b[43m_python_pjit_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m     \u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjit_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    266\u001b[39m maybe_fastpath_data = _get_fastpath_data(\n\u001b[32m    267\u001b[39m     executable, out_tree, args_flat, out_flat, jaxpr.effects, jaxpr.consts,\n\u001b[32m    268\u001b[39m     jit_info.abstracted_axes, pgle_profiler,\n\u001b[32m    269\u001b[39m     const_args)\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m outs, maybe_fastpath_data, _need_to_rebuild_with_fdo(pgle_profiler)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/penny_skl/lib/python3.12/site-packages/jax/_src/pjit.py:149\u001b[39m, in \u001b[36m_python_pjit_helper\u001b[39m\u001b[34m(fun, jit_info, *args, **kwargs)\u001b[39m\n\u001b[32m    146\u001b[39m   out_flat, compiled, profiler, const_args = _pjit_call_impl_python(\n\u001b[32m    147\u001b[39m       *args_flat, **p.params)\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m   out_flat = \u001b[43mjit_p\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    150\u001b[39m   compiled = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    151\u001b[39m   profiler = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/penny_skl/lib/python3.12/site-packages/jax/_src/core.py:632\u001b[39m, in \u001b[36mPrimitive.bind\u001b[39m\u001b[34m(self, *args, **params)\u001b[39m\n\u001b[32m    630\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **params):\n\u001b[32m    631\u001b[39m   args = args \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.skip_canonicalization \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(canonicalize_value, args)\n\u001b[32m--> \u001b[39m\u001b[32m632\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_true_bind\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/penny_skl/lib/python3.12/site-packages/jax/_src/core.py:648\u001b[39m, in \u001b[36mPrimitive._true_bind\u001b[39m\u001b[34m(self, *args, **params)\u001b[39m\n\u001b[32m    646\u001b[39m trace_ctx.set_trace(eval_trace)\n\u001b[32m    647\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbind_with_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprev_trace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    650\u001b[39m   trace_ctx.set_trace(prev_trace)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/penny_skl/lib/python3.12/site-packages/jax/_src/core.py:660\u001b[39m, in \u001b[36mPrimitive.bind_with_trace\u001b[39m\u001b[34m(self, trace, args, params)\u001b[39m\n\u001b[32m    658\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_current_trace(trace):\n\u001b[32m    659\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.to_lojax(*args, **params)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m660\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrace\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess_primitive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    661\u001b[39m trace.process_primitive(\u001b[38;5;28mself\u001b[39m, args, params)  \u001b[38;5;66;03m# may raise lojax error\u001b[39;00m\n\u001b[32m    662\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt apply typeof to args: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/penny_skl/lib/python3.12/site-packages/jax/_src/interpreters/ad.py:924\u001b[39m, in \u001b[36mLinearizeTrace.process_primitive\u001b[39m\u001b[34m(self, primitive, args, params)\u001b[39m\n\u001b[32m    922\u001b[39m lin = primitive_linearizations.get(primitive, fallback)\n\u001b[32m    923\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m core.set_current_trace(\u001b[38;5;28mself\u001b[39m.parent_trace):\n\u001b[32m--> \u001b[39m\u001b[32m924\u001b[39m   primal_out, tangent_nzs_out, residuals, linearized = \u001b[43mlin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    925\u001b[39m \u001b[43m      \u001b[49m\u001b[43mtangent_nzs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mprimals_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m (core.set_current_trace(\u001b[38;5;28mself\u001b[39m.tangent_trace),\n\u001b[32m    927\u001b[39m       source_info_util.set_name_stack(\u001b[38;5;28mself\u001b[39m._name_stack_suffix())):\n\u001b[32m    928\u001b[39m   tangent_out = linearized(residuals, *tangents_in)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/penny_skl/lib/python3.12/site-packages/jax/_src/pjit.py:2125\u001b[39m, in \u001b[36m_pjit_linearize\u001b[39m\u001b[34m(nzs, jaxpr, in_shardings, out_shardings, in_layouts, out_layouts, donated_invars, ctx_mesh, name, keep_unused, inline, compiler_options_kvs, *primals_in)\u001b[39m\n\u001b[32m   2122\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(x \u001b[38;5;28;01mfor\u001b[39;00m nz, x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(is_nz_l, l) \u001b[38;5;28;01mif\u001b[39;00m nz)\n\u001b[32m   2124\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(in_shardings) == \u001b[38;5;28mlen\u001b[39m(primal_jaxpr.in_avals)\n\u001b[32m-> \u001b[39m\u001b[32m2125\u001b[39m ans = \u001b[43mjit_p\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mprimals_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprimal_jaxpr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2126\u001b[39m \u001b[43m                 \u001b[49m\u001b[43min_shardings\u001b[49m\u001b[43m=\u001b[49m\u001b[43min_shardings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2127\u001b[39m \u001b[43m                 \u001b[49m\u001b[43mout_shardings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprimal_out_shardings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2128\u001b[39m \u001b[43m                 \u001b[49m\u001b[43min_layouts\u001b[49m\u001b[43m=\u001b[49m\u001b[43min_layouts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2129\u001b[39m \u001b[43m                 \u001b[49m\u001b[43mout_layouts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprimal_out_layouts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2130\u001b[39m \u001b[43m                 \u001b[49m\u001b[43mdonated_invars\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdonated_invars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2131\u001b[39m \u001b[43m                 \u001b[49m\u001b[43mctx_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mctx_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2132\u001b[39m \u001b[43m                 \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2133\u001b[39m \u001b[43m                 \u001b[49m\u001b[43mkeep_unused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2134\u001b[39m \u001b[43m                 \u001b[49m\u001b[43minline\u001b[49m\u001b[43m=\u001b[49m\u001b[43minline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2135\u001b[39m \u001b[43m                 \u001b[49m\u001b[43mcompiler_options_kvs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompiler_options_kvs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2136\u001b[39m ans = subs_list(out_fwd, ans, ans)\n\u001b[32m   2137\u001b[39m ans = subs_list(in_fwd, primals_in, ans)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/penny_skl/lib/python3.12/site-packages/jax/_src/core.py:632\u001b[39m, in \u001b[36mPrimitive.bind\u001b[39m\u001b[34m(self, *args, **params)\u001b[39m\n\u001b[32m    630\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **params):\n\u001b[32m    631\u001b[39m   args = args \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.skip_canonicalization \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(canonicalize_value, args)\n\u001b[32m--> \u001b[39m\u001b[32m632\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_true_bind\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/penny_skl/lib/python3.12/site-packages/jax/_src/core.py:648\u001b[39m, in \u001b[36mPrimitive._true_bind\u001b[39m\u001b[34m(self, *args, **params)\u001b[39m\n\u001b[32m    646\u001b[39m trace_ctx.set_trace(eval_trace)\n\u001b[32m    647\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbind_with_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprev_trace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    650\u001b[39m   trace_ctx.set_trace(prev_trace)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/penny_skl/lib/python3.12/site-packages/jax/_src/core.py:660\u001b[39m, in \u001b[36mPrimitive.bind_with_trace\u001b[39m\u001b[34m(self, trace, args, params)\u001b[39m\n\u001b[32m    658\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_current_trace(trace):\n\u001b[32m    659\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.to_lojax(*args, **params)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m660\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrace\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess_primitive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    661\u001b[39m trace.process_primitive(\u001b[38;5;28mself\u001b[39m, args, params)  \u001b[38;5;66;03m# may raise lojax error\u001b[39;00m\n\u001b[32m    662\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt apply typeof to args: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/penny_skl/lib/python3.12/site-packages/jax/_src/core.py:1189\u001b[39m, in \u001b[36mEvalTrace.process_primitive\u001b[39m\u001b[34m(self, primitive, args, params)\u001b[39m\n\u001b[32m   1187\u001b[39m args = \u001b[38;5;28mmap\u001b[39m(full_lower, args)\n\u001b[32m   1188\u001b[39m check_eval_args(args)\n\u001b[32m-> \u001b[39m\u001b[32m1189\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprimitive\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimpl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/penny_skl/lib/python3.12/site-packages/jax/_src/pjit.py:1671\u001b[39m, in \u001b[36m_pjit_call_impl\u001b[39m\u001b[34m(jaxpr, in_shardings, out_shardings, in_layouts, out_layouts, donated_invars, ctx_mesh, name, keep_unused, inline, compiler_options_kvs, *args)\u001b[39m\n\u001b[32m   1663\u001b[39m donated_argnums = \u001b[38;5;28mtuple\u001b[39m(i \u001b[38;5;28;01mfor\u001b[39;00m i, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(donated_invars) \u001b[38;5;28;01mif\u001b[39;00m d)\n\u001b[32m   1664\u001b[39m cache_key = pxla.JitGlobalCppCacheKeys(\n\u001b[32m   1665\u001b[39m     donate_argnums=donated_argnums, donate_argnames=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1666\u001b[39m     device=\u001b[38;5;28;01mNone\u001b[39;00m, backend=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1669\u001b[39m     in_layouts_treedef=\u001b[38;5;28;01mNone\u001b[39;00m, in_layouts_leaves=in_layouts,\n\u001b[32m   1670\u001b[39m     out_layouts_treedef=\u001b[38;5;28;01mNone\u001b[39;00m, out_layouts_leaves=out_layouts)\n\u001b[32m-> \u001b[39m\u001b[32m1671\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mxc\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_xla\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpjit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1672\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall_impl_cache_miss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1673\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtree_util\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdispatch_registry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpxla\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcc_shard_arg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1674\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_get_cpp_global_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcache_key\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontains_explicit_attributes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/penny_skl/lib/python3.12/site-packages/jax/_src/pjit.py:1647\u001b[39m, in \u001b[36m_pjit_call_impl.<locals>.call_impl_cache_miss\u001b[39m\u001b[34m(*args_, **kwargs_)\u001b[39m\n\u001b[32m   1643\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_impl_cache_miss\u001b[39m(*args_, **kwargs_):\n\u001b[32m   1644\u001b[39m   \u001b[38;5;66;03m# args_ do not include the const args\u001b[39;00m\n\u001b[32m   1645\u001b[39m   \u001b[38;5;66;03m# See https://docs.jax.dev/en/latest/internals/constants.html.\u001b[39;00m\n\u001b[32m   1646\u001b[39m   \u001b[38;5;66;03m# TODO(necula): remove num_const_args when fixing the C++ path\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1647\u001b[39m   out_flat, compiled, pgle_profiler, const_args = \u001b[43m_pjit_call_impl_python\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1648\u001b[39m \u001b[43m      \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_shardings\u001b[49m\u001b[43m=\u001b[49m\u001b[43min_shardings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1649\u001b[39m \u001b[43m      \u001b[49m\u001b[43mout_shardings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout_shardings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_layouts\u001b[49m\u001b[43m=\u001b[49m\u001b[43min_layouts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1650\u001b[39m \u001b[43m      \u001b[49m\u001b[43mout_layouts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout_layouts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdonated_invars\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdonated_invars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1651\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mctx_mesh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_unused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1652\u001b[39m \u001b[43m      \u001b[49m\u001b[43minline\u001b[49m\u001b[43m=\u001b[49m\u001b[43minline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompiler_options_kvs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompiler_options_kvs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1653\u001b[39m   fastpath_data = _get_fastpath_data(\n\u001b[32m   1654\u001b[39m       compiled, tree_structure(out_flat), args, out_flat,\n\u001b[32m   1655\u001b[39m       jaxpr.effects, jaxpr.consts, \u001b[38;5;28;01mNone\u001b[39;00m, pgle_profiler,\n\u001b[32m   1656\u001b[39m       const_args)\n\u001b[32m   1657\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m out_flat, fastpath_data, _need_to_rebuild_with_fdo(pgle_profiler)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/penny_skl/lib/python3.12/site-packages/jax/_src/pjit.py:1600\u001b[39m, in \u001b[36m_pjit_call_impl_python\u001b[39m\u001b[34m(jaxpr, in_shardings, out_shardings, in_layouts, out_layouts, donated_invars, ctx_mesh, name, keep_unused, inline, compiler_options_kvs, *args)\u001b[39m\n\u001b[32m   1588\u001b[39m \u001b[38;5;66;03m# Passing mutable PGLE profile here since it should be extracted by JAXPR to\u001b[39;00m\n\u001b[32m   1589\u001b[39m \u001b[38;5;66;03m# initialize the fdo_profile compile option.\u001b[39;00m\n\u001b[32m   1590\u001b[39m computation = _resolve_and_lower(\n\u001b[32m   1591\u001b[39m     args, jaxpr=jaxpr, in_shardings=in_shardings,\n\u001b[32m   1592\u001b[39m     out_shardings=out_shardings, in_layouts=in_layouts,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1598\u001b[39m     compiler_options_kvs=compiler_options_kvs,\n\u001b[32m   1599\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1600\u001b[39m compiled = \u001b[43mcomputation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1602\u001b[39m \u001b[38;5;66;03m# This check is expensive so only do it if enable_checks is on.\u001b[39;00m\n\u001b[32m   1603\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m compiled._auto_spmd_lowering \u001b[38;5;129;01mand\u001b[39;00m config.enable_checks.value:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/penny_skl/lib/python3.12/site-packages/jax/_src/interpreters/pxla.py:2527\u001b[39m, in \u001b[36mMeshComputation.compile\u001b[39m\u001b[34m(self, compiler_options, device_assignment)\u001b[39m\n\u001b[32m   2524\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(compilation_device_list, (\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m), xc.DeviceList))\n\u001b[32m   2526\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._executable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m compiler_options_kvs \u001b[38;5;129;01mor\u001b[39;00m device_assignment:\n\u001b[32m-> \u001b[39m\u001b[32m2527\u001b[39m   executable = \u001b[43mUnloadedMeshExecutable\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_hlo\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2528\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_hlo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompile_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2529\u001b[39m \u001b[43m      \u001b[49m\u001b[43mcompiler_options_kvs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompiler_options_kvs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2530\u001b[39m \u001b[43m      \u001b[49m\u001b[43mdevice_list\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompilation_device_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2531\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m compiler_options_kvs:\n\u001b[32m   2532\u001b[39m     \u001b[38;5;28mself\u001b[39m._executable = executable\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/penny_skl/lib/python3.12/site-packages/jax/_src/interpreters/pxla.py:3073\u001b[39m, in \u001b[36mUnloadedMeshExecutable.from_hlo\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   3070\u001b[39m       \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   3072\u001b[39m util.test_event(\u001b[33m\"\u001b[39m\u001b[33mpxla_cached_compilation\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m3073\u001b[39m xla_executable = \u001b[43m_cached_compilation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3074\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhlo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmesh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspmd_lowering\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtuple_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauto_spmd_lowering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_prop_to_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3076\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_prop_to_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhost_callbacks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3077\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpmap_nreps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompiler_options_kvs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpgle_profiler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3079\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m auto_spmd_lowering:\n\u001b[32m   3080\u001b[39m   \u001b[38;5;28;01massert\u001b[39;00m mesh \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/penny_skl/lib/python3.12/site-packages/jax/_src/interpreters/pxla.py:2854\u001b[39m, in \u001b[36m_cached_compilation\u001b[39m\u001b[34m(computation, name, mesh, spmd_lowering, tuple_args, auto_spmd_lowering, allow_prop_to_inputs, allow_prop_to_outputs, host_callbacks, backend, da, pmap_nreps, compiler_options_kvs, pgle_profiler)\u001b[39m\n\u001b[32m   2846\u001b[39m compile_options = create_compile_options(\n\u001b[32m   2847\u001b[39m     computation, mesh, spmd_lowering, tuple_args, auto_spmd_lowering,\n\u001b[32m   2848\u001b[39m     allow_prop_to_inputs, allow_prop_to_outputs, backend,\n\u001b[32m   2849\u001b[39m     dev, pmap_nreps, compiler_options)\n\u001b[32m   2851\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m dispatch.log_elapsed_time(\n\u001b[32m   2852\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mFinished XLA compilation of \u001b[39m\u001b[38;5;132;01m{fun_name}\u001b[39;00m\u001b[33m in \u001b[39m\u001b[38;5;132;01m{elapsed_time:.9f}\u001b[39;00m\u001b[33m sec\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2853\u001b[39m     fun_name=name, event=dispatch.BACKEND_COMPILE_EVENT):\n\u001b[32m-> \u001b[39m\u001b[32m2854\u001b[39m   xla_executable = \u001b[43mcompiler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile_or_get_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2855\u001b[39m \u001b[43m      \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomputation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhost_callbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2856\u001b[39m \u001b[43m      \u001b[49m\u001b[43mda\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpgle_profiler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2857\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m xla_executable\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/penny_skl/lib/python3.12/site-packages/jax/_src/compiler.py:491\u001b[39m, in \u001b[36mcompile_or_get_cached\u001b[39m\u001b[34m(backend, computation, devices, compile_options, host_callbacks, executable_devices, pgle_profiler)\u001b[39m\n\u001b[32m    489\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    490\u001b[39m   log_persistent_cache_miss(module_name, cache_key)\n\u001b[32m--> \u001b[39m\u001b[32m491\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile_and_write_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m      \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m      \u001b[49m\u001b[43mcomputation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m      \u001b[49m\u001b[43mexecutable_devices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m      \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m      \u001b[49m\u001b[43mhost_callbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m      \u001b[49m\u001b[43mcache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/penny_skl/lib/python3.12/site-packages/jax/_src/compiler.py:759\u001b[39m, in \u001b[36m_compile_and_write_cache\u001b[39m\u001b[34m(backend, computation, executable_devices, compile_options, host_callbacks, module_name, cache_key)\u001b[39m\n\u001b[32m    749\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_compile_and_write_cache\u001b[39m(\n\u001b[32m    750\u001b[39m     backend: xc.Client,\n\u001b[32m    751\u001b[39m     computation: ir.Module,\n\u001b[32m   (...)\u001b[39m\u001b[32m    756\u001b[39m     cache_key: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    757\u001b[39m ) -> xc.LoadedExecutable:\n\u001b[32m    758\u001b[39m   start_time = time.monotonic()\n\u001b[32m--> \u001b[39m\u001b[32m759\u001b[39m   executable = \u001b[43mbackend_compile_and_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    760\u001b[39m \u001b[43m      \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomputation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable_devices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhost_callbacks\u001b[49m\n\u001b[32m    761\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    762\u001b[39m   compile_time = time.monotonic() - start_time\n\u001b[32m    763\u001b[39m   _cache_write(\n\u001b[32m    764\u001b[39m       cache_key, compile_time, module_name, backend, executable, host_callbacks\n\u001b[32m    765\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/penny_skl/lib/python3.12/site-packages/jax/_src/profiler.py:359\u001b[39m, in \u001b[36mannotate_function.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    356\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    358\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, **decorator_kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/penny_skl/lib/python3.12/site-packages/jax/_src/compiler.py:375\u001b[39m, in \u001b[36mbackend_compile_and_load\u001b[39m\u001b[34m(backend, module, executable_devices, options, host_callbacks)\u001b[39m\n\u001b[32m    366\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m backend.compile_and_load(\n\u001b[32m    367\u001b[39m           built_c,\n\u001b[32m    368\u001b[39m           executable_devices=executable_devices,\n\u001b[32m    369\u001b[39m           compile_options=options,\n\u001b[32m    370\u001b[39m           host_callbacks=host_callbacks,\n\u001b[32m    371\u001b[39m       )\n\u001b[32m    372\u001b[39m     \u001b[38;5;66;03m# Some backends don't have `host_callbacks` option yet\u001b[39;00m\n\u001b[32m    373\u001b[39m     \u001b[38;5;66;03m# TODO(sharadmv): remove this fallback when all backends allow `compile`\u001b[39;00m\n\u001b[32m    374\u001b[39m     \u001b[38;5;66;03m# to take in `host_callbacks`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m375\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile_and_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbuilt_c\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexecutable_devices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutable_devices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _jax.JaxRuntimeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    381\u001b[39m   \u001b[38;5;28;01mfor\u001b[39;00m error_handler \u001b[38;5;129;01min\u001b[39;00m _XLA_RUNTIME_ERROR_HANDLERS:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "results = main(n_runs=1, n_epochs=200, output_dir='lih_results', data_dir='eqnn_force_field_data_LiH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c76c796-d49d-4e77-b24a-06e5a3d4c5e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf23a076-d35e-4a69-a0a7-7d40780e8b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Comparison of Graph Equivariant vs Non-Equivariant QML for LiH Energy/Force Prediction\n",
    "\n",
    "This script runs both methods on LiH molecular data and compares their performance.\n",
    "All results are saved to an output directory for later analysis.\n",
    "\n",
    "Methods compared:\n",
    "1. Rotationally Equivariant QML - Uses SO(3) equivariant encoding with Heisenberg observable\n",
    "2. Graph Embedding Equivariant QML - Simple QNN with basic rotations\n",
    "\n",
    "Usage:\n",
    "    python run_comparison.py --n_runs 3 --n_epochs 100 --output_dir results\n",
    "\"\"\"\n",
    "\n",
    "import pennylane as qml\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "\n",
    "import jax\n",
    "jax.config.update('jax_platform_name', 'cpu')\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "from jax import numpy as jnp\n",
    "\n",
    "from jax.example_libraries import optimizers\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.optimize import curve_fit\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =============================================================================\n",
    "# ROTATIONALLY EQUIVARIANT QML MODEL\n",
    "# =============================================================================\n",
    "\n",
    "class EquivariantQML:\n",
    "    \"\"\"\n",
    "    Rotationally Equivariant Quantum Machine Learning model for molecular properties.\n",
    "    Uses SO(3) equivariant encoding with Heisenberg Hamiltonian observable.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_qubits=3, depth=6, blocks=2, seed=42):\n",
    "        self.num_qubits = num_qubits\n",
    "        self.depth = depth\n",
    "        self.blocks = blocks\n",
    "        self.seed = seed\n",
    "        \n",
    "        # Pauli matrices\n",
    "        self.X = np.array([[0, 1], [1, 0]])\n",
    "        self.Y = np.array([[0, -1.0j], [1.0j, 0]])\n",
    "        self.Z = np.array([[1, 0], [0, -1]])\n",
    "        \n",
    "        self.sigmas = jnp.array(np.array([self.X, self.Y, self.Z]))\n",
    "        self.sigmas_sigmas = jnp.array(np.array([\n",
    "            np.kron(self.X, self.X),\n",
    "            np.kron(self.Y, self.Y),\n",
    "            np.kron(self.Z, self.Z)\n",
    "        ]))\n",
    "        \n",
    "        # Create device and circuit\n",
    "        self.dev = qml.device(\"default.qubit\", wires=num_qubits)\n",
    "        self._create_circuit()\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self._init_params()\n",
    "    \n",
    "    def _create_circuit(self):\n",
    "        \"\"\"Create the equivariant quantum circuit.\"\"\"\n",
    "        num_qubits = self.num_qubits\n",
    "        depth = self.depth\n",
    "        blocks = self.blocks\n",
    "        sigmas = self.sigmas\n",
    "        sigmas_sigmas = self.sigmas_sigmas\n",
    "        \n",
    "        # Heisenberg observable\n",
    "        Heisenberg = [\n",
    "            qml.PauliX(0) @ qml.PauliX(1),\n",
    "            qml.PauliY(0) @ qml.PauliY(1),\n",
    "            qml.PauliZ(0) @ qml.PauliZ(1),\n",
    "        ]\n",
    "        self.Observable = qml.Hamiltonian(np.ones((3)), Heisenberg)\n",
    "        \n",
    "        def singlet(wires):\n",
    "            qml.Hadamard(wires=wires[0])\n",
    "            qml.PauliZ(wires=wires[0])\n",
    "            qml.PauliX(wires=wires[1])\n",
    "            qml.CNOT(wires=wires)\n",
    "        \n",
    "        def equivariant_encoding(alpha, data, wires):\n",
    "            hamiltonian = jnp.einsum(\"i,ijk\", data, sigmas)\n",
    "            U = jax.scipy.linalg.expm(-1.0j * alpha * hamiltonian / 2)\n",
    "            qml.QubitUnitary(U, wires=wires, id=\"E\")\n",
    "        \n",
    "        def trainable_layer(weight, wires):\n",
    "            hamiltonian = jnp.einsum(\"ijk->jk\", sigmas_sigmas)\n",
    "            U = jax.scipy.linalg.expm(-1.0j * weight * hamiltonian)\n",
    "            qml.QubitUnitary(U, wires=wires, id=\"U\")\n",
    "        \n",
    "        @qml.qnode(self.dev, interface=\"jax\")\n",
    "        def circuit(data, params):\n",
    "            weights = params[\"params\"][\"weights\"]\n",
    "            alphas = params[\"params\"][\"alphas\"]\n",
    "            \n",
    "            # Initial entangled state\n",
    "            if num_qubits >= 2:\n",
    "                singlet(wires=[0, 1])\n",
    "            if num_qubits >= 3:\n",
    "                qml.CNOT(wires=[1, 2])\n",
    "            \n",
    "            # Initial encoding\n",
    "            for i in range(num_qubits):\n",
    "                equivariant_encoding(alphas[i, 0], jnp.asarray(data, dtype=complex)[i % 1, ...], wires=[i])\n",
    "            \n",
    "            # Layers\n",
    "            for d in range(depth):\n",
    "                qml.Barrier()\n",
    "                for b in range(blocks):\n",
    "                    for i in range(0, num_qubits - 1, 2):\n",
    "                        trainable_layer(weights[i, d + 1, b], wires=[i, (i + 1) % num_qubits])\n",
    "                    for i in range(1, num_qubits, 2):\n",
    "                        trainable_layer(weights[i, d + 1, b], wires=[i, (i + 1) % num_qubits])\n",
    "                \n",
    "                for i in range(num_qubits):\n",
    "                    equivariant_encoding(alphas[i, d + 1], jnp.asarray(data, dtype=complex)[i % 1, ...], wires=[i])\n",
    "            \n",
    "            return qml.expval(self.Observable)\n",
    "        \n",
    "        self.circuit = circuit\n",
    "        self.vec_circuit = jax.vmap(circuit, (0, None), 0)\n",
    "    \n",
    "    def _init_params(self):\n",
    "        \"\"\"Initialize trainable parameters.\"\"\"\n",
    "        np.random.seed(self.seed)\n",
    "        limit = np.sqrt(1.0 / (self.num_qubits * self.depth))\n",
    "        weights = np.random.uniform(-limit, limit, (self.num_qubits, self.depth + 1, self.blocks))\n",
    "        \n",
    "        np.random.seed(self.seed + 1)\n",
    "        alphas = np.random.uniform(0.3, 0.8, (self.num_qubits, self.depth + 1))\n",
    "        \n",
    "        self.params = {\n",
    "            \"params\": {\n",
    "                \"weights\": jnp.array(weights),\n",
    "                \"alphas\": jnp.array(alphas),\n",
    "                \"epsilon\": None\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def get_params(self):\n",
    "        return self.params\n",
    "    \n",
    "    def set_params(self, params):\n",
    "        self.params = params\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# GRAPH EMBEDDING EQUIVARIANT QML MODEL\n",
    "# =============================================================================\n",
    "\n",
    "class NonEquivariantQML:\n",
    "    \"\"\"\n",
    "    Graph Embedding Equivariant QNN for molecular properties.\n",
    "    Uses basic rotations without symmetry preservation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_qubits=4, depth=3, seed=42):\n",
    "        self.num_qubits = num_qubits\n",
    "        self.depth = depth\n",
    "        self.seed = seed\n",
    "        \n",
    "        self.dev = qml.device(\"default.qubit\", wires=num_qubits)\n",
    "        self._create_circuit()\n",
    "        self._init_params()\n",
    "    \n",
    "    def _create_circuit(self):\n",
    "        \"\"\"Create the simple QNN circuit.\"\"\"\n",
    "        num_qubits = self.num_qubits\n",
    "        depth = self.depth\n",
    "        \n",
    "        @qml.qnode(self.dev, interface=\"jax\", diff_method=\"backprop\")\n",
    "        def circuit(positions, params):\n",
    "            weights = params[\"weights\"]\n",
    "            \n",
    "            # Single feature: bond length\n",
    "            dist = jnp.linalg.norm(positions[1] - positions[0])\n",
    "            \n",
    "            # Initialize\n",
    "            for i in range(num_qubits):\n",
    "                qml.RY(0.5, wires=i)\n",
    "            \n",
    "            # Simple layers\n",
    "            for layer in range(depth):\n",
    "                for i in range(num_qubits):\n",
    "                    qml.RY(weights[layer, i, 0] * dist, wires=i)\n",
    "                \n",
    "                for i in range(num_qubits - 1):\n",
    "                    qml.CNOT(wires=[i, i + 1])\n",
    "                \n",
    "                for i in range(num_qubits):\n",
    "                    qml.RZ(weights[layer, i, 1], wires=i)\n",
    "                    qml.RY(weights[layer, i, 2], wires=i)\n",
    "            \n",
    "            return qml.expval(qml.PauliZ(0) + qml.PauliZ(1) + qml.PauliZ(2) + qml.PauliZ(3))\n",
    "        \n",
    "        self.circuit = circuit\n",
    "        self.vec_circuit = jax.vmap(circuit, (0, None), 0)\n",
    "    \n",
    "    def _init_params(self):\n",
    "        \"\"\"Initialize parameters.\"\"\"\n",
    "        np.random.seed(self.seed)\n",
    "        weights = np.random.normal(0, 0.1, (self.depth, self.num_qubits, 3))\n",
    "        self.params = {\"weights\": jnp.array(weights)}\n",
    "    \n",
    "    def get_params(self):\n",
    "        return self.params\n",
    "    \n",
    "    def set_params(self, params):\n",
    "        self.params = params\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# TRAINING FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def train_equivariant(model, data_train, E_train, F_train, data_test, E_test, F_test,\n",
    "                      n_epochs=200, lr=0.01, lambda_E=1.5, lambda_F=2.0):\n",
    "    \"\"\"Train the equivariant model.\"\"\"\n",
    "    \n",
    "    def energy_single(coords, params):\n",
    "        return model.circuit(coords, params)\n",
    "    \n",
    "    def force_single(coords, params):\n",
    "        grad_fn = jax.grad(energy_single, argnums=0)\n",
    "        return -grad_fn(coords, params)\n",
    "    \n",
    "    vec_force = jax.vmap(force_single, (0, None), 0)\n",
    "    \n",
    "    @jax.jit\n",
    "    def mse_loss(predictions, targets):\n",
    "        return jnp.mean((predictions - targets) ** 2)\n",
    "    \n",
    "    @jax.jit\n",
    "    def cost(params, data, E_target, F_target):\n",
    "        E_pred = model.vec_circuit(data, params)\n",
    "        E_loss = mse_loss(E_pred, E_target)\n",
    "        \n",
    "        F_pred = vec_force(data, params)\n",
    "        F_loss = mse_loss(F_pred, F_target)\n",
    "        \n",
    "        total_loss = lambda_E * E_loss + lambda_F * F_loss\n",
    "        return total_loss, (E_loss, F_loss)\n",
    "    \n",
    "    opt_init, opt_update, get_params = optimizers.adam(lr)\n",
    "    opt_state = opt_init(model.params)\n",
    "    \n",
    "    history = {\"epoch\": [], \"train_loss\": [], \"test_E_loss\": [], \"test_F_loss\": []}\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        (loss, (E_loss, F_loss)), grads = jax.value_and_grad(cost, argnums=0, has_aux=True)(\n",
    "            get_params(opt_state), data_train, E_train, F_train\n",
    "        )\n",
    "        opt_state = opt_update(epoch, grads, opt_state)\n",
    "        \n",
    "        if (epoch + 1) % max(1, n_epochs // 20) == 0:\n",
    "            test_params = get_params(opt_state)\n",
    "            E_pred_test = np.array(model.vec_circuit(data_test, test_params))\n",
    "            F_pred_test = np.array(vec_force(data_test, test_params))\n",
    "            \n",
    "            E_test_loss = np.mean((E_pred_test - np.array(E_test)) ** 2)\n",
    "            F_test_loss = np.mean((F_pred_test - np.array(F_test)) ** 2)\n",
    "            \n",
    "            history[\"epoch\"].append(epoch + 1)\n",
    "            history[\"train_loss\"].append(float(loss))\n",
    "            history[\"test_E_loss\"].append(float(E_test_loss))\n",
    "            history[\"test_F_loss\"].append(float(F_test_loss))\n",
    "    \n",
    "    model.set_params(get_params(opt_state))\n",
    "    return history\n",
    "\n",
    "\n",
    "def train_non_equivariant(model, pos_train, E_train, F_train, pos_test, E_test, F_test,\n",
    "                          n_epochs=200, lr=0.01, lambda_E=2.0, lambda_F=1.0):\n",
    "    \"\"\"Train the non-equivariant model.\"\"\"\n",
    "    \n",
    "    def energy_single(coords, params):\n",
    "        return model.circuit(coords, params)\n",
    "    \n",
    "    def force_single(coords, params):\n",
    "        grad_fn = jax.grad(energy_single, argnums=0)\n",
    "        return -grad_fn(coords, params)\n",
    "    \n",
    "    vec_force = jax.vmap(force_single, (0, None), 0)\n",
    "    \n",
    "    @jax.jit\n",
    "    def combined_loss(params, positions, E_target, F_target):\n",
    "        E_pred = model.vec_circuit(positions, params)\n",
    "        E_loss = jnp.mean((E_pred - E_target) ** 2)\n",
    "        \n",
    "        F_pred_full = vec_force(positions, params)\n",
    "        F_pred_z = F_pred_full[:, 1, 2]\n",
    "        F_loss = jnp.mean((F_pred_z - F_target) ** 2)\n",
    "        \n",
    "        E_loss = jnp.where(jnp.isnan(E_loss), 1.0, E_loss)\n",
    "        F_loss = jnp.where(jnp.isnan(F_loss), 1.0, F_loss)\n",
    "        \n",
    "        total_loss = lambda_E * E_loss + lambda_F * F_loss\n",
    "        return total_loss, (E_loss, F_loss)\n",
    "    \n",
    "    opt_init, opt_update, get_params = optimizers.adam(lr)\n",
    "    opt_state = opt_init(model.params)\n",
    "    \n",
    "    history = {\"epoch\": [], \"train_loss\": [], \"test_E_loss\": [], \"test_F_loss\": []}\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        (loss, (E_loss, F_loss)), grads = jax.value_and_grad(combined_loss, has_aux=True)(\n",
    "            get_params(opt_state), pos_train, E_train, F_train\n",
    "        )\n",
    "        \n",
    "        grad_norm = jnp.sqrt(sum(jnp.sum(jnp.square(g)) for g in jax.tree.leaves(grads) if g is not None))\n",
    "        if grad_norm > 10.0:\n",
    "            grads = jax.tree.map(lambda g: g * (10.0 / grad_norm) if g is not None else g, grads)\n",
    "        \n",
    "        opt_state = opt_update(epoch, grads, opt_state)\n",
    "        \n",
    "        if (epoch + 1) % max(1, n_epochs // 20) == 0:\n",
    "            test_params = get_params(opt_state)\n",
    "            E_pred_test = np.array(model.vec_circuit(pos_test, test_params))\n",
    "            F_pred_test = np.array(vec_force(pos_test, test_params))[:, 1, 2]\n",
    "            \n",
    "            E_test_loss = np.mean((E_pred_test - np.array(E_test)) ** 2)\n",
    "            F_test_loss = np.mean((F_pred_test - np.array(F_test)) ** 2)\n",
    "            \n",
    "            history[\"epoch\"].append(epoch + 1)\n",
    "            history[\"train_loss\"].append(float(loss))\n",
    "            history[\"test_E_loss\"].append(float(E_test_loss))\n",
    "            history[\"test_F_loss\"].append(float(F_test_loss))\n",
    "    \n",
    "    model.set_params(get_params(opt_state))\n",
    "    return history\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# DATA LOADING\n",
    "# =============================================================================\n",
    "\n",
    "def load_lih_data(data_dir=\"eqnn_force_field_data_LiH\"):\n",
    "    \"\"\"Load LiH molecular data.\"\"\"\n",
    "    energy = np.load(os.path.join(data_dir, \"Energy.npy\"))\n",
    "    forces = np.load(os.path.join(data_dir, \"Forces.npy\"))\n",
    "    positions = np.load(os.path.join(data_dir, \"Positions.npy\"))\n",
    "    \n",
    "    return energy, forces, positions\n",
    "\n",
    "\n",
    "def prepare_data(energy, forces, positions, test_split=0.2, seed=42):\n",
    "    \"\"\"Prepare and scale data for training.\"\"\"\n",
    "    shape = positions.shape\n",
    "    \n",
    "    # Scale energy\n",
    "    energy_scaler = MinMaxScaler((-1, 1))\n",
    "    if energy.ndim == 1:\n",
    "        energy = energy.reshape(-1, 1)\n",
    "    energy_scaled = energy_scaler.fit_transform(energy).flatten()\n",
    "    \n",
    "    # Center molecule positions\n",
    "    n_atoms_total = positions.shape[1]\n",
    "    positions_centered = np.zeros((shape[0], n_atoms_total - 1, 3))\n",
    "    positions_centered[:, 0, :] = positions[:, 1, :] - positions[:, 0, :]\n",
    "    \n",
    "    # Scale forces (z-component of H atom)\n",
    "    forces_H = forces[:, 1:, :]\n",
    "    force_scaler = MinMaxScaler((-1, 1))\n",
    "    forces_z_only = forces_H[:, 0, 2].reshape(-1, 1)\n",
    "    forces_z_scaled = force_scaler.fit_transform(forces_z_only).flatten()\n",
    "    \n",
    "    forces_scaled = np.zeros_like(forces_H)\n",
    "    forces_scaled[:, 0, 2] = forces_z_scaled\n",
    "    \n",
    "    # Train/test split\n",
    "    np.random.seed(seed)\n",
    "    n_samples = shape[0]\n",
    "    indices_train = np.random.choice(np.arange(n_samples), size=int((1-test_split) * n_samples), replace=False)\n",
    "    indices_test = np.setdiff1d(np.arange(n_samples), indices_train)\n",
    "    \n",
    "    data = {\n",
    "        \"energy_scaler\": energy_scaler,\n",
    "        \"force_scaler\": force_scaler,\n",
    "        \"energy_scaled\": energy_scaled,\n",
    "        \"forces_scaled\": forces_scaled,\n",
    "        \"positions_centered\": positions_centered,\n",
    "        \"positions_raw\": positions,\n",
    "        \"forces_H\": forces_H,\n",
    "        \"indices_train\": indices_train,\n",
    "        \"indices_test\": indices_test,\n",
    "    }\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# EVALUATION\n",
    "# =============================================================================\n",
    "\n",
    "def evaluate_model(model, data, model_type=\"equivariant\"):\n",
    "    \"\"\"Evaluate model and compute metrics.\"\"\"\n",
    "    \n",
    "    positions_centered = data[\"positions_centered\"]\n",
    "    positions_raw = data[\"positions_raw\"]\n",
    "    energy_scaled = data[\"energy_scaled\"]\n",
    "    forces_z_scaled = data[\"forces_scaled\"][:, 0, 2]\n",
    "    energy_scaler = data[\"energy_scaler\"]\n",
    "    force_scaler = data[\"force_scaler\"]\n",
    "    indices_train = data[\"indices_train\"]\n",
    "    indices_test = data[\"indices_test\"]\n",
    "    forces_H = data[\"forces_H\"]\n",
    "    \n",
    "    if model_type == \"equivariant\":\n",
    "        # Get predictions\n",
    "        E_pred_scaled = np.array(model.vec_circuit(jnp.array(positions_centered), model.params))\n",
    "        \n",
    "        def energy_single(coords, params):\n",
    "            return model.circuit(coords, params)\n",
    "        def force_single(coords, params):\n",
    "            return -jax.grad(energy_single, argnums=0)(coords, params)\n",
    "        vec_force = jax.vmap(force_single, (0, None), 0)\n",
    "        \n",
    "        F_pred_scaled = np.array(vec_force(jnp.array(positions_centered), model.params))\n",
    "        F_pred_z_scaled = F_pred_scaled[:, 0, 2]\n",
    "    else:\n",
    "        E_pred_scaled = np.array(model.vec_circuit(jnp.array(positions_raw), model.params))\n",
    "        \n",
    "        def energy_single(coords, params):\n",
    "            return model.circuit(coords, params)\n",
    "        def force_single(coords, params):\n",
    "            return -jax.grad(energy_single, argnums=0)(coords, params)\n",
    "        vec_force = jax.vmap(force_single, (0, None), 0)\n",
    "        \n",
    "        F_pred_all = np.array(vec_force(jnp.array(positions_raw), model.params))\n",
    "        F_pred_z_scaled = F_pred_all[:, 1, 2]\n",
    "    \n",
    "    # Post-correction for energy\n",
    "    def corr_E(E, a, b, c):\n",
    "        return a * E**2 + b * E + c\n",
    "    \n",
    "    try:\n",
    "        popt_E, _ = curve_fit(corr_E, E_pred_scaled[indices_train], energy_scaled[indices_train])\n",
    "        E_pred_corrected = corr_E(E_pred_scaled, *popt_E)\n",
    "    except:\n",
    "        E_pred_corrected = E_pred_scaled\n",
    "    \n",
    "    # Post-correction for force\n",
    "    try:\n",
    "        lr_model = LinearRegression()\n",
    "        lr_model.fit(F_pred_z_scaled[indices_train].reshape(-1, 1), forces_z_scaled[indices_train])\n",
    "        F_pred_corrected = lr_model.predict(F_pred_z_scaled.reshape(-1, 1)).flatten()\n",
    "    except:\n",
    "        F_pred_corrected = F_pred_z_scaled\n",
    "    \n",
    "    # Inverse transform\n",
    "    E_pred_original = energy_scaler.inverse_transform(E_pred_corrected.reshape(-1, 1)).flatten()\n",
    "    F_pred_original = force_scaler.inverse_transform(F_pred_corrected.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    E_true_original = energy_scaler.inverse_transform(energy_scaled.reshape(-1, 1)).flatten()\n",
    "    F_true_original = forces_H[:, 0, 2]\n",
    "    \n",
    "    # Compute metrics on test set\n",
    "    E_mae = np.mean(np.abs(E_pred_original[indices_test] - E_true_original[indices_test]))\n",
    "    E_rmse = np.sqrt(np.mean((E_pred_original[indices_test] - E_true_original[indices_test]) ** 2))\n",
    "    E_r2 = 1 - np.sum((E_pred_original[indices_test] - E_true_original[indices_test])**2) / \\\n",
    "               np.sum((E_true_original[indices_test] - E_true_original[indices_test].mean())**2)\n",
    "    \n",
    "    F_mae = np.mean(np.abs(F_pred_original[indices_test] - F_true_original[indices_test]))\n",
    "    F_rmse = np.sqrt(np.mean((F_pred_original[indices_test] - F_true_original[indices_test]) ** 2))\n",
    "    F_r2 = 1 - np.sum((F_pred_original[indices_test] - F_true_original[indices_test])**2) / \\\n",
    "               np.sum((F_true_original[indices_test] - F_true_original[indices_test].mean())**2)\n",
    "    \n",
    "    metrics = {\n",
    "        \"E_mae_Ha\": float(E_mae),\n",
    "        \"E_mae_eV\": float(E_mae * 27.2114),\n",
    "        \"E_rmse_Ha\": float(E_rmse),\n",
    "        \"E_rmse_eV\": float(E_rmse * 27.2114),\n",
    "        \"E_r2\": float(E_r2),\n",
    "        \"F_mae\": float(F_mae),\n",
    "        \"F_rmse\": float(F_rmse),\n",
    "        \"F_r2\": float(F_r2),\n",
    "    }\n",
    "    \n",
    "    predictions = {\n",
    "        \"E_pred\": E_pred_original.tolist(),\n",
    "        \"E_true\": E_true_original.tolist(),\n",
    "        \"F_pred\": F_pred_original.tolist(),\n",
    "        \"F_true\": F_true_original.tolist(),\n",
    "        \"indices_test\": indices_test.tolist(),\n",
    "    }\n",
    "    \n",
    "    return metrics, predictions\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN COMPARISON\n",
    "# =============================================================================\n",
    "\n",
    "def run_comparison(n_runs=3, n_epochs=100, output_dir=\"lih_comparison_results\", data_dir=\"eqnn_force_field_data_LiH\"):\n",
    "    \"\"\"Run comparison between equivariant and non-equivariant models.\"\"\"\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"LiH Energy/Force Prediction: Rotationally Equivariant vs Graph Embedding Equivariant QML\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Load data\n",
    "    print(\"\\nLoading data...\")\n",
    "    try:\n",
    "        energy, forces, positions = load_lih_data(data_dir)\n",
    "        print(f\"  Loaded {len(energy)} samples\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  ERROR: Data not found in {data_dir}\")\n",
    "        print(\"  Please ensure the LiH data files are available\")\n",
    "        return None\n",
    "    \n",
    "    # Prepare data\n",
    "    data = prepare_data(energy, forces, positions)\n",
    "    print(f\"  Train: {len(data['indices_train'])}, Test: {len(data['indices_test'])}\")\n",
    "    \n",
    "    # Results storage\n",
    "    all_results = {\n",
    "        \"config\": {\n",
    "            \"n_runs\": n_runs,\n",
    "            \"n_epochs\": n_epochs,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "        },\n",
    "        \"equivariant\": {\"runs\": [], \"metrics_summary\": {}},\n",
    "        \"non_equivariant\": {\"runs\": [], \"metrics_summary\": {}},\n",
    "    }\n",
    "    \n",
    "    # Prepare training data\n",
    "    E_train = data[\"energy_scaled\"][data[\"indices_train\"]]\n",
    "    E_test = data[\"energy_scaled\"][data[\"indices_test\"]]\n",
    "    \n",
    "    # For equivariant model\n",
    "    data_train_eq = jnp.array(data[\"positions_centered\"][data[\"indices_train\"]])\n",
    "    data_test_eq = jnp.array(data[\"positions_centered\"][data[\"indices_test\"]])\n",
    "    F_train_eq = data[\"forces_scaled\"][data[\"indices_train\"]]\n",
    "    F_test_eq = data[\"forces_scaled\"][data[\"indices_test\"]]\n",
    "    \n",
    "    # For non-equivariant model\n",
    "    pos_train_neq = jnp.array(data[\"positions_raw\"][data[\"indices_train\"]])\n",
    "    pos_test_neq = jnp.array(data[\"positions_raw\"][data[\"indices_test\"]])\n",
    "    F_train_neq = data[\"forces_scaled\"][data[\"indices_train\"], 0, 2]\n",
    "    F_test_neq = data[\"forces_scaled\"][data[\"indices_test\"], 0, 2]\n",
    "    \n",
    "    # Run experiments\n",
    "    for run in range(n_runs):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"RUN {run+1}/{n_runs}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        run_seed = 42 + run * 100\n",
    "        \n",
    "        # --- Rotationally Equivariant Model ---\n",
    "        print(f\"\\n[Rotationally Equivariant QML]\")\n",
    "        eq_model = EquivariantQML(num_qubits=3, depth=6, blocks=2, seed=run_seed)\n",
    "        \n",
    "        print(f\"  Training for {n_epochs} epochs...\")\n",
    "        eq_history = train_equivariant(\n",
    "            eq_model, data_train_eq, E_train, F_train_eq,\n",
    "            data_test_eq, E_test, F_test_eq, n_epochs=n_epochs\n",
    "        )\n",
    "        \n",
    "        print(f\"  Evaluating...\")\n",
    "        eq_metrics, eq_predictions = evaluate_model(eq_model, data, \"equivariant\")\n",
    "        \n",
    "        print(f\"  Energy: MAE={eq_metrics['E_mae_Ha']:.6f} Ha, R²={eq_metrics['E_r2']:.4f}\")\n",
    "        print(f\"  Force:  MAE={eq_metrics['F_mae']:.4f} eV/Å, R²={eq_metrics['F_r2']:.4f}\")\n",
    "        \n",
    "        all_results[\"equivariant\"][\"runs\"].append({\n",
    "            \"run_id\": run,\n",
    "            \"seed\": run_seed,\n",
    "            \"history\": eq_history,\n",
    "            \"metrics\": eq_metrics,\n",
    "            \"predictions\": eq_predictions,\n",
    "        })\n",
    "        \n",
    "        # --- Graph Embedding Equivariant Model ---\n",
    "        print(f\"\\n[Graph Embedding Equivariant QML]\")\n",
    "        neq_model = NonEquivariantQML(num_qubits=4, depth=3, seed=run_seed)\n",
    "        \n",
    "        print(f\"  Training for {n_epochs} epochs...\")\n",
    "        neq_history = train_non_equivariant(\n",
    "            neq_model, pos_train_neq, E_train, F_train_neq,\n",
    "            pos_test_neq, E_test, F_test_neq, n_epochs=n_epochs\n",
    "        )\n",
    "        \n",
    "        print(f\"  Evaluating...\")\n",
    "        neq_metrics, neq_predictions = evaluate_model(neq_model, data, \"non_equivariant\")\n",
    "        \n",
    "        print(f\"  Energy: MAE={neq_metrics['E_mae_Ha']:.6f} Ha, R²={neq_metrics['E_r2']:.4f}\")\n",
    "        print(f\"  Force:  MAE={neq_metrics['F_mae']:.4f} eV/Å, R²={neq_metrics['F_r2']:.4f}\")\n",
    "        \n",
    "        all_results[\"non_equivariant\"][\"runs\"].append({\n",
    "            \"run_id\": run,\n",
    "            \"seed\": run_seed,\n",
    "            \"history\": neq_history,\n",
    "            \"metrics\": neq_metrics,\n",
    "            \"predictions\": neq_predictions,\n",
    "        })\n",
    "    \n",
    "    # Compute summary statistics\n",
    "    for model_type in [\"equivariant\", \"non_equivariant\"]:\n",
    "        metrics_list = [r[\"metrics\"] for r in all_results[model_type][\"runs\"]]\n",
    "        \n",
    "        summary = {}\n",
    "        for key in metrics_list[0].keys():\n",
    "            values = [m[key] for m in metrics_list]\n",
    "            summary[key] = {\n",
    "                \"mean\": float(np.mean(values)),\n",
    "                \"std\": float(np.std(values)),\n",
    "                \"min\": float(np.min(values)),\n",
    "                \"max\": float(np.max(values)),\n",
    "                \"values\": values,\n",
    "            }\n",
    "        all_results[model_type][\"metrics_summary\"] = summary\n",
    "    \n",
    "    # Save results\n",
    "    results_path = os.path.join(output_dir, \"results.json\")\n",
    "    with open(results_path, 'w') as f:\n",
    "        json.dump(all_results, f, indent=2)\n",
    "    print(f\"\\nResults saved to: {results_path}\")\n",
    "    \n",
    "    # Save numpy arrays for easy loading\n",
    "    np.savez(os.path.join(output_dir, \"metrics.npz\"),\n",
    "             eq_E_r2=[r[\"metrics\"][\"E_r2\"] for r in all_results[\"equivariant\"][\"runs\"]],\n",
    "             eq_F_r2=[r[\"metrics\"][\"F_r2\"] for r in all_results[\"equivariant\"][\"runs\"]],\n",
    "             eq_E_mae=[r[\"metrics\"][\"E_mae_Ha\"] for r in all_results[\"equivariant\"][\"runs\"]],\n",
    "             eq_F_mae=[r[\"metrics\"][\"F_mae\"] for r in all_results[\"equivariant\"][\"runs\"]],\n",
    "             neq_E_r2=[r[\"metrics\"][\"E_r2\"] for r in all_results[\"non_equivariant\"][\"runs\"]],\n",
    "             neq_F_r2=[r[\"metrics\"][\"F_r2\"] for r in all_results[\"non_equivariant\"][\"runs\"]],\n",
    "             neq_E_mae=[r[\"metrics\"][\"E_mae_Ha\"] for r in all_results[\"non_equivariant\"][\"runs\"]],\n",
    "             neq_F_mae=[r[\"metrics\"][\"F_mae\"] for r in all_results[\"non_equivariant\"][\"runs\"]])\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\n{'Metric':<20} {'Rotationally Equivariant QML':<30} {'Graph Embedding Equivariant QML':<30}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    for metric in [\"E_r2\", \"E_mae_Ha\", \"F_r2\", \"F_mae\"]:\n",
    "        eq_mean = all_results[\"equivariant\"][\"metrics_summary\"][metric][\"mean\"]\n",
    "        eq_std = all_results[\"equivariant\"][\"metrics_summary\"][metric][\"std\"]\n",
    "        neq_mean = all_results[\"non_equivariant\"][\"metrics_summary\"][metric][\"mean\"]\n",
    "        neq_std = all_results[\"non_equivariant\"][\"metrics_summary\"][metric][\"std\"]\n",
    "        \n",
    "        print(f\"{metric:<20} {eq_mean:.4f} ± {eq_std:.4f}            {neq_mean:.4f} ± {neq_std:.4f}\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN\n",
    "# =============================================================================\n",
    "\n",
    "def main(n_runs=2, n_epochs=50, output_dir=\"lih_comparison_results\", data_dir=\"eqnn_force_field_data_LiH\"):\n",
    "    \"\"\"\n",
    "    Main function - can be called directly from Jupyter or command line.\n",
    "    \n",
    "    Args:\n",
    "        n_runs: Number of runs for each model\n",
    "        n_epochs: Training epochs per run\n",
    "        output_dir: Directory to save results\n",
    "        data_dir: Directory containing LiH data (.npy files)\n",
    "    \n",
    "    Returns:\n",
    "        results dictionary\n",
    "    \"\"\"\n",
    "    return run_comparison(\n",
    "        n_runs=n_runs,\n",
    "        n_epochs=n_epochs,\n",
    "        output_dir=output_dir,\n",
    "        data_dir=data_dir\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    \n",
    "    # Check if running in Jupyter\n",
    "    if 'ipykernel' in sys.modules:\n",
    "        # Running in Jupyter - use defaults or call main() directly\n",
    "        print(\"Running in Jupyter notebook. Call main() directly with parameters:\")\n",
    "        print(\"  results = main(n_runs=2, n_epochs=50, output_dir='lih_results', data_dir='eqnn_force_field_data_LiH')\")\n",
    "    else:\n",
    "        # Running as script - use argparse\n",
    "        parser = argparse.ArgumentParser(description=\"Compare Rotationally Equivariant vs Graph Embedding Equivariant QML on LiH\")\n",
    "        parser.add_argument(\"--n_runs\", type=int, default=2, help=\"Number of runs\")\n",
    "        parser.add_argument(\"--n_epochs\", type=int, default=50, help=\"Training epochs per run\")\n",
    "        parser.add_argument(\"--output_dir\", type=str, default=\"lih_comparison_results\", help=\"Output directory\")\n",
    "        parser.add_argument(\"--data_dir\", type=str, default=\"eqnn_force_field_data_LiH\", help=\"Data directory\")\n",
    "        \n",
    "        args = parser.parse_args()\n",
    "        \n",
    "        results = main(\n",
    "            n_runs=args.n_runs,\n",
    "            n_epochs=args.n_epochs,\n",
    "            output_dir=args.output_dir,\n",
    "            data_dir=args.data_dir\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ac7b56b-1b28-4826-8ca2-724fe17a9ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LiH Energy/Force Prediction: Equivariant vs Non-Equivariant QML\n",
      "======================================================================\n",
      "\n",
      "Loading data...\n",
      "  Loaded 2400 samples\n",
      "  Train: 1920, Test: 480\n",
      "\n",
      "======================================================================\n",
      "RUN 1/1\n",
      "======================================================================\n",
      "\n",
      "[Equivariant Model]\n",
      "  Training for 200 epochs...\n",
      "  Evaluating...\n",
      "  Energy: MAE=0.033382 Ha, R²=0.9966\n",
      "  Force:  MAE=3.2999 eV/Å, R²=0.9239\n",
      "\n",
      "[Non-Equivariant Model]\n",
      "  Training for 200 epochs...\n",
      "  Evaluating...\n",
      "  Energy: MAE=0.026232 Ha, R²=0.9979\n",
      "  Force:  MAE=2.5130 eV/Å, R²=0.9584\n",
      "\n",
      "Results saved to: lih_results/results.json\n",
      "\n",
      "======================================================================\n",
      "SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Metric               Equivariant               Non-Equivariant          \n",
      "----------------------------------------------------------------------\n",
      "E_r2                 0.9966 ± 0.0000       0.9979 ± 0.0000\n",
      "E_mae_Ha             0.0334 ± 0.0000       0.0262 ± 0.0000\n",
      "F_r2                 0.9239 ± 0.0000       0.9584 ± 0.0000\n",
      "F_mae                3.2999 ± 0.0000       2.5130 ± 0.0000\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "results = main(n_runs=1, n_epochs=200, output_dir='lih_results', data_dir='eqnn_force_field_data_LiH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5103f415-d4c4-46f9-98d7-302d6cf1baf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
