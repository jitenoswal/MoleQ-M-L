{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c67cd237-cde2-4eb3-9b54-e9b779160d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saumya/.conda/envs/penny_skl/lib/python3.12/site-packages/pennylane/__init__.py:209: RuntimeWarning: PennyLane is not yet compatible with JAX versions > 0.6.2. You have version 0.8.0 installed. Please downgrade JAX to 0.6.2 to avoid runtime errors using python -m pip install jax~=0.6.0 jaxlib~=0.6.0\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in Jupyter. Call main() with parameters:\n",
      "  results = main(k_folds=5, n_epochs=200, output_dir='kfold_results_lih')\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "LiH QML K-Fold Cross-Validation Comparison\n",
    "============================================\n",
    "\n",
    "Compares generalizability of four approaches using k-fold cross-validation:\n",
    "1. Rotationally Equivariant QML - Uses SO(3) equivariant encoding with Heisenberg observable\n",
    "2. Non-Equivariant QML - Simple QNN with basic rotations\n",
    "3. Graph Permutation Equivariant QML - Uses graph-based permutation-symmetric encoding\n",
    "4. Classical Rotationally Equivariant NN - Classical MLP on pairwise distances (E(3) invariant)\n",
    "\n",
    "This script evaluates model generalizability by:\n",
    "- Splitting data into k folds\n",
    "- Training on k-1 folds, testing on held-out fold\n",
    "- Repeating for each fold\n",
    "- Computing variance across folds (measures sensitivity to data splits)\n",
    "\n",
    "Usage (command line):\n",
    "    python run_kfold_comparison_lih.py --k_folds 5 --n_epochs 200 --output_dir kfold_results_lih\n",
    "\n",
    "Usage (Jupyter):\n",
    "    from run_kfold_comparison_lih import main\n",
    "    results = main(k_folds=5, n_epochs=200, output_dir='kfold_results_lih')\n",
    "\"\"\"\n",
    "\n",
    "import pennylane as qml\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import jax\n",
    "jax.config.update('jax_platform_name', 'cpu')\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "from jax import numpy as jnp\n",
    "from jax.example_libraries import optimizers\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.optimize import curve_fit\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 1. ROTATIONALLY EQUIVARIANT QML MODEL (SO(3))\n",
    "# =============================================================================\n",
    "\n",
    "class RotationallyEquivariantQML:\n",
    "    \"\"\"\n",
    "    Rotationally Equivariant Quantum Machine Learning model for LiH.\n",
    "    Uses SO(3) equivariant encoding with native PennyLane gates.\n",
    "    \n",
    "    Architecture adapted from NH₃ implementation with:\n",
    "    - Multiple qubits with singlet initialization\n",
    "    - Learnable head_scale and head_bias for output\n",
    "    - Proper alpha initialization in [0.5, 1.5] range\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_qubits=6, depth=6, seed=42):\n",
    "        self.n_qubits = n_qubits\n",
    "        self.depth = depth\n",
    "        self.seed = seed\n",
    "        \n",
    "        self.dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "        \n",
    "        # Heisenberg observable\n",
    "        self.observable = (\n",
    "            qml.PauliX(0) @ qml.PauliX(1)\n",
    "            + qml.PauliY(0) @ qml.PauliY(1)\n",
    "            + qml.PauliZ(0) @ qml.PauliZ(1)\n",
    "        )\n",
    "        \n",
    "        self._build_circuit()\n",
    "        self._init_params()\n",
    "    \n",
    "    def _singlet(self, wires):\n",
    "        \"\"\"Create singlet state on two qubits.\"\"\"\n",
    "        w0, w1 = wires\n",
    "        qml.Hadamard(wires=w0)\n",
    "        qml.PauliZ(wires=w0)\n",
    "        qml.PauliX(wires=w1)\n",
    "        qml.CNOT(wires=[w0, w1])\n",
    "    \n",
    "    def _equivariant_encoding(self, alpha, vec3, wire):\n",
    "        \"\"\"SO(3) equivariant encoding using qml.Rot.\"\"\"\n",
    "        r = jnp.array(vec3, dtype=jnp.float64)\n",
    "        norm = jnp.linalg.norm(r) + 1e-12\n",
    "        n = r / norm\n",
    "        theta = alpha * norm\n",
    "        qml.Rot(theta * n[0], theta * n[1], theta * n[2], wires=wire)\n",
    "    \n",
    "    def _pair_layer(self, weight, wires):\n",
    "        \"\"\"Trainable Heisenberg-like interaction.\"\"\"\n",
    "        qml.IsingXX(weight, wires=wires)\n",
    "        qml.IsingYY(weight, wires=wires)\n",
    "        qml.IsingZZ(weight, wires=wires)\n",
    "    \n",
    "    def _build_circuit(self):\n",
    "        \"\"\"Build the quantum circuit.\"\"\"\n",
    "        @qml.qnode(self.dev, interface=\"jax\", diff_method=\"backprop\")\n",
    "        def circuit(coords, params):\n",
    "            \"\"\"\n",
    "            coords: (1, 3) - H position relative to Li\n",
    "            params: {\"weights\", \"alphas\", \"head_scale\", \"head_bias\"}\n",
    "            \"\"\"\n",
    "            weights = params[\"weights\"]\n",
    "            alphas = params[\"alphas\"]\n",
    "            \n",
    "            # Initialize singlets on pairs of qubits\n",
    "            for i in range(0, self.n_qubits - 1, 2):\n",
    "                self._singlet([i, i + 1])\n",
    "            \n",
    "            # Initial encoding - all qubits encode the same H position\n",
    "            for i in range(self.n_qubits):\n",
    "                self._equivariant_encoding(alphas[i, 0], coords[0], i)\n",
    "            \n",
    "            # Variational layers\n",
    "            for d in range(self.depth):\n",
    "                qml.Barrier()\n",
    "                # Even pairs\n",
    "                for i in range(0, self.n_qubits - 1, 2):\n",
    "                    self._pair_layer(weights[i, d], [i, (i + 1) % self.n_qubits])\n",
    "                # Odd pairs\n",
    "                for i in range(1, self.n_qubits, 2):\n",
    "                    self._pair_layer(weights[i, d], [i, (i + 1) % self.n_qubits])\n",
    "                # Re-encoding\n",
    "                for i in range(self.n_qubits):\n",
    "                    self._equivariant_encoding(alphas[i, d + 1], coords[0], i)\n",
    "            \n",
    "            return qml.expval(self.observable)\n",
    "        \n",
    "        self.circuit = circuit\n",
    "        self.vec_circuit = jax.vmap(circuit, in_axes=(0, None), out_axes=0)\n",
    "    \n",
    "    def _init_params(self):\n",
    "        \"\"\"Initialize parameters with proper ranges.\"\"\"\n",
    "        np.random.seed(self.seed)\n",
    "        \n",
    "        # Weights: small initial values, only first row non-zero initially\n",
    "        weights = np.zeros((self.n_qubits, self.depth), dtype=np.float64)\n",
    "        weights[0] = np.random.uniform(0.0, np.pi, size=(self.depth,))\n",
    "        \n",
    "        # Alphas: in [0.5, 1.5] range for stable encoding\n",
    "        alphas = np.random.uniform(0.5, 1.5, size=(self.n_qubits, self.depth + 1))\n",
    "        \n",
    "        self.params = {\n",
    "            \"weights\": jnp.array(weights),\n",
    "            \"alphas\": jnp.array(alphas),\n",
    "            \"head_scale\": jnp.array(1.0),\n",
    "            \"head_bias\": jnp.array(0.0),\n",
    "        }\n",
    "    \n",
    "    def energy(self, coords, params):\n",
    "        \"\"\"Compute energy with head transformation.\"\"\"\n",
    "        raw = self.circuit(coords, params)\n",
    "        return params[\"head_scale\"] * raw + params[\"head_bias\"]\n",
    "    \n",
    "    def get_params(self):\n",
    "        return self.params\n",
    "    \n",
    "    def set_params(self, params):\n",
    "        self.params = params\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. NON-EQUIVARIANT QML MODEL\n",
    "# =============================================================================\n",
    "\n",
    "class NonEquivariantQML:\n",
    "    \"\"\"Non-Equivariant QML model for LiH.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_qubits=4, depth=4, seed=42):\n",
    "        self.n_qubits = n_qubits\n",
    "        self.depth = depth\n",
    "        self.seed = seed\n",
    "        \n",
    "        self.dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "        self._build_circuit()\n",
    "        self._init_params()\n",
    "    \n",
    "    def _build_circuit(self):\n",
    "        @qml.qnode(self.dev, interface=\"jax\", diff_method=\"backprop\")\n",
    "        def circuit(positions, params):\n",
    "            weights = params[\"weights\"]\n",
    "            \n",
    "            # Compute bond length\n",
    "            dist = jnp.linalg.norm(positions[1] - positions[0])\n",
    "            \n",
    "            # Encode distance\n",
    "            for i in range(self.n_qubits):\n",
    "                qml.Hadamard(wires=i)\n",
    "                qml.RY(dist * np.pi, wires=i)\n",
    "            \n",
    "            # Variational layers\n",
    "            for layer in range(self.depth):\n",
    "                for i in range(self.n_qubits):\n",
    "                    qml.RX(weights[layer, i, 0], wires=i)\n",
    "                    qml.RY(weights[layer, i, 1], wires=i)\n",
    "                    qml.RZ(weights[layer, i, 2], wires=i)\n",
    "                for i in range(self.n_qubits - 1):\n",
    "                    qml.CNOT(wires=[i, i + 1])\n",
    "                qml.CNOT(wires=[self.n_qubits - 1, 0])\n",
    "            \n",
    "            return qml.expval(qml.PauliZ(0))\n",
    "        \n",
    "        self.circuit = circuit\n",
    "        self.vec_circuit = jax.vmap(circuit, in_axes=(0, None), out_axes=0)\n",
    "    \n",
    "    def _init_params(self):\n",
    "        np.random.seed(self.seed)\n",
    "        weights = np.random.uniform(-np.pi, np.pi, (self.depth, self.n_qubits, 3))\n",
    "        self.params = {\n",
    "            \"weights\": jnp.array(weights),\n",
    "        }\n",
    "    \n",
    "    def get_params(self):\n",
    "        return self.params\n",
    "    \n",
    "    def set_params(self, params):\n",
    "        self.params = params\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3. GRAPH PERMUTATION EQUIVARIANT QML MODEL\n",
    "# =============================================================================\n",
    "\n",
    "class GraphPermutationEquivariantQML:\n",
    "    \"\"\"\n",
    "    Graph Permutation Equivariant Quantum Machine Learning model.\n",
    "    \n",
    "    Uses permutation-symmetric encoding based on graph structure:\n",
    "    - Encodes interatomic distances (permutation invariant features)\n",
    "    - Uses symmetric pooling operations\n",
    "    - Circuit structure respects graph connectivity\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_qubits=4, depth=4, seed=42):\n",
    "        self.n_qubits = n_qubits\n",
    "        self.depth = depth\n",
    "        self.seed = seed\n",
    "        \n",
    "        self.dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "        self._build_circuit()\n",
    "        self._init_params()\n",
    "    \n",
    "    def _build_circuit(self):\n",
    "        \"\"\"Create the graph permutation equivariant circuit.\"\"\"\n",
    "        n_qubits = self.n_qubits\n",
    "        depth = self.depth\n",
    "        \n",
    "        @qml.qnode(self.dev, interface=\"jax\", diff_method=\"backprop\")\n",
    "        def circuit(positions, params):\n",
    "            \"\"\"Graph-based permutation equivariant circuit.\"\"\"\n",
    "            weights = params[\"weights\"]       # (depth, n_qubits, 4)\n",
    "            edge_weights = params[\"edge_weights\"]  # (depth, num_edges, 2)\n",
    "            global_weights = params[\"global_weights\"]  # (depth, 3)\n",
    "            \n",
    "            # Compute pairwise distances (permutation invariant features)\n",
    "            dist_LiH = jnp.linalg.norm(positions[1] - positions[0])\n",
    "            \n",
    "            # Compute direction cosines for directional information\n",
    "            direction = (positions[1] - positions[0]) / (dist_LiH + 1e-8)\n",
    "            \n",
    "            # Symmetric aggregated features\n",
    "            center_of_mass = jnp.mean(positions, axis=0)\n",
    "            spread = jnp.std(positions)\n",
    "            \n",
    "            # Feature vector (all permutation invariant/equivariant)\n",
    "            features = jnp.array([\n",
    "                dist_LiH,\n",
    "                spread,\n",
    "                jnp.linalg.norm(center_of_mass),\n",
    "                direction[2]  # z-component for force direction\n",
    "            ])\n",
    "            \n",
    "            # === Symmetric Initial State ===\n",
    "            for i in range(n_qubits):\n",
    "                qml.Hadamard(wires=i)\n",
    "            \n",
    "            # === Graph-based Encoding Layers ===\n",
    "            for layer in range(depth):\n",
    "                # Node update\n",
    "                for i in range(n_qubits):\n",
    "                    angle_y = weights[layer, i, 0] * features[0] + weights[layer, i, 1] * features[1]\n",
    "                    angle_z = weights[layer, i, 2] * features[2] + weights[layer, i, 3] * features[3]\n",
    "                    \n",
    "                    qml.RY(angle_y, wires=i)\n",
    "                    qml.RZ(angle_z, wires=i)\n",
    "                \n",
    "                # Edge operations (ring topology)\n",
    "                edges = [(i, (i+1) % n_qubits) for i in range(n_qubits)]\n",
    "                \n",
    "                for e_idx, (i, j) in enumerate(edges):\n",
    "                    edge_angle = edge_weights[layer, e_idx % edge_weights.shape[1], 0] * dist_LiH\n",
    "                    \n",
    "                    qml.CNOT(wires=[i, j])\n",
    "                    qml.RZ(edge_angle, wires=j)\n",
    "                    qml.CNOT(wires=[i, j])\n",
    "                \n",
    "                # Global pooling layer\n",
    "                global_angle = global_weights[layer, 0] * dist_LiH + global_weights[layer, 1]\n",
    "                for i in range(n_qubits):\n",
    "                    qml.RY(global_angle * global_weights[layer, 2], wires=i)\n",
    "            \n",
    "            # === Permutation Symmetric Measurement ===\n",
    "            obs = sum(qml.PauliZ(i) for i in range(n_qubits))\n",
    "            return qml.expval(obs)\n",
    "        \n",
    "        self.circuit = circuit\n",
    "        self.vec_circuit = jax.vmap(circuit, in_axes=(0, None), out_axes=0)\n",
    "    \n",
    "    def _init_params(self):\n",
    "        \"\"\"Initialize parameters with Xavier-like initialization.\"\"\"\n",
    "        np.random.seed(self.seed)\n",
    "        \n",
    "        num_edges = self.n_qubits\n",
    "        \n",
    "        limit = np.sqrt(2.0 / (self.n_qubits + 4))\n",
    "        weights = np.random.uniform(-limit, limit, (self.depth, self.n_qubits, 4))\n",
    "        edge_weights = np.random.uniform(-0.5, 0.5, (self.depth, num_edges, 2))\n",
    "        global_weights = np.random.uniform(-0.3, 0.3, (self.depth, 3))\n",
    "        \n",
    "        self.params = {\n",
    "            \"weights\": jnp.array(weights),\n",
    "            \"edge_weights\": jnp.array(edge_weights),\n",
    "            \"global_weights\": jnp.array(global_weights)\n",
    "        }\n",
    "    \n",
    "    def get_params(self):\n",
    "        return self.params\n",
    "    \n",
    "    def set_params(self, params):\n",
    "        self.params = params\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 4. CLASSICAL ROTATIONALLY EQUIVARIANT NN (E(3) INVARIANT)\n",
    "# =============================================================================\n",
    "\n",
    "class ClassicalRotationallyEquivariantNN:\n",
    "    \"\"\"\n",
    "    Classical Rotationally Equivariant Neural Network for LiH.\n",
    "    Uses physics-inspired E(3) invariant features with smooth activations.\n",
    "    \n",
    "    Key improvements over basic version:\n",
    "    - SiLU activation (smooth for autodiff force computation)\n",
    "    - Multiple physics-inspired features (not just raw distance)\n",
    "    - Larger network with skip connections\n",
    "    - Two-phase training with force warmup\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dims=[128, 128, 64], seed=42):\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.seed = seed\n",
    "        \n",
    "        # Physics parameters for LiH\n",
    "        self.r_eq = 1.6  # Equilibrium Li-H distance in Å\n",
    "        self.morse_alpha = 2.0\n",
    "        \n",
    "        # RBF parameters\n",
    "        self.rbf_centers = jnp.linspace(0.8, 3.0, 8)  # 8 Gaussians\n",
    "        self.rbf_width = 0.3\n",
    "        \n",
    "        # Number of features: distance + 1/r + Morse + 8 RBF = 11\n",
    "        self.n_features = 11\n",
    "        \n",
    "        self._init_params()\n",
    "        self._create_model()\n",
    "    \n",
    "    def _init_params(self):\n",
    "        \"\"\"Initialize MLP parameters with Xavier initialization.\"\"\"\n",
    "        np.random.seed(self.seed)\n",
    "        \n",
    "        # Feature dimension -> hidden -> output\n",
    "        layer_sizes = [self.n_features] + self.hidden_dims + [1]\n",
    "        \n",
    "        params = {\"weights\": [], \"biases\": []}\n",
    "        \n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            fan_in = layer_sizes[i]\n",
    "            fan_out = layer_sizes[i + 1]\n",
    "            limit = np.sqrt(6.0 / (fan_in + fan_out))\n",
    "            \n",
    "            W = np.random.uniform(-limit, limit, (fan_in, fan_out))\n",
    "            b = np.zeros(fan_out)\n",
    "            \n",
    "            params[\"weights\"].append(jnp.array(W))\n",
    "            params[\"biases\"].append(jnp.array(b))\n",
    "        \n",
    "        # Skip connection weights (from input features to last hidden layer)\n",
    "        skip_dim = layer_sizes[-2]  # Last hidden layer dimension\n",
    "        params[\"skip_weight\"] = jnp.array(\n",
    "            np.random.uniform(-0.1, 0.1, (self.n_features, skip_dim))\n",
    "        )\n",
    "        \n",
    "        self.params = params\n",
    "    \n",
    "    def _create_model(self):\n",
    "        \"\"\"Create the forward pass function with physics-inspired features.\"\"\"\n",
    "        \n",
    "        def compute_features(positions):\n",
    "            \"\"\"Compute physics-inspired invariant features from positions.\"\"\"\n",
    "            # Li at index 0, H at index 1\n",
    "            r_vec = positions[1] - positions[0]\n",
    "            r = jnp.linalg.norm(r_vec) + 1e-12\n",
    "            \n",
    "            # Feature 1: Normalized distance\n",
    "            f_dist = r / 2.0  # Normalize by typical scale\n",
    "            \n",
    "            # Feature 2: Inverse distance (Coulomb-like)\n",
    "            f_inv = 1.0 / r\n",
    "            \n",
    "            # Feature 3: Morse-like term\n",
    "            f_morse = jnp.exp(-self.morse_alpha * (r - self.r_eq))\n",
    "            \n",
    "            # Features 4-11: RBF encoding (8 Gaussians)\n",
    "            f_rbf = jnp.exp(-((r - self.rbf_centers) ** 2) / (2 * self.rbf_width ** 2))\n",
    "            \n",
    "            # Concatenate all features\n",
    "            features = jnp.concatenate([\n",
    "                jnp.array([f_dist, f_inv, f_morse]),\n",
    "                f_rbf\n",
    "            ])\n",
    "            \n",
    "            return features\n",
    "        \n",
    "        def mlp_forward(x, params):\n",
    "            \"\"\"MLP forward pass with SiLU activation and skip connection.\"\"\"\n",
    "            weights = params[\"weights\"]\n",
    "            biases = params[\"biases\"]\n",
    "            skip_weight = params[\"skip_weight\"]\n",
    "            \n",
    "            h = x\n",
    "            for i in range(len(weights) - 1):\n",
    "                h = jnp.dot(h, weights[i]) + biases[i]\n",
    "                # SiLU activation: x * sigmoid(x) - smooth for autodiff!\n",
    "                h = h * jax.nn.sigmoid(h)\n",
    "                \n",
    "                # Add skip connection to last hidden layer\n",
    "                if i == len(weights) - 2:\n",
    "                    h = h + 0.1 * jnp.dot(x, skip_weight)\n",
    "            \n",
    "            # Output layer (no activation)\n",
    "            h = jnp.dot(h, weights[-1]) + biases[-1]\n",
    "            return h.squeeze(-1)\n",
    "        \n",
    "        def energy_from_positions(positions, params):\n",
    "            \"\"\"Compute energy from atomic positions.\"\"\"\n",
    "            features = compute_features(positions)\n",
    "            energy = mlp_forward(features, params)\n",
    "            return energy\n",
    "        \n",
    "        def force_from_positions(positions, params):\n",
    "            \"\"\"Compute forces as negative gradient of energy.\"\"\"\n",
    "            grad_fn = jax.grad(energy_from_positions, argnums=0)\n",
    "            return -grad_fn(positions, params)\n",
    "        \n",
    "        self.compute_features = compute_features\n",
    "        self.energy_fn = energy_from_positions\n",
    "        self.force_fn = force_from_positions\n",
    "        self.vec_energy = jax.vmap(energy_from_positions, (0, None), 0)\n",
    "        self.vec_force = jax.vmap(force_from_positions, (0, None), 0)\n",
    "    \n",
    "    def get_params(self):\n",
    "        return self.params\n",
    "    \n",
    "    def set_params(self, params):\n",
    "        self.params = params\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# TRAINING FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def train_rotationally_equivariant(model, data_train, E_train, F_train, n_epochs=200, \n",
    "                                    lr=3e-3, wE=1.0, wF_max=5.0, warmup_frac=0.4):\n",
    "    \"\"\"Train the rotationally equivariant QML model with force warmup curriculum.\"\"\"\n",
    "    warmup_epochs = int(n_epochs * warmup_frac)\n",
    "    \n",
    "    def raw_energy(coords, params):\n",
    "        \"\"\"Raw circuit output.\"\"\"\n",
    "        return model.circuit(coords, params)\n",
    "    \n",
    "    vec_raw_energy = jax.vmap(raw_energy, (0, None), 0)\n",
    "    \n",
    "    def vec_force_fn(coords_batch, params):\n",
    "        \"\"\"Compute forces as -grad(energy).\"\"\"\n",
    "        def single_force(coords):\n",
    "            grad_fn = jax.grad(raw_energy, argnums=0)\n",
    "            return -grad_fn(coords, params)\n",
    "        return jax.vmap(single_force)(coords_batch)\n",
    "    \n",
    "    @jax.jit\n",
    "    def loss_fn(params, coords, E_target, F_target, wF):\n",
    "        # Energy with head transformation\n",
    "        E_raw = vec_raw_energy(coords, params)\n",
    "        E_pred = params[\"head_scale\"] * E_raw + params[\"head_bias\"]\n",
    "        L_E = jnp.mean((E_pred - E_target) ** 2)\n",
    "        \n",
    "        # Forces (scaled by head_scale)\n",
    "        F_raw = vec_force_fn(coords, params)\n",
    "        F_pred = params[\"head_scale\"] * F_raw\n",
    "        F_pred_z = F_pred[:, 0, 2]  # H atom z-component\n",
    "        L_F = jnp.mean((F_pred_z - F_target) ** 2)\n",
    "        \n",
    "        # Handle NaNs\n",
    "        L_E = jnp.where(jnp.isnan(L_E), 1.0, L_E)\n",
    "        L_F = jnp.where(jnp.isnan(L_F), 1.0, L_F)\n",
    "        \n",
    "        return wE * L_E + wF * L_F, (L_E, L_F)\n",
    "    \n",
    "    opt_init, opt_update, get_params = optimizers.adam(lr)\n",
    "    opt_state = opt_init(model.params)\n",
    "    \n",
    "    history = {\"epoch\": [], \"train_loss\": [], \"test_E_loss\": [], \"test_F_loss\": []}\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # Warmup curriculum: gradually increase force weight\n",
    "        if epoch < warmup_epochs:\n",
    "            wF = wF_max * (epoch / warmup_epochs)\n",
    "        else:\n",
    "            wF = wF_max\n",
    "        \n",
    "        (loss, (L_E, L_F)), grads = jax.value_and_grad(loss_fn, has_aux=True)(\n",
    "            get_params(opt_state), data_train, E_train, F_train, wF\n",
    "        )\n",
    "        \n",
    "        # Gradient clipping\n",
    "        grad_norm = jnp.sqrt(sum(jnp.sum(g**2) for g in jax.tree.leaves(grads)))\n",
    "        if grad_norm > 10.0:\n",
    "            grads = jax.tree.map(lambda g: g * (10.0 / grad_norm), grads)\n",
    "        \n",
    "        opt_state = opt_update(epoch, grads, opt_state)\n",
    "        \n",
    "        if (epoch + 1) % max(1, n_epochs // 20) == 0:\n",
    "            history[\"epoch\"].append(epoch + 1)\n",
    "            history[\"train_loss\"].append(float(loss))\n",
    "            history[\"test_E_loss\"].append(float(L_E))\n",
    "            history[\"test_F_loss\"].append(float(L_F))\n",
    "    \n",
    "    model.set_params(get_params(opt_state))\n",
    "    return history\n",
    "\n",
    "\n",
    "def train_non_equivariant(model, positions, E_train, F_train, n_epochs=200, \n",
    "                          lr=0.01, lambda_E=2.0, lambda_F=1.0):\n",
    "    \"\"\"Train the non-equivariant QML model.\"\"\"\n",
    "    \n",
    "    def energy_single(pos, params):\n",
    "        return model.circuit(pos, params)\n",
    "    \n",
    "    def force_single(pos, params):\n",
    "        return -jax.grad(energy_single, argnums=0)(pos, params)\n",
    "    \n",
    "    vec_force = jax.vmap(force_single, (0, None), 0)\n",
    "    \n",
    "    @jax.jit\n",
    "    def loss_fn(params, positions, E_target, F_target):\n",
    "        E_pred = model.vec_circuit(positions, params)\n",
    "        E_loss = jnp.mean((E_pred - E_target) ** 2)\n",
    "        \n",
    "        F_pred = vec_force(positions, params)\n",
    "        F_pred_z = F_pred[:, 1, 2]\n",
    "        F_loss = jnp.mean((F_pred_z - F_target) ** 2)\n",
    "        \n",
    "        return lambda_E * E_loss + lambda_F * F_loss, (E_loss, F_loss)\n",
    "    \n",
    "    opt_init, opt_update, get_params = optimizers.adam(lr)\n",
    "    opt_state = opt_init(model.params)\n",
    "    \n",
    "    history = {\"epoch\": [], \"train_loss\": [], \"test_E_loss\": [], \"test_F_loss\": []}\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        (loss, (E_loss, F_loss)), grads = jax.value_and_grad(loss_fn, has_aux=True)(\n",
    "            get_params(opt_state), positions, E_train, F_train\n",
    "        )\n",
    "        \n",
    "        grad_norm = jnp.sqrt(sum(jnp.sum(g**2) for g in jax.tree.leaves(grads)))\n",
    "        if grad_norm > 10.0:\n",
    "            grads = jax.tree.map(lambda g: g * (10.0 / grad_norm), grads)\n",
    "        \n",
    "        opt_state = opt_update(epoch, grads, opt_state)\n",
    "        \n",
    "        if (epoch + 1) % max(1, n_epochs // 20) == 0:\n",
    "            history[\"epoch\"].append(epoch + 1)\n",
    "            history[\"train_loss\"].append(float(loss))\n",
    "            history[\"test_E_loss\"].append(float(E_loss))\n",
    "            history[\"test_F_loss\"].append(float(F_loss))\n",
    "    \n",
    "    model.set_params(get_params(opt_state))\n",
    "    return history\n",
    "\n",
    "\n",
    "def train_graph_permutation(model, positions, E_train, F_train, n_epochs=200, \n",
    "                            lr=0.01, lambda_E=1.5, lambda_F=1.5):\n",
    "    \"\"\"Train the graph permutation equivariant QML model.\"\"\"\n",
    "    \n",
    "    def energy_single(pos, params):\n",
    "        return model.circuit(pos, params)\n",
    "    \n",
    "    def force_single(pos, params):\n",
    "        return -jax.grad(energy_single, argnums=0)(pos, params)\n",
    "    \n",
    "    vec_force = jax.vmap(force_single, (0, None), 0)\n",
    "    \n",
    "    @jax.jit\n",
    "    def loss_fn(params, positions, E_target, F_target):\n",
    "        E_pred = model.vec_circuit(positions, params)\n",
    "        E_loss = jnp.mean((E_pred - E_target) ** 2)\n",
    "        \n",
    "        F_pred = vec_force(positions, params)\n",
    "        F_pred_z = F_pred[:, 1, 2]\n",
    "        F_loss = jnp.mean((F_pred_z - F_target) ** 2)\n",
    "        \n",
    "        # Handle NaNs\n",
    "        E_loss = jnp.where(jnp.isnan(E_loss), 1.0, E_loss)\n",
    "        F_loss = jnp.where(jnp.isnan(F_loss), 1.0, F_loss)\n",
    "        \n",
    "        return lambda_E * E_loss + lambda_F * F_loss, (E_loss, F_loss)\n",
    "    \n",
    "    opt_init, opt_update, get_params = optimizers.adam(lr)\n",
    "    opt_state = opt_init(model.params)\n",
    "    \n",
    "    history = {\"epoch\": [], \"train_loss\": [], \"test_E_loss\": [], \"test_F_loss\": []}\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        (loss, (E_loss, F_loss)), grads = jax.value_and_grad(loss_fn, has_aux=True)(\n",
    "            get_params(opt_state), positions, E_train, F_train\n",
    "        )\n",
    "        \n",
    "        grad_norm = jnp.sqrt(sum(jnp.sum(g**2) for g in jax.tree.leaves(grads)))\n",
    "        if grad_norm > 10.0:\n",
    "            grads = jax.tree.map(lambda g: g * (10.0 / grad_norm), grads)\n",
    "        \n",
    "        opt_state = opt_update(epoch, grads, opt_state)\n",
    "        \n",
    "        if (epoch + 1) % max(1, n_epochs // 20) == 0:\n",
    "            history[\"epoch\"].append(epoch + 1)\n",
    "            history[\"train_loss\"].append(float(loss))\n",
    "            history[\"test_E_loss\"].append(float(E_loss))\n",
    "            history[\"test_F_loss\"].append(float(F_loss))\n",
    "    \n",
    "    model.set_params(get_params(opt_state))\n",
    "    return history\n",
    "\n",
    "\n",
    "def train_classical_equivariant(model, positions, E_train, F_train, n_epochs=200, \n",
    "                                 lr=3e-3, wE=1.0, wF_max=2.0, warmup_frac=0.3):\n",
    "    \"\"\"\n",
    "    Train the classical equivariant NN model with two-phase training.\n",
    "    \n",
    "    Phase 1 (warmup): Energy-only training to establish good features\n",
    "    Phase 2: Combined energy + forces with gradual force weight ramp\n",
    "    \"\"\"\n",
    "    warmup_epochs = int(n_epochs * warmup_frac)\n",
    "    \n",
    "    def huber_loss(pred, target, delta=0.5):\n",
    "        \"\"\"Huber loss - robust to outliers.\"\"\"\n",
    "        diff = pred - target\n",
    "        abs_diff = jnp.abs(diff)\n",
    "        return jnp.mean(jnp.where(abs_diff <= delta, \n",
    "                                   0.5 * diff**2, \n",
    "                                   delta * (abs_diff - 0.5 * delta)))\n",
    "    \n",
    "    @jax.jit\n",
    "    def loss_fn(params, positions, E_target, F_target, wF):\n",
    "        E_pred = model.vec_energy(positions, params)\n",
    "        E_loss = jnp.mean((E_pred - E_target) ** 2)\n",
    "        \n",
    "        F_pred = model.vec_force(positions, params)\n",
    "        F_pred_z = F_pred[:, 1, 2]  # H atom z-component\n",
    "        F_loss = huber_loss(F_pred_z, F_target, delta=0.5)\n",
    "        \n",
    "        # Handle NaNs\n",
    "        E_loss = jnp.where(jnp.isnan(E_loss), 1.0, E_loss)\n",
    "        F_loss = jnp.where(jnp.isnan(F_loss), 1.0, F_loss)\n",
    "        \n",
    "        return wE * E_loss + wF * F_loss, (E_loss, F_loss)\n",
    "    \n",
    "    opt_init, opt_update, get_params = optimizers.adam(lr)\n",
    "    opt_state = opt_init(model.params)\n",
    "    \n",
    "    history = {\"epoch\": [], \"train_loss\": [], \"test_E_loss\": [], \"test_F_loss\": []}\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # Two-phase training:\n",
    "        # Phase 1 (warmup): Energy only (wF = 0)\n",
    "        # Phase 2: Gradually increase force weight\n",
    "        if epoch < warmup_epochs:\n",
    "            wF = 0.0  # Energy-only warmup\n",
    "        else:\n",
    "            # Ramp force weight from 0 to wF_max over first half of phase 2\n",
    "            phase2_progress = (epoch - warmup_epochs) / max(1, (n_epochs - warmup_epochs) / 2)\n",
    "            wF = min(wF_max, wF_max * phase2_progress)\n",
    "        \n",
    "        (loss, (E_loss, F_loss)), grads = jax.value_and_grad(loss_fn, has_aux=True)(\n",
    "            get_params(opt_state), positions, E_train, F_train, wF\n",
    "        )\n",
    "        \n",
    "        # Gradient clipping\n",
    "        grad_norm = jnp.sqrt(sum(jnp.sum(jnp.square(g)) for g in jax.tree.leaves(grads) if g is not None))\n",
    "        if grad_norm > 5.0:\n",
    "            grads = jax.tree.map(lambda g: g * (5.0 / grad_norm) if g is not None else g, grads)\n",
    "        \n",
    "        opt_state = opt_update(epoch, grads, opt_state)\n",
    "        \n",
    "        if (epoch + 1) % max(1, n_epochs // 20) == 0:\n",
    "            history[\"epoch\"].append(epoch + 1)\n",
    "            history[\"train_loss\"].append(float(loss))\n",
    "            history[\"test_E_loss\"].append(float(E_loss))\n",
    "            history[\"test_F_loss\"].append(float(F_loss))\n",
    "    \n",
    "    model.set_params(get_params(opt_state))\n",
    "    return history\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# EVALUATION FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def evaluate_fold(E_pred_train, E_pred_test, F_pred_train, F_pred_test,\n",
    "                  E_train_true, E_test_true, F_train_true, F_test_true,\n",
    "                  energy_scaler, force_scaler):\n",
    "    \"\"\"Evaluate predictions on a single fold with post-correction.\"\"\"\n",
    "    \n",
    "    # Post-correction for energy (quadratic fit)\n",
    "    def corr_E(E, a, b, c):\n",
    "        return a * E**2 + b * E + c\n",
    "    \n",
    "    try:\n",
    "        popt_E, _ = curve_fit(corr_E, E_pred_train, E_train_true, maxfev=5000)\n",
    "        E_pred_test_corr = corr_E(E_pred_test, *popt_E)\n",
    "    except:\n",
    "        E_pred_test_corr = E_pred_test\n",
    "    \n",
    "    # Post-correction for forces (linear fit)\n",
    "    try:\n",
    "        lr_model = LinearRegression()\n",
    "        lr_model.fit(F_pred_train.reshape(-1, 1), F_train_true.reshape(-1, 1))\n",
    "        F_pred_test_corr = lr_model.predict(F_pred_test.reshape(-1, 1)).flatten()\n",
    "    except:\n",
    "        F_pred_test_corr = F_pred_test\n",
    "    \n",
    "    # Inverse transform to original units\n",
    "    E_pred_final = energy_scaler.inverse_transform(E_pred_test_corr.reshape(-1, 1)).flatten()\n",
    "    E_true_final = energy_scaler.inverse_transform(E_test_true.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    F_pred_final = force_scaler.inverse_transform(F_pred_test_corr.reshape(-1, 1)).flatten()\n",
    "    F_true_final = force_scaler.inverse_transform(F_test_true.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Compute metrics\n",
    "    E_mae = np.mean(np.abs(E_pred_final - E_true_final))\n",
    "    E_rmse = np.sqrt(np.mean((E_pred_final - E_true_final)**2))\n",
    "    ss_res_E = np.sum((E_pred_final - E_true_final)**2)\n",
    "    ss_tot_E = np.sum((E_true_final - E_true_final.mean())**2)\n",
    "    E_r2 = 1 - ss_res_E / ss_tot_E if ss_tot_E > 0 else 0\n",
    "    \n",
    "    F_mae = np.mean(np.abs(F_pred_final - F_true_final))\n",
    "    F_rmse = np.sqrt(np.mean((F_pred_final - F_true_final)**2))\n",
    "    ss_res_F = np.sum((F_pred_final - F_true_final)**2)\n",
    "    ss_tot_F = np.sum((F_true_final - F_true_final.mean())**2)\n",
    "    F_r2 = 1 - ss_res_F / ss_tot_F if ss_tot_F > 0 else 0\n",
    "    \n",
    "    metrics = {\n",
    "        \"E_r2\": float(E_r2),\n",
    "        \"E_mae_Ha\": float(E_mae),\n",
    "        \"E_rmse_Ha\": float(E_rmse),\n",
    "        \"E_mae_eV\": float(E_mae * 27.2114),\n",
    "        \"F_r2\": float(F_r2),\n",
    "        \"F_mae\": float(F_mae),\n",
    "        \"F_rmse\": float(F_rmse),\n",
    "    }\n",
    "    \n",
    "    predictions = {\n",
    "        \"E_pred\": E_pred_final.tolist(),\n",
    "        \"E_true\": E_true_final.tolist(),\n",
    "        \"F_pred\": F_pred_final.tolist(),\n",
    "        \"F_true\": F_true_final.tolist(),\n",
    "    }\n",
    "    \n",
    "    return metrics, predictions\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# K-FOLD CROSS-VALIDATION FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def run_kfold_cv(k_folds, n_epochs, data_dir, output_dir, seed=42):\n",
    "    \"\"\"\n",
    "    Run k-fold cross-validation for all four methods on LiH.\n",
    "    \n",
    "    Args:\n",
    "        k_folds: Number of folds for cross-validation\n",
    "        n_epochs: Number of training epochs per fold\n",
    "        data_dir: Directory containing LiH dataset\n",
    "        output_dir: Directory to save results\n",
    "        seed: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with all results\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"LiH K-FOLD CROSS-VALIDATION COMPARISON\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"K-Folds: {k_folds}\")\n",
    "    print(f\"Epochs per fold: {n_epochs}\")\n",
    "    print(f\"Data directory: {data_dir}\")\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Load data\n",
    "    print(\"Loading LiH dataset...\")\n",
    "    energy = np.load(os.path.join(data_dir, \"Energy.npy\"))\n",
    "    forces = np.load(os.path.join(data_dir, \"Forces.npy\"))\n",
    "    positions = np.load(os.path.join(data_dir, \"Positions.npy\"))\n",
    "    \n",
    "    N_samples = len(energy)\n",
    "    print(f\"  Loaded {N_samples} samples\")\n",
    "    print(f\"  Positions shape: {positions.shape}\")\n",
    "    print(f\"  Energy shape: {energy.shape}\")\n",
    "    print(f\"  Forces shape: {forces.shape}\")\n",
    "    \n",
    "    # Prepare scalers (fit on entire dataset for consistency)\n",
    "    energy_scaler = MinMaxScaler((-1, 1))\n",
    "    if energy.ndim == 1:\n",
    "        energy = energy.reshape(-1, 1)\n",
    "    energy_scaled = energy_scaler.fit_transform(energy).flatten()\n",
    "    \n",
    "    # Forces: H atom z-component\n",
    "    forces_H = forces[:, 1:, :]\n",
    "    force_scaler = MinMaxScaler((-1, 1))\n",
    "    forces_z = forces_H[:, 0, 2].reshape(-1, 1)\n",
    "    forces_z_scaled = force_scaler.fit_transform(forces_z).flatten()\n",
    "    \n",
    "    # Centered positions for equivariant model (H relative to Li)\n",
    "    positions_centered = np.zeros((N_samples, 1, 3))\n",
    "    positions_centered[:, 0, :] = positions[:, 1, :] - positions[:, 0, :]\n",
    "    \n",
    "    # Initialize k-fold\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=seed)\n",
    "    \n",
    "    # Results storage\n",
    "    results = {\n",
    "        \"config\": {\n",
    "            \"k_folds\": k_folds,\n",
    "            \"n_epochs\": n_epochs,\n",
    "            \"n_samples\": N_samples,\n",
    "            \"seed\": seed,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "        },\n",
    "        \"rotationally_equivariant\": {\"folds\": [], \"summary\": {}},\n",
    "        \"non_equivariant\": {\"folds\": [], \"summary\": {}},\n",
    "        \"graph_permutation_equivariant\": {\"folds\": [], \"summary\": {}},\n",
    "        \"classical_equivariant\": {\"folds\": [], \"summary\": {}},\n",
    "    }\n",
    "    \n",
    "    method_names = [\n",
    "        \"rotationally_equivariant\",\n",
    "        \"non_equivariant\", \n",
    "        \"graph_permutation_equivariant\",\n",
    "        \"classical_equivariant\"\n",
    "    ]\n",
    "    \n",
    "    # Run k-fold CV\n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(kf.split(positions)):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"FOLD {fold_idx + 1}/{k_folds}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"  Train samples: {len(train_idx)}, Test samples: {len(test_idx)}\")\n",
    "        \n",
    "        fold_seed = seed + fold_idx * 100\n",
    "        \n",
    "        # Prepare fold data\n",
    "        E_train = energy_scaled[train_idx]\n",
    "        E_test = energy_scaled[test_idx]\n",
    "        F_train = forces_z_scaled[train_idx]\n",
    "        F_test = forces_z_scaled[test_idx]\n",
    "        \n",
    "        # Centered positions for equivariant model\n",
    "        pos_centered_train = jnp.array(positions_centered[train_idx])\n",
    "        pos_centered_test = jnp.array(positions_centered[test_idx])\n",
    "        \n",
    "        # Raw positions for other models\n",
    "        pos_raw_train = jnp.array(positions[train_idx])\n",
    "        pos_raw_test = jnp.array(positions[test_idx])\n",
    "        \n",
    "        # ==================== 1. Rotationally Equivariant ====================\n",
    "        print(f\"\\n  [1/4] Rotationally Equivariant QML...\")\n",
    "        rot_model = RotationallyEquivariantQML(n_qubits=6, depth=6, seed=fold_seed)\n",
    "        \n",
    "        rot_history = train_rotationally_equivariant(\n",
    "            rot_model, pos_centered_train, jnp.array(E_train), jnp.array(F_train),\n",
    "            n_epochs=n_epochs, lr=3e-3, wE=1.0, wF_max=5.0, warmup_frac=0.4\n",
    "        )\n",
    "        \n",
    "        # Evaluate with head transformation\n",
    "        params = rot_model.params\n",
    "        E_raw_train = np.array(rot_model.vec_circuit(pos_centered_train, params))\n",
    "        E_raw_test = np.array(rot_model.vec_circuit(pos_centered_test, params))\n",
    "        E_pred_train_rot = float(params[\"head_scale\"]) * E_raw_train + float(params[\"head_bias\"])\n",
    "        E_pred_test_rot = float(params[\"head_scale\"]) * E_raw_test + float(params[\"head_bias\"])\n",
    "        \n",
    "        def raw_energy(coords, params):\n",
    "            return rot_model.circuit(coords, params)\n",
    "        def force_single(coords, params):\n",
    "            return -jax.grad(raw_energy, argnums=0)(coords, params)\n",
    "        vec_force_rot = jax.vmap(force_single, (0, None), 0)\n",
    "        \n",
    "        F_raw_train = np.array(vec_force_rot(pos_centered_train, params))[:, 0, 2]\n",
    "        F_raw_test = np.array(vec_force_rot(pos_centered_test, params))[:, 0, 2]\n",
    "        F_pred_train_rot = float(params[\"head_scale\"]) * F_raw_train\n",
    "        F_pred_test_rot = float(params[\"head_scale\"]) * F_raw_test\n",
    "        \n",
    "        rot_metrics, rot_preds = evaluate_fold(\n",
    "            E_pred_train_rot, E_pred_test_rot, F_pred_train_rot, F_pred_test_rot,\n",
    "            E_train, E_test, F_train, F_test,\n",
    "            energy_scaler, force_scaler\n",
    "        )\n",
    "        \n",
    "        results[\"rotationally_equivariant\"][\"folds\"].append({\n",
    "            \"fold\": fold_idx + 1,\n",
    "            \"metrics\": rot_metrics,\n",
    "            \"history\": rot_history,\n",
    "        })\n",
    "        print(f\"    Energy R²: {rot_metrics['E_r2']:.4f}, Force R²: {rot_metrics['F_r2']:.4f}\")\n",
    "        \n",
    "        # ==================== 2. Non-Equivariant ====================\n",
    "        print(f\"\\n  [2/4] Non-Equivariant QML...\")\n",
    "        non_eq_model = NonEquivariantQML(n_qubits=4, depth=4, seed=fold_seed)\n",
    "        \n",
    "        non_eq_history = train_non_equivariant(\n",
    "            non_eq_model, pos_raw_train, jnp.array(E_train), jnp.array(F_train),\n",
    "            n_epochs=n_epochs, lr=0.01, lambda_E=2.0, lambda_F=1.0\n",
    "        )\n",
    "        \n",
    "        # Evaluate\n",
    "        E_pred_train_ne = np.array(non_eq_model.vec_circuit(pos_raw_train, non_eq_model.params))\n",
    "        E_pred_test_ne = np.array(non_eq_model.vec_circuit(pos_raw_test, non_eq_model.params))\n",
    "        \n",
    "        def energy_single_ne(pos, params):\n",
    "            return non_eq_model.circuit(pos, params)\n",
    "        def force_single_ne(pos, params):\n",
    "            return -jax.grad(energy_single_ne, argnums=0)(pos, params)\n",
    "        vec_force_ne = jax.vmap(force_single_ne, (0, None), 0)\n",
    "        \n",
    "        F_pred_train_ne = np.array(vec_force_ne(pos_raw_train, non_eq_model.params))[:, 1, 2]\n",
    "        F_pred_test_ne = np.array(vec_force_ne(pos_raw_test, non_eq_model.params))[:, 1, 2]\n",
    "        \n",
    "        non_eq_metrics, non_eq_preds = evaluate_fold(\n",
    "            E_pred_train_ne, E_pred_test_ne, F_pred_train_ne, F_pred_test_ne,\n",
    "            E_train, E_test, F_train, F_test,\n",
    "            energy_scaler, force_scaler\n",
    "        )\n",
    "        \n",
    "        results[\"non_equivariant\"][\"folds\"].append({\n",
    "            \"fold\": fold_idx + 1,\n",
    "            \"metrics\": non_eq_metrics,\n",
    "            \"history\": non_eq_history,\n",
    "        })\n",
    "        print(f\"    Energy R²: {non_eq_metrics['E_r2']:.4f}, Force R²: {non_eq_metrics['F_r2']:.4f}\")\n",
    "        \n",
    "        # ==================== 3. Graph Permutation Equivariant ====================\n",
    "        print(f\"\\n  [3/4] Graph Permutation Equivariant QML...\")\n",
    "        graph_model = GraphPermutationEquivariantQML(n_qubits=4, depth=4, seed=fold_seed)\n",
    "        \n",
    "        graph_history = train_graph_permutation(\n",
    "            graph_model, pos_raw_train, jnp.array(E_train), jnp.array(F_train),\n",
    "            n_epochs=n_epochs, lr=0.01, lambda_E=1.5, lambda_F=1.5\n",
    "        )\n",
    "        \n",
    "        # Evaluate\n",
    "        E_pred_train_g = np.array(graph_model.vec_circuit(pos_raw_train, graph_model.params))\n",
    "        E_pred_test_g = np.array(graph_model.vec_circuit(pos_raw_test, graph_model.params))\n",
    "        \n",
    "        def energy_single_g(pos, params):\n",
    "            return graph_model.circuit(pos, params)\n",
    "        def force_single_g(pos, params):\n",
    "            return -jax.grad(energy_single_g, argnums=0)(pos, params)\n",
    "        vec_force_g = jax.vmap(force_single_g, (0, None), 0)\n",
    "        \n",
    "        F_pred_train_g = np.array(vec_force_g(pos_raw_train, graph_model.params))[:, 1, 2]\n",
    "        F_pred_test_g = np.array(vec_force_g(pos_raw_test, graph_model.params))[:, 1, 2]\n",
    "        \n",
    "        graph_metrics, graph_preds = evaluate_fold(\n",
    "            E_pred_train_g, E_pred_test_g, F_pred_train_g, F_pred_test_g,\n",
    "            E_train, E_test, F_train, F_test,\n",
    "            energy_scaler, force_scaler\n",
    "        )\n",
    "        \n",
    "        results[\"graph_permutation_equivariant\"][\"folds\"].append({\n",
    "            \"fold\": fold_idx + 1,\n",
    "            \"metrics\": graph_metrics,\n",
    "            \"history\": graph_history,\n",
    "        })\n",
    "        print(f\"    Energy R²: {graph_metrics['E_r2']:.4f}, Force R²: {graph_metrics['F_r2']:.4f}\")\n",
    "        \n",
    "        # ==================== 4. Classical Equivariant ====================\n",
    "        print(f\"\\n  [4/4] Classical Rotationally Equivariant NN...\")\n",
    "        classical_model = ClassicalRotationallyEquivariantNN(hidden_dims=[128, 128, 64], seed=fold_seed)\n",
    "        \n",
    "        classical_history = train_classical_equivariant(\n",
    "            classical_model, pos_raw_train, jnp.array(E_train), jnp.array(F_train),\n",
    "            n_epochs=n_epochs, lr=3e-3, wE=1.0, wF_max=2.0, warmup_frac=0.3\n",
    "        )\n",
    "        \n",
    "        # Evaluate\n",
    "        E_pred_train_c = np.array(classical_model.vec_energy(pos_raw_train, classical_model.params))\n",
    "        E_pred_test_c = np.array(classical_model.vec_energy(pos_raw_test, classical_model.params))\n",
    "        F_pred_train_c = np.array(classical_model.vec_force(pos_raw_train, classical_model.params))[:, 1, 2]\n",
    "        F_pred_test_c = np.array(classical_model.vec_force(pos_raw_test, classical_model.params))[:, 1, 2]\n",
    "        \n",
    "        classical_metrics, classical_preds = evaluate_fold(\n",
    "            E_pred_train_c, E_pred_test_c, F_pred_train_c, F_pred_test_c,\n",
    "            E_train, E_test, F_train, F_test,\n",
    "            energy_scaler, force_scaler\n",
    "        )\n",
    "        \n",
    "        results[\"classical_equivariant\"][\"folds\"].append({\n",
    "            \"fold\": fold_idx + 1,\n",
    "            \"metrics\": classical_metrics,\n",
    "            \"history\": classical_history,\n",
    "        })\n",
    "        print(f\"    Energy R²: {classical_metrics['E_r2']:.4f}, Force R²: {classical_metrics['F_r2']:.4f}\")\n",
    "    \n",
    "    # Compute summary statistics\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"COMPUTING SUMMARY STATISTICS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    metrics_keys = [\"E_r2\", \"E_mae_Ha\", \"E_rmse_Ha\", \"F_r2\", \"F_mae\", \"F_rmse\"]\n",
    "    \n",
    "    for method in method_names:\n",
    "        folds_data = results[method][\"folds\"]\n",
    "        summary = {}\n",
    "        \n",
    "        for metric in metrics_keys:\n",
    "            values = [fold[\"metrics\"][metric] for fold in folds_data]\n",
    "            summary[metric] = {\n",
    "                \"mean\": float(np.mean(values)),\n",
    "                \"std\": float(np.std(values)),\n",
    "                \"min\": float(np.min(values)),\n",
    "                \"max\": float(np.max(values)),\n",
    "                \"values\": values,\n",
    "            }\n",
    "        \n",
    "        # Coefficient of variation\n",
    "        for metric in [\"E_r2\", \"F_r2\"]:\n",
    "            values = summary[metric][\"values\"]\n",
    "            mean = summary[metric][\"mean\"]\n",
    "            if mean != 0:\n",
    "                summary[f\"{metric}_cv\"] = float(np.std(values) / abs(mean))\n",
    "            else:\n",
    "                summary[f\"{metric}_cv\"] = float('inf')\n",
    "        \n",
    "        results[method][\"summary\"] = summary\n",
    "    \n",
    "    # Print summary table\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"K-FOLD CROSS-VALIDATION SUMMARY\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    print(f\"\\n{'Method':<35} {'E_R² (mean±std)':<20} {'F_R² (mean±std)':<20} {'E_CV':<10} {'F_CV':<10}\")\n",
    "    print(\"-\"*100)\n",
    "    \n",
    "    method_labels = {\n",
    "        \"rotationally_equivariant\": \"Rot. Equiv. QML\",\n",
    "        \"non_equivariant\": \"Non-Equiv. QML\",\n",
    "        \"graph_permutation_equivariant\": \"Graph Perm. QML\",\n",
    "        \"classical_equivariant\": \"Classical Equiv. NN\",\n",
    "    }\n",
    "    \n",
    "    for method in method_names:\n",
    "        summary = results[method][\"summary\"]\n",
    "        e_r2 = summary[\"E_r2\"]\n",
    "        f_r2 = summary[\"F_r2\"]\n",
    "        e_cv = summary.get(\"E_r2_cv\", 0)\n",
    "        f_cv = summary.get(\"F_r2_cv\", 0)\n",
    "        \n",
    "        print(f\"{method_labels[method]:<35} \"\n",
    "              f\"{e_r2['mean']:.4f}±{e_r2['std']:.4f}       \"\n",
    "              f\"{f_r2['mean']:.4f}±{f_r2['std']:.4f}       \"\n",
    "              f\"{e_cv:.4f}     {f_cv:.4f}\")\n",
    "    \n",
    "    print(\"=\"*100)\n",
    "    print(\"CV = Coefficient of Variation (lower = more consistent across folds)\")\n",
    "    \n",
    "    # Save results\n",
    "    results_path = os.path.join(output_dir, \"kfold_results.json\")\n",
    "    with open(results_path, \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    print(f\"\\nResults saved to: {results_path}\")\n",
    "    \n",
    "    # Save NPZ\n",
    "    npz_data = {}\n",
    "    for method in method_names:\n",
    "        for metric in metrics_keys:\n",
    "            key = f\"{method}_{metric}\"\n",
    "            npz_data[key] = np.array(results[method][\"summary\"][metric][\"values\"])\n",
    "    \n",
    "    npz_path = os.path.join(output_dir, \"kfold_metrics.npz\")\n",
    "    np.savez(npz_path, **npz_data)\n",
    "    print(f\"Metrics saved to: {npz_path}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def main(k_folds=5, n_epochs=200, output_dir=\"kfold_results_lih\", \n",
    "         data_dir=\"eqnn_force_field_data_LiH\", seed=42):\n",
    "    \"\"\"\n",
    "    Main function for k-fold cross-validation on LiH.\n",
    "    \n",
    "    Args:\n",
    "        k_folds: Number of folds (default: 5)\n",
    "        n_epochs: Epochs per fold (default: 200)\n",
    "        output_dir: Output directory\n",
    "        data_dir: Data directory\n",
    "        seed: Random seed\n",
    "    \n",
    "    Returns:\n",
    "        Results dictionary\n",
    "    \"\"\"\n",
    "    return run_kfold_cv(k_folds, n_epochs, data_dir, output_dir, seed)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    \n",
    "    if 'ipykernel' in sys.modules:\n",
    "        print(\"Running in Jupyter. Call main() with parameters:\")\n",
    "        print(\"  results = main(k_folds=5, n_epochs=200, output_dir='kfold_results_lih')\")\n",
    "    else:\n",
    "        parser = argparse.ArgumentParser(description=\"LiH K-Fold Cross-Validation Comparison\")\n",
    "        parser.add_argument(\"--k_folds\", type=int, default=5, help=\"Number of folds\")\n",
    "        parser.add_argument(\"--n_epochs\", type=int, default=200, help=\"Epochs per fold\")\n",
    "        parser.add_argument(\"--output_dir\", type=str, default=\"kfold_results_lih\", help=\"Output directory\")\n",
    "        parser.add_argument(\"--data_dir\", type=str, default=\"eqnn_force_field_data_LiH\", help=\"Data directory\")\n",
    "        parser.add_argument(\"--seed\", type=int, default=42, help=\"Random seed\")\n",
    "        \n",
    "        args = parser.parse_args()\n",
    "        \n",
    "        main(\n",
    "            k_folds=args.k_folds,\n",
    "            n_epochs=args.n_epochs,\n",
    "            output_dir=args.output_dir,\n",
    "            data_dir=args.data_dir,\n",
    "            seed=args.seed\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6464d32-14a8-4150-ad69-f9e7076ea088",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:2025-12-03 03:49:55,504:jax._src.xla_bridge:850: An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LiH K-FOLD CROSS-VALIDATION COMPARISON\n",
      "================================================================================\n",
      "K-Folds: 5\n",
      "Epochs per fold: 200\n",
      "Data directory: eqnn_force_field_data_LiH\n",
      "Output directory: kfold_results_lih\n",
      "================================================================================\n",
      "\n",
      "Loading LiH dataset...\n",
      "  Loaded 2400 samples\n",
      "  Positions shape: (2400, 2, 3)\n",
      "  Energy shape: (2400,)\n",
      "  Forces shape: (2400, 2, 3)\n",
      "\n",
      "============================================================\n",
      "FOLD 1/5\n",
      "============================================================\n",
      "  Train samples: 1920, Test samples: 480\n",
      "\n",
      "  [1/4] Rotationally Equivariant QML...\n",
      "    Energy R²: 0.9963, Force R²: 0.9911\n",
      "\n",
      "  [2/4] Non-Equivariant QML...\n",
      "    Energy R²: 0.9976, Force R²: 0.8255\n",
      "\n",
      "  [3/4] Graph Permutation Equivariant QML...\n",
      "    Energy R²: 0.9965, Force R²: 0.9612\n",
      "\n",
      "  [4/4] Classical Rotationally Equivariant NN...\n",
      "    Energy R²: 0.9980, Force R²: 0.9955\n",
      "\n",
      "============================================================\n",
      "FOLD 2/5\n",
      "============================================================\n",
      "  Train samples: 1920, Test samples: 480\n",
      "\n",
      "  [1/4] Rotationally Equivariant QML...\n",
      "    Energy R²: 0.9969, Force R²: 0.9975\n",
      "\n",
      "  [2/4] Non-Equivariant QML...\n",
      "    Energy R²: 0.9974, Force R²: 0.7992\n",
      "\n",
      "  [3/4] Graph Permutation Equivariant QML...\n",
      "    Energy R²: 0.9967, Force R²: 0.9735\n",
      "\n",
      "  [4/4] Classical Rotationally Equivariant NN...\n",
      "    Energy R²: 0.9979, Force R²: 0.9943\n",
      "\n",
      "============================================================\n",
      "FOLD 3/5\n",
      "============================================================\n",
      "  Train samples: 1920, Test samples: 480\n",
      "\n",
      "  [1/4] Rotationally Equivariant QML...\n",
      "    Energy R²: 0.9959, Force R²: 0.9955\n",
      "\n",
      "  [2/4] Non-Equivariant QML...\n",
      "    Energy R²: 0.9966, Force R²: 0.8664\n",
      "\n",
      "  [3/4] Graph Permutation Equivariant QML...\n",
      "    Energy R²: 0.9969, Force R²: 0.9906\n",
      "\n",
      "  [4/4] Classical Rotationally Equivariant NN...\n",
      "    Energy R²: 0.9974, Force R²: 0.9960\n",
      "\n",
      "============================================================\n",
      "FOLD 4/5\n",
      "============================================================\n",
      "  Train samples: 1920, Test samples: 480\n",
      "\n",
      "  [1/4] Rotationally Equivariant QML...\n",
      "    Energy R²: 0.9942, Force R²: 0.9742\n",
      "\n",
      "  [2/4] Non-Equivariant QML...\n",
      "    Energy R²: 0.9961, Force R²: 0.8918\n",
      "\n",
      "  [3/4] Graph Permutation Equivariant QML...\n",
      "    Energy R²: 0.9962, Force R²: 0.9910\n",
      "\n",
      "  [4/4] Classical Rotationally Equivariant NN...\n",
      "    Energy R²: 0.9969, Force R²: 0.9962\n",
      "\n",
      "============================================================\n",
      "FOLD 5/5\n",
      "============================================================\n",
      "  Train samples: 1920, Test samples: 480\n",
      "\n",
      "  [1/4] Rotationally Equivariant QML...\n",
      "    Energy R²: 0.9967, Force R²: 0.9957\n",
      "\n",
      "  [2/4] Non-Equivariant QML...\n",
      "    Energy R²: 0.9970, Force R²: 0.8037\n",
      "\n",
      "  [3/4] Graph Permutation Equivariant QML...\n",
      "    Energy R²: 0.9976, Force R²: 0.9891\n",
      "\n",
      "  [4/4] Classical Rotationally Equivariant NN...\n",
      "    Energy R²: 0.9978, Force R²: 0.9930\n",
      "\n",
      "================================================================================\n",
      "COMPUTING SUMMARY STATISTICS\n",
      "================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "K-FOLD CROSS-VALIDATION SUMMARY\n",
      "====================================================================================================\n",
      "\n",
      "Method                              E_R² (mean±std)      F_R² (mean±std)      E_CV       F_CV      \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Rot. Equiv. QML                     0.9960±0.0010       0.9908±0.0085       0.0010     0.0086\n",
      "Non-Equiv. QML                      0.9970±0.0005       0.8373±0.0362       0.0005     0.0432\n",
      "Graph Perm. QML                     0.9968±0.0005       0.9811±0.0119       0.0005     0.0121\n",
      "Classical Equiv. NN                 0.9976±0.0004       0.9950±0.0012       0.0004     0.0012\n",
      "====================================================================================================\n",
      "CV = Coefficient of Variation (lower = more consistent across folds)\n",
      "\n",
      "Results saved to: kfold_results_lih/kfold_results.json\n",
      "Metrics saved to: kfold_results_lih/kfold_metrics.npz\n"
     ]
    }
   ],
   "source": [
    "results = main(k_folds=5, n_epochs=200, output_dir='kfold_results_lih')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a196abf5-8b3b-40fd-ab10-c5b5612f4d48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
