{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6538f18-9d10-4e96-b8c7-9480b9a29f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saumya/.conda/envs/penny_skl/lib/python3.12/site-packages/pennylane/__init__.py:209: RuntimeWarning: PennyLane is not yet compatible with JAX versions > 0.6.2. You have version 0.8.0 installed. Please downgrade JAX to 0.6.2 to avoid runtime errors using python -m pip install jax~=0.6.0 jaxlib~=0.6.0\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in Jupyter. Call main() with parameters:\n",
      "  results = main(k_folds=5, n_epochs=300, output_dir='kfold_results')\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "NH₃ QML K-Fold Cross-Validation Comparison\n",
    "============================================\n",
    "\n",
    "Compares generalizability of four approaches using k-fold cross-validation:\n",
    "1. Rotationally Equivariant QML - EQNN-style with SO(3) equivariant encoding\n",
    "2. Non-Equivariant QML - Simple QNN with basic rotations (no symmetry)\n",
    "3. Graph Permutation Equivariant QML - Graph-based encoding with permutation symmetry\n",
    "4. Classical Rotationally Equivariant NN - Classical MLP on pairwise distances (E(3) invariant)\n",
    "\n",
    "This script evaluates model generalizability by:\n",
    "- Splitting data into k folds\n",
    "- Training on k-1 folds, testing on held-out fold\n",
    "- Repeating for each fold\n",
    "- Computing variance across folds (measures sensitivity to data splits)\n",
    "\n",
    "Usage (command line):\n",
    "    python run_kfold_comparison_nh3.py --k_folds 5 --n_epochs 300 --output_dir kfold_results\n",
    "\n",
    "Usage (Jupyter):\n",
    "    from run_kfold_comparison_nh3 import main\n",
    "    results = main(k_folds=5, n_epochs=300, output_dir='kfold_results')\n",
    "\"\"\"\n",
    "\n",
    "import pennylane as qml\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import jax\n",
    "jax.config.update('jax_platform_name', 'cpu')\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "from jax import numpy as jnp\n",
    "from jax.example_libraries import optimizers\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.optimize import curve_fit\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def huber(residual, delta=1.0):\n",
    "    \"\"\"Elementwise Huber loss for robust force training.\"\"\"\n",
    "    abs_r = jnp.abs(residual)\n",
    "    quad = 0.5 * residual**2\n",
    "    lin = delta * (abs_r - 0.5 * delta)\n",
    "    return jnp.where(abs_r <= delta, quad, lin)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 1. ROTATIONALLY EQUIVARIANT QML MODEL (SO(3))\n",
    "# =============================================================================\n",
    "\n",
    "class RotationallyEquivariantQML:\n",
    "    \"\"\"Rotationally Equivariant QML for NH₃.\"\"\"\n",
    "    \n",
    "    def __init__(self, depth=6, rep=2, active_atoms=3, seed=42):\n",
    "        self.depth = depth\n",
    "        self.rep = rep\n",
    "        self.active_atoms = active_atoms\n",
    "        self.n_qubits = active_atoms * rep\n",
    "        self.seed = seed\n",
    "        \n",
    "        self.dev = qml.device(\"default.qubit\", wires=self.n_qubits)\n",
    "        self.observable = (\n",
    "            qml.PauliX(0) @ qml.PauliX(1)\n",
    "            + qml.PauliY(0) @ qml.PauliY(1)\n",
    "            + qml.PauliZ(0) @ qml.PauliZ(1)\n",
    "        )\n",
    "        \n",
    "        self._build_circuit()\n",
    "        self._init_params()\n",
    "    \n",
    "    def _singlet(self, wires):\n",
    "        w0, w1 = wires\n",
    "        qml.Hadamard(wires=w0)\n",
    "        qml.PauliZ(wires=w0)\n",
    "        qml.PauliX(wires=w1)\n",
    "        qml.CNOT(wires=[w0, w1])\n",
    "    \n",
    "    def _equivariant_encoding(self, alpha, vec3, wire):\n",
    "        r = jnp.array(vec3, dtype=jnp.float64)\n",
    "        norm = jnp.linalg.norm(r) + 1e-12\n",
    "        n = r / norm\n",
    "        theta = alpha * norm\n",
    "        qml.Rot(theta * n[0], theta * n[1], theta * n[2], wires=wire)\n",
    "    \n",
    "    def _pair_layer(self, weight, wires):\n",
    "        qml.IsingXX(weight, wires=wires)\n",
    "        qml.IsingYY(weight, wires=wires)\n",
    "        qml.IsingZZ(weight, wires=wires)\n",
    "    \n",
    "    def _build_circuit(self):\n",
    "        @qml.qnode(self.dev, interface=\"jax\", diff_method=\"backprop\")\n",
    "        def circuit(coords, params):\n",
    "            weights = params[\"weights\"]\n",
    "            alphas = params[\"alphas\"]\n",
    "            \n",
    "            for i in range(0, self.n_qubits - 1, 2):\n",
    "                self._singlet([i, i + 1])\n",
    "            \n",
    "            for i in range(self.n_qubits):\n",
    "                self._equivariant_encoding(alphas[i, 0], coords[i % self.active_atoms], i)\n",
    "            \n",
    "            for d in range(self.depth):\n",
    "                qml.Barrier()\n",
    "                for i in range(0, self.n_qubits - 1, 2):\n",
    "                    self._pair_layer(weights[i, d], [i, (i + 1) % self.n_qubits])\n",
    "                for i in range(1, self.n_qubits, 2):\n",
    "                    self._pair_layer(weights[i, d], [i, (i + 1) % self.n_qubits])\n",
    "                for i in range(self.n_qubits):\n",
    "                    self._equivariant_encoding(alphas[i, d + 1], coords[i % self.active_atoms], i)\n",
    "            \n",
    "            return qml.expval(self.observable)\n",
    "        \n",
    "        self.circuit = circuit\n",
    "        self.vec_circuit = jax.vmap(circuit, in_axes=(0, None), out_axes=0)\n",
    "    \n",
    "    def _init_params(self):\n",
    "        np.random.seed(self.seed)\n",
    "        weights0 = np.zeros((self.n_qubits, self.depth), dtype=np.float64)\n",
    "        weights0[0] = np.random.uniform(0.0, np.pi, size=(self.depth,))\n",
    "        alphas0 = np.random.uniform(0.5, 1.5, size=(self.n_qubits, self.depth + 1))\n",
    "        \n",
    "        self.params = {\n",
    "            \"weights\": jnp.array(weights0),\n",
    "            \"alphas\": jnp.array(alphas0),\n",
    "            \"head_scale\": jnp.array(1.0),\n",
    "            \"head_bias\": jnp.array(0.0),\n",
    "        }\n",
    "    \n",
    "    def energy(self, coords, params):\n",
    "        raw = self.circuit(coords, params)\n",
    "        return params[\"head_scale\"] * raw + params[\"head_bias\"]\n",
    "    \n",
    "    def force(self, coords, params):\n",
    "        grad_fn = jax.grad(self.energy, argnums=0)\n",
    "        return -grad_fn(coords, params)\n",
    "    \n",
    "    def get_params(self):\n",
    "        return self.params\n",
    "    \n",
    "    def set_params(self, params):\n",
    "        self.params = params\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. NON-EQUIVARIANT QML MODEL\n",
    "# =============================================================================\n",
    "\n",
    "class NonEquivariantQML:\n",
    "    \"\"\"Non-Equivariant QML for NH₃.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_qubits=6, depth=4, seed=42):\n",
    "        self.n_qubits = n_qubits\n",
    "        self.depth = depth\n",
    "        self.seed = seed\n",
    "        \n",
    "        self.dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "        self._build_circuit()\n",
    "        self._init_params()\n",
    "    \n",
    "    def _build_circuit(self):\n",
    "        @qml.qnode(self.dev, interface=\"jax\", diff_method=\"backprop\")\n",
    "        def circuit(distances, params):\n",
    "            weights = params[\"weights\"]\n",
    "            \n",
    "            for i in range(self.n_qubits):\n",
    "                qml.Hadamard(wires=i)\n",
    "            \n",
    "            for i, d in enumerate(distances[:3]):\n",
    "                qml.RY(d * np.pi, wires=i)\n",
    "                qml.RY(d * np.pi, wires=i + 3)\n",
    "            \n",
    "            for layer in range(self.depth):\n",
    "                for i in range(self.n_qubits):\n",
    "                    qml.RX(weights[layer, i, 0], wires=i)\n",
    "                    qml.RY(weights[layer, i, 1], wires=i)\n",
    "                    qml.RZ(weights[layer, i, 2], wires=i)\n",
    "                for i in range(self.n_qubits - 1):\n",
    "                    qml.CNOT(wires=[i, i + 1])\n",
    "                qml.CNOT(wires=[self.n_qubits - 1, 0])\n",
    "            \n",
    "            return qml.expval(qml.PauliZ(0))\n",
    "        \n",
    "        self.circuit = circuit\n",
    "        self.vec_circuit = jax.vmap(circuit, in_axes=(0, None), out_axes=0)\n",
    "    \n",
    "    def _init_params(self):\n",
    "        np.random.seed(self.seed)\n",
    "        weights = np.random.uniform(-np.pi, np.pi, (self.depth, self.n_qubits, 3))\n",
    "        self.params = {\n",
    "            \"weights\": jnp.array(weights),\n",
    "            \"head_scale\": jnp.array(1.0),\n",
    "            \"head_bias\": jnp.array(0.0),\n",
    "        }\n",
    "    \n",
    "    def get_params(self):\n",
    "        return self.params\n",
    "    \n",
    "    def set_params(self, params):\n",
    "        self.params = params\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3. GRAPH PERMUTATION EQUIVARIANT QML MODEL\n",
    "# =============================================================================\n",
    "\n",
    "class GraphPermutationEquivariantQML:\n",
    "    \"\"\"\n",
    "    Graph Permutation Equivariant QML for NH₃.\n",
    "    Encodes N-H bonds as graph edges with geometric features.\n",
    "    Uses permutation-symmetric encoding with learnable parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_qubits=6, depth=4, seed=42):\n",
    "        self.n_qubits = n_qubits  # 3 bonds × 2 qubits per bond\n",
    "        self.depth = depth\n",
    "        self.seed = seed\n",
    "        \n",
    "        self.dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "        self._build_circuit()\n",
    "        self._init_params()\n",
    "    \n",
    "    def _build_circuit(self):\n",
    "        \"\"\"Build the graph-based quantum circuit.\"\"\"\n",
    "        \n",
    "        @qml.qnode(self.dev, interface=\"jax\", diff_method=\"backprop\")\n",
    "        def circuit(positions, params):\n",
    "            \"\"\"\n",
    "            positions: (4, 3) - [N, H1, H2, H3] coordinates\n",
    "            params: {\"weights\": (depth, n_qubits, 3)}\n",
    "            \"\"\"\n",
    "            weights = params[\"weights\"]\n",
    "            \n",
    "            # N at index 0, H atoms at indices 1, 2, 3\n",
    "            N_pos = positions[0]\n",
    "            H_positions = positions[1:]  # (3, 3)\n",
    "            \n",
    "            # Compute bond vectors and distances\n",
    "            bonds = H_positions - N_pos[None, :]  # (3, 3)\n",
    "            distances = jnp.linalg.norm(bonds, axis=1)  # (3,)\n",
    "            \n",
    "            # Compute angles between bonds\n",
    "            def compute_angle(v1, v2):\n",
    "                cos_angle = jnp.dot(v1, v2) / (jnp.linalg.norm(v1) * jnp.linalg.norm(v2) + 1e-12)\n",
    "                return jnp.arccos(jnp.clip(cos_angle, -1.0, 1.0))\n",
    "            \n",
    "            angle_01 = compute_angle(bonds[0], bonds[1])\n",
    "            angle_02 = compute_angle(bonds[0], bonds[2])\n",
    "            angle_12 = compute_angle(bonds[1], bonds[2])\n",
    "            \n",
    "            # Initialize qubits\n",
    "            for i in range(self.n_qubits):\n",
    "                qml.RY(0.5, wires=i)\n",
    "            \n",
    "            # Apply layers\n",
    "            for layer in range(self.depth):\n",
    "                # Encode bond distances (symmetric across bond pairs)\n",
    "                qml.RY(weights[layer, 0, 0] * distances[0], wires=0)\n",
    "                qml.RY(weights[layer, 1, 0] * distances[0], wires=1)\n",
    "                qml.RY(weights[layer, 2, 0] * distances[1], wires=2)\n",
    "                qml.RY(weights[layer, 3, 0] * distances[1], wires=3)\n",
    "                qml.RY(weights[layer, 4, 0] * distances[2], wires=4)\n",
    "                qml.RY(weights[layer, 5, 0] * distances[2], wires=5)\n",
    "                \n",
    "                # Entangle within bonds\n",
    "                qml.CNOT(wires=[0, 1])\n",
    "                qml.CNOT(wires=[2, 3])\n",
    "                qml.CNOT(wires=[4, 5])\n",
    "                \n",
    "                # Encode angular information\n",
    "                qml.RZ(weights[layer, 0, 1] * angle_01, wires=0)\n",
    "                qml.RZ(weights[layer, 2, 1] * angle_01, wires=2)\n",
    "                qml.RZ(weights[layer, 0, 2] * angle_02, wires=0)\n",
    "                qml.RZ(weights[layer, 4, 2] * angle_02, wires=4)\n",
    "                qml.RZ(weights[layer, 2, 2] * angle_12, wires=2)\n",
    "                qml.RZ(weights[layer, 4, 2] * angle_12, wires=4)\n",
    "                \n",
    "                # Cross-bond entanglement\n",
    "                qml.CNOT(wires=[1, 2])\n",
    "                qml.CNOT(wires=[3, 4])\n",
    "                qml.CNOT(wires=[5, 0])\n",
    "                \n",
    "                # Additional rotations\n",
    "                for i in range(self.n_qubits):\n",
    "                    qml.RZ(weights[layer, i, 1], wires=i)\n",
    "                    qml.RY(weights[layer, i, 2], wires=i)\n",
    "            \n",
    "            return qml.expval(qml.sum(*(qml.PauliZ(i) for i in range(self.n_qubits))))\n",
    "        \n",
    "        self.circuit = circuit\n",
    "        self.vec_circuit = jax.vmap(circuit, in_axes=(0, None), out_axes=0)\n",
    "    \n",
    "    def _init_params(self):\n",
    "        \"\"\"Initialize parameters.\"\"\"\n",
    "        np.random.seed(self.seed)\n",
    "        self.params = {\n",
    "            \"weights\": jnp.array(np.random.normal(0, 0.1, (self.depth, self.n_qubits, 3)))\n",
    "        }\n",
    "    \n",
    "    def get_params(self):\n",
    "        return self.params\n",
    "    \n",
    "    def set_params(self, params):\n",
    "        self.params = params\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 4. CLASSICAL ROTATIONALLY EQUIVARIANT NN (IMPROVED)\n",
    "# =============================================================================\n",
    "\n",
    "class ClassicalRotationallyEquivariantNN:\n",
    "    \"\"\"Improved Classical Rotationally Equivariant Neural Network for NH₃.\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dims=[128, 128, 64], seed=42):\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.seed = seed\n",
    "        self._init_params()\n",
    "        self._create_model()\n",
    "    \n",
    "    def _init_params(self):\n",
    "        np.random.seed(self.seed)\n",
    "        n_features = 24\n",
    "        layer_sizes = [n_features] + self.hidden_dims + [1]\n",
    "        \n",
    "        params = {\n",
    "            \"weights\": [],\n",
    "            \"biases\": [],\n",
    "            \"skip_weights\": [],\n",
    "            \"output_scale\": jnp.array(1.0),\n",
    "            \"output_bias\": jnp.array(0.0),\n",
    "        }\n",
    "        \n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            fan_in = layer_sizes[i]\n",
    "            fan_out = layer_sizes[i + 1]\n",
    "            std = np.sqrt(2.0 / fan_in)\n",
    "            W = np.random.normal(0, std, (fan_in, fan_out))\n",
    "            b = np.zeros(fan_out)\n",
    "            params[\"weights\"].append(jnp.array(W))\n",
    "            params[\"biases\"].append(jnp.array(b))\n",
    "        \n",
    "        skip_std = np.sqrt(2.0 / n_features)\n",
    "        params[\"skip_weights\"] = jnp.array(np.random.normal(0, skip_std, (n_features, self.hidden_dims[-1])))\n",
    "        params[\"rbf_coeffs\"] = jnp.array(np.random.normal(0, 0.1, (6, 8)))\n",
    "        params[\"rbf_output\"] = jnp.array(np.random.normal(0, 0.1, (6,)))\n",
    "        \n",
    "        self.params = params\n",
    "    \n",
    "    def _create_model(self):\n",
    "        def silu(x):\n",
    "            return x * jax.nn.sigmoid(x)\n",
    "        \n",
    "        def compute_features(positions):\n",
    "            eps = 1e-8\n",
    "            d_NH1 = jnp.linalg.norm(positions[1] - positions[0]) + eps\n",
    "            d_NH2 = jnp.linalg.norm(positions[2] - positions[0]) + eps\n",
    "            d_NH3 = jnp.linalg.norm(positions[3] - positions[0]) + eps\n",
    "            d_H1H2 = jnp.linalg.norm(positions[2] - positions[1]) + eps\n",
    "            d_H1H3 = jnp.linalg.norm(positions[3] - positions[1]) + eps\n",
    "            d_H2H3 = jnp.linalg.norm(positions[3] - positions[2]) + eps\n",
    "            \n",
    "            distances = jnp.array([d_NH1, d_NH2, d_NH3, d_H1H2, d_H1H3, d_H2H3])\n",
    "            dist_norm = distances / 1.5\n",
    "            inv_dist = 1.0 / distances\n",
    "            inv_dist_norm = inv_dist / 1.0\n",
    "            \n",
    "            r_eq = jnp.array([1.01, 1.01, 1.01, 1.63, 1.63, 1.63])\n",
    "            alpha = 2.0\n",
    "            morse = jnp.exp(-alpha * (distances - r_eq))\n",
    "            \n",
    "            def compute_angle(p1, p2, p_center):\n",
    "                v1 = p1 - p_center\n",
    "                v2 = p2 - p_center\n",
    "                cos_angle = jnp.dot(v1, v2) / (jnp.linalg.norm(v1) * jnp.linalg.norm(v2) + eps)\n",
    "                return jnp.arccos(jnp.clip(cos_angle, -1.0 + eps, 1.0 - eps))\n",
    "            \n",
    "            angle_H1NH2 = compute_angle(positions[1], positions[2], positions[0])\n",
    "            angle_H1NH3 = compute_angle(positions[1], positions[3], positions[0])\n",
    "            angle_H2NH3 = compute_angle(positions[2], positions[3], positions[0])\n",
    "            \n",
    "            angles = jnp.array([angle_H1NH2, angle_H1NH3, angle_H2NH3])\n",
    "            angles_norm = angles / jnp.pi\n",
    "            cos_angles = jnp.cos(angles)\n",
    "            \n",
    "            features = jnp.concatenate([dist_norm, inv_dist_norm, morse, angles_norm, cos_angles])\n",
    "            return features, distances\n",
    "        \n",
    "        def rbf_energy(distances, params):\n",
    "            centers = jnp.linspace(0.8, 2.5, 8)\n",
    "            width = 0.2\n",
    "            rbf = jnp.exp(-((distances[:, None] - centers[None, :]) ** 2) / (2 * width ** 2))\n",
    "            pair_energies = jnp.sum(rbf * params[\"rbf_coeffs\"], axis=1)\n",
    "            return jnp.dot(pair_energies, params[\"rbf_output\"])\n",
    "        \n",
    "        def mlp_forward(features, params):\n",
    "            weights = params[\"weights\"]\n",
    "            biases = params[\"biases\"]\n",
    "            h = features\n",
    "            \n",
    "            for i in range(len(weights) - 1):\n",
    "                h = jnp.dot(h, weights[i]) + biases[i]\n",
    "                h = silu(h)\n",
    "                if i == len(weights) - 2:\n",
    "                    skip = jnp.dot(features, params[\"skip_weights\"])\n",
    "                    h = h + 0.1 * skip\n",
    "            \n",
    "            h = jnp.dot(h, weights[-1]) + biases[-1]\n",
    "            return h.squeeze(-1)\n",
    "        \n",
    "        def energy_from_positions(positions, params):\n",
    "            features, distances = compute_features(positions)\n",
    "            mlp_energy = mlp_forward(features, params)\n",
    "            rbf_contrib = rbf_energy(distances, params)\n",
    "            total_energy = params[\"output_scale\"] * mlp_energy + rbf_contrib + params[\"output_bias\"]\n",
    "            return total_energy\n",
    "        \n",
    "        def force_from_positions(positions, params):\n",
    "            grad_fn = jax.grad(energy_from_positions, argnums=0)\n",
    "            return -grad_fn(positions, params)\n",
    "        \n",
    "        self.energy_fn = energy_from_positions\n",
    "        self.force_fn = force_from_positions\n",
    "        self.vec_energy = jax.vmap(energy_from_positions, (0, None), 0)\n",
    "        self.vec_force = jax.vmap(force_from_positions, (0, None), 0)\n",
    "    \n",
    "    def get_params(self):\n",
    "        return self.params\n",
    "    \n",
    "    def set_params(self, params):\n",
    "        self.params = params\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# TRAINING FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def train_rotationally_equivariant(model, pos_H, E_train, F_train, n_epochs=300, lr=3e-3,\n",
    "                                    wE=1.0, wF_max=5.0, warmup_frac=0.4):\n",
    "    \"\"\"Train Rotationally Equivariant QML with force warmup curriculum.\"\"\"\n",
    "    warmup_epochs = int(n_epochs * warmup_frac)\n",
    "    \n",
    "    def raw_energy(coords, params):\n",
    "        return model.circuit(coords, params)\n",
    "    \n",
    "    vec_raw_energy = jax.vmap(raw_energy, (0, None), 0)\n",
    "    \n",
    "    def vec_force_fn(coords_batch, params):\n",
    "        def single_force(coords):\n",
    "            grad_fn = jax.grad(raw_energy, argnums=0)\n",
    "            return -grad_fn(coords, params)\n",
    "        return jax.vmap(single_force)(coords_batch)\n",
    "    \n",
    "    @jax.jit\n",
    "    def loss_fn(params, coords, E_target, F_target, wF):\n",
    "        E_raw = vec_raw_energy(coords, params)\n",
    "        E_pred = params[\"head_scale\"] * E_raw + params[\"head_bias\"]\n",
    "        L_E = jnp.mean((E_pred - E_target) ** 2)\n",
    "        \n",
    "        F_raw = vec_force_fn(coords, params)\n",
    "        F_pred = params[\"head_scale\"] * F_raw\n",
    "        L_F = jnp.mean((F_pred - F_target) ** 2)\n",
    "        \n",
    "        L_E = jnp.where(jnp.isnan(L_E), 1.0, L_E)\n",
    "        L_F = jnp.where(jnp.isnan(L_F), 1.0, L_F)\n",
    "        \n",
    "        return wE * L_E + wF * L_F, (L_E, L_F)\n",
    "    \n",
    "    opt_init, opt_update, get_params = optimizers.adam(lr)\n",
    "    opt_state = opt_init(model.params)\n",
    "    \n",
    "    history = {\"epoch\": [], \"train_loss\": [], \"test_E_loss\": [], \"test_F_loss\": []}\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch < warmup_epochs:\n",
    "            wF = wF_max * (epoch / warmup_epochs)\n",
    "        else:\n",
    "            wF = wF_max\n",
    "        \n",
    "        (loss, (L_E, L_F)), grads = jax.value_and_grad(loss_fn, has_aux=True)(\n",
    "            get_params(opt_state), pos_H, E_train, F_train, wF\n",
    "        )\n",
    "        \n",
    "        grad_norm = jnp.sqrt(sum(jnp.sum(g**2) for g in jax.tree.leaves(grads)))\n",
    "        if grad_norm > 10.0:\n",
    "            grads = jax.tree.map(lambda g: g * (10.0 / grad_norm), grads)\n",
    "        \n",
    "        opt_state = opt_update(epoch, grads, opt_state)\n",
    "        \n",
    "        if (epoch + 1) % max(1, n_epochs // 20) == 0:\n",
    "            history[\"epoch\"].append(epoch + 1)\n",
    "            history[\"train_loss\"].append(float(loss))\n",
    "            history[\"test_E_loss\"].append(float(L_E))\n",
    "            history[\"test_F_loss\"].append(float(L_F))\n",
    "    \n",
    "    model.set_params(get_params(opt_state))\n",
    "    return history\n",
    "\n",
    "\n",
    "def train_non_equivariant(model, distances, E_train, F_train_flat, n_epochs=300,\n",
    "                          lr=1e-3, lambda_E=2.0, lambda_F=1.0):\n",
    "    \"\"\"Train Non-Equivariant QML with combined loss.\"\"\"\n",
    "    \n",
    "    def energy_fn(dists, params):\n",
    "        raw = model.circuit(dists, params)\n",
    "        return params[\"head_scale\"] * raw + params[\"head_bias\"]\n",
    "    \n",
    "    vec_energy = jax.vmap(energy_fn, (0, None), 0)\n",
    "    \n",
    "    def force_fn(dists, params):\n",
    "        return -jax.grad(energy_fn, argnums=0)(dists, params)\n",
    "    \n",
    "    vec_force = jax.vmap(force_fn, (0, None), 0)\n",
    "    \n",
    "    @jax.jit\n",
    "    def combined_loss(params, dists, E_target, F_target):\n",
    "        E_pred = vec_energy(dists, params)\n",
    "        E_loss = jnp.mean((E_pred - E_target) ** 2)\n",
    "        \n",
    "        F_pred = vec_force(dists, params)\n",
    "        F_loss = jnp.mean((F_pred - F_target) ** 2)\n",
    "        \n",
    "        E_loss = jnp.where(jnp.isnan(E_loss), 1.0, E_loss)\n",
    "        F_loss = jnp.where(jnp.isnan(F_loss), 1.0, F_loss)\n",
    "        \n",
    "        return lambda_E * E_loss + lambda_F * F_loss, (E_loss, F_loss)\n",
    "    \n",
    "    opt_init, opt_update, get_params = optimizers.adam(lr)\n",
    "    opt_state = opt_init(model.params)\n",
    "    \n",
    "    history = {\"epoch\": [], \"train_loss\": [], \"test_E_loss\": [], \"test_F_loss\": []}\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        (loss, (E_loss, F_loss)), grads = jax.value_and_grad(combined_loss, has_aux=True)(\n",
    "            get_params(opt_state), distances, E_train, F_train_flat\n",
    "        )\n",
    "        \n",
    "        grad_norm = jnp.sqrt(sum(jnp.sum(g**2) for g in jax.tree.leaves(grads)))\n",
    "        if grad_norm > 10.0:\n",
    "            grads = jax.tree.map(lambda g: g * (10.0 / grad_norm), grads)\n",
    "        \n",
    "        opt_state = opt_update(epoch, grads, opt_state)\n",
    "        \n",
    "        if (epoch + 1) % max(1, n_epochs // 20) == 0:\n",
    "            history[\"epoch\"].append(epoch + 1)\n",
    "            history[\"train_loss\"].append(float(loss))\n",
    "            history[\"test_E_loss\"].append(float(E_loss))\n",
    "            history[\"test_F_loss\"].append(float(F_loss))\n",
    "    \n",
    "    model.set_params(get_params(opt_state))\n",
    "    return history\n",
    "\n",
    "\n",
    "def train_graph_permutation(model, positions, E_train, F_train_H, n_epochs=300,\n",
    "                            lr=0.01, n_epochs_energy=None, n_epochs_combined=None):\n",
    "    \"\"\"\n",
    "    Train Graph Permutation Equivariant QML with two-phase training.\n",
    "    Uses raw positions and computes forces on H atoms.\n",
    "    \n",
    "    Args:\n",
    "        model: GraphPermutationEquivariantQML instance\n",
    "        positions: (N, 4, 3) - atomic positions [N, H1, H2, H3]\n",
    "        E_train: (N,) - energies\n",
    "        F_train_H: (N, 3, 3) - forces on H atoms only\n",
    "        n_epochs: total epochs (split evenly if phase epochs not specified)\n",
    "        lr: learning rate\n",
    "    \"\"\"\n",
    "    if n_epochs_energy is None:\n",
    "        n_epochs_energy = n_epochs // 2\n",
    "    if n_epochs_combined is None:\n",
    "        n_epochs_combined = n_epochs - n_epochs_energy\n",
    "    \n",
    "    # Phase 1: Energy only\n",
    "    @jax.jit\n",
    "    def energy_loss(params, positions, E_target):\n",
    "        E_pred = model.vec_circuit(positions, params)\n",
    "        return jnp.mean((E_pred - E_target)**2)\n",
    "    \n",
    "    opt_init, opt_update, get_params = optimizers.adam(lr)\n",
    "    opt_state = opt_init(model.params)\n",
    "    \n",
    "    history = {\"epoch\": [], \"train_loss\": [], \"test_E_loss\": [], \"test_F_loss\": []}\n",
    "    \n",
    "    for step in range(n_epochs_energy):\n",
    "        params = get_params(opt_state)\n",
    "        loss, grads = jax.value_and_grad(energy_loss)(params, positions, E_train)\n",
    "        \n",
    "        grad_norm = jnp.sqrt(sum(jnp.sum(jnp.square(g)) for g in jax.tree.leaves(grads)))\n",
    "        if grad_norm > 10.0:\n",
    "            grads = jax.tree.map(lambda g: g * (10.0 / grad_norm), grads)\n",
    "        \n",
    "        opt_state = opt_update(step, grads, opt_state)\n",
    "        \n",
    "        if (step + 1) % max(1, n_epochs // 20) == 0:\n",
    "            history[\"epoch\"].append(step + 1)\n",
    "            history[\"train_loss\"].append(float(loss))\n",
    "            history[\"test_E_loss\"].append(float(loss))\n",
    "            history[\"test_F_loss\"].append(0.0)\n",
    "    \n",
    "    # Phase 2: Combined energy + forces\n",
    "    trained_params = get_params(opt_state)\n",
    "    \n",
    "    def force_single(coords, params):\n",
    "        grad_fn = jax.grad(lambda c, p: model.circuit(c, p), argnums=0)\n",
    "        return -grad_fn(coords, params)\n",
    "    \n",
    "    vec_force = jax.vmap(force_single, in_axes=(0, None), out_axes=0)\n",
    "    \n",
    "    @jax.jit\n",
    "    def combined_loss(params, positions, E_target, F_target):\n",
    "        E_pred = model.vec_circuit(positions, params)\n",
    "        E_loss = jnp.mean((E_pred - E_target)**2)\n",
    "        \n",
    "        F_pred_full = vec_force(positions, params)\n",
    "        F_pred_H = F_pred_full[:, 1:, :]  # Forces on H atoms only\n",
    "        F_loss = jnp.mean((F_pred_H - F_target)**2)\n",
    "        \n",
    "        E_loss = jnp.where(jnp.isnan(E_loss), 1.0, E_loss)\n",
    "        F_loss = jnp.where(jnp.isnan(F_loss), 1.0, F_loss)\n",
    "        \n",
    "        return 2.0 * E_loss + 1.0 * F_loss, (E_loss, F_loss)\n",
    "    \n",
    "    opt_state = opt_init(trained_params)\n",
    "    \n",
    "    for step in range(n_epochs_combined):\n",
    "        params = get_params(opt_state)\n",
    "        (loss, (E_loss, F_loss)), grads = jax.value_and_grad(combined_loss, has_aux=True)(\n",
    "            params, positions, E_train, F_train_H\n",
    "        )\n",
    "        \n",
    "        grad_norm = jnp.sqrt(sum(jnp.sum(jnp.square(g)) for g in jax.tree.leaves(grads)))\n",
    "        if grad_norm > 10.0:\n",
    "            grads = jax.tree.map(lambda g: g * (10.0 / grad_norm), grads)\n",
    "        \n",
    "        opt_state = opt_update(step, grads, opt_state)\n",
    "        \n",
    "        if (step + 1) % max(1, n_epochs // 20) == 0:\n",
    "            history[\"epoch\"].append(n_epochs_energy + step + 1)\n",
    "            history[\"train_loss\"].append(float(loss))\n",
    "            history[\"test_E_loss\"].append(float(E_loss))\n",
    "            history[\"test_F_loss\"].append(float(F_loss))\n",
    "    \n",
    "    model.set_params(get_params(opt_state))\n",
    "    return history\n",
    "\n",
    "\n",
    "def train_classical_equivariant(model, positions, E_train, F_train, n_epochs=300,\n",
    "                                 lr=0.003, lambda_E=1.0, lambda_F=2.0, warmup_frac=0.3):\n",
    "    \"\"\"Train Classical Equivariant NN with two-phase training.\"\"\"\n",
    "    warmup_epochs = int(n_epochs * warmup_frac)\n",
    "    \n",
    "    @jax.jit\n",
    "    def energy_loss(params, positions, E_target):\n",
    "        E_pred = model.vec_energy(positions, params)\n",
    "        E_loss = jnp.mean((E_pred - E_target) ** 2)\n",
    "        return jnp.where(jnp.isnan(E_loss), 1.0, E_loss)\n",
    "    \n",
    "    @jax.jit\n",
    "    def combined_loss(params, positions, E_target, F_target, wF):\n",
    "        E_pred = model.vec_energy(positions, params)\n",
    "        E_loss = jnp.mean((E_pred - E_target) ** 2)\n",
    "        \n",
    "        F_pred_full = model.vec_force(positions, params)\n",
    "        F_pred_H = F_pred_full[:, 1:, :]\n",
    "        F_residual = F_pred_H - F_target\n",
    "        F_loss = jnp.mean(huber(F_residual, delta=0.5))\n",
    "        \n",
    "        E_loss = jnp.where(jnp.isnan(E_loss), 1.0, E_loss)\n",
    "        F_loss = jnp.where(jnp.isnan(F_loss), 1.0, F_loss)\n",
    "        \n",
    "        return lambda_E * E_loss + wF * F_loss, (E_loss, F_loss)\n",
    "    \n",
    "    opt_init, opt_update, get_params = optimizers.adam(lr)\n",
    "    opt_state = opt_init(model.params)\n",
    "    \n",
    "    history = {\"epoch\": [], \"train_loss\": [], \"test_E_loss\": [], \"test_F_loss\": []}\n",
    "    \n",
    "    # Phase 1: Energy warmup\n",
    "    for epoch in range(warmup_epochs):\n",
    "        loss, grads = jax.value_and_grad(energy_loss)(\n",
    "            get_params(opt_state), positions, E_train\n",
    "        )\n",
    "        \n",
    "        grad_norm = jnp.sqrt(sum(jnp.sum(jnp.square(g)) for g in jax.tree.leaves(grads) if g is not None))\n",
    "        if grad_norm > 5.0:\n",
    "            grads = jax.tree.map(lambda g: g * (5.0 / grad_norm) if g is not None else g, grads)\n",
    "        \n",
    "        opt_state = opt_update(epoch, grads, opt_state)\n",
    "        \n",
    "        if (epoch + 1) % max(1, n_epochs // 20) == 0:\n",
    "            history[\"epoch\"].append(epoch + 1)\n",
    "            history[\"train_loss\"].append(float(loss))\n",
    "            history[\"test_E_loss\"].append(float(loss))\n",
    "            history[\"test_F_loss\"].append(0.0)\n",
    "    \n",
    "    # Phase 2: Combined with force ramp\n",
    "    for epoch in range(warmup_epochs, n_epochs):\n",
    "        progress = (epoch - warmup_epochs) / max(1, n_epochs - warmup_epochs)\n",
    "        wF = lambda_F * min(1.0, progress * 2)\n",
    "        \n",
    "        (loss, (E_loss, F_loss)), grads = jax.value_and_grad(combined_loss, has_aux=True)(\n",
    "            get_params(opt_state), positions, E_train, F_train, wF\n",
    "        )\n",
    "        \n",
    "        grad_norm = jnp.sqrt(sum(jnp.sum(jnp.square(g)) for g in jax.tree.leaves(grads) if g is not None))\n",
    "        if grad_norm > 5.0:\n",
    "            grads = jax.tree.map(lambda g: g * (5.0 / grad_norm) if g is not None else g, grads)\n",
    "        \n",
    "        opt_state = opt_update(epoch, grads, opt_state)\n",
    "        \n",
    "        if (epoch + 1) % max(1, n_epochs // 20) == 0:\n",
    "            history[\"epoch\"].append(epoch + 1)\n",
    "            history[\"train_loss\"].append(float(loss))\n",
    "            history[\"test_E_loss\"].append(float(E_loss))\n",
    "            history[\"test_F_loss\"].append(float(F_loss))\n",
    "    \n",
    "    model.set_params(get_params(opt_state))\n",
    "    return history\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# EVALUATION FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def evaluate_fold(E_pred_train, E_pred_test, F_pred_train, F_pred_test,\n",
    "                  E_train_true, E_test_true, F_train_true, F_test_true,\n",
    "                  energy_scaler, force_scaler):\n",
    "    \"\"\"\n",
    "    Evaluate predictions on a single fold with post-correction.\n",
    "    \"\"\"\n",
    "    # Post-correction for energy (quadratic fit on training data)\n",
    "    def corr_E(E, a, b, c):\n",
    "        return a * E**2 + b * E + c\n",
    "    \n",
    "    try:\n",
    "        popt_E, _ = curve_fit(corr_E, E_pred_train, E_train_true, maxfev=5000)\n",
    "        E_pred_test_corr = corr_E(E_pred_test, *popt_E)\n",
    "    except:\n",
    "        E_pred_test_corr = E_pred_test\n",
    "    \n",
    "    # Post-correction for forces (linear fit on training data)\n",
    "    try:\n",
    "        F_pred_train_flat = F_pred_train.flatten().reshape(-1, 1)\n",
    "        F_train_true_flat = F_train_true.flatten().reshape(-1, 1)\n",
    "        lr_model = LinearRegression()\n",
    "        lr_model.fit(F_pred_train_flat, F_train_true_flat)\n",
    "        F_pred_test_corr = lr_model.predict(F_pred_test.flatten().reshape(-1, 1)).reshape(F_pred_test.shape)\n",
    "    except:\n",
    "        F_pred_test_corr = F_pred_test\n",
    "    \n",
    "    # Inverse transform to original units\n",
    "    E_pred_final = energy_scaler.inverse_transform(E_pred_test_corr.reshape(-1, 1)).flatten()\n",
    "    E_true_final = energy_scaler.inverse_transform(E_test_true.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    F_pred_final = force_scaler.inverse_transform(F_pred_test_corr.flatten().reshape(-1, 1)).reshape(F_pred_test.shape)\n",
    "    F_true_final = force_scaler.inverse_transform(F_test_true.flatten().reshape(-1, 1)).reshape(F_test_true.shape)\n",
    "    \n",
    "    # Compute metrics\n",
    "    E_mae = np.mean(np.abs(E_pred_final - E_true_final))\n",
    "    E_rmse = np.sqrt(np.mean((E_pred_final - E_true_final)**2))\n",
    "    ss_res_E = np.sum((E_pred_final - E_true_final)**2)\n",
    "    ss_tot_E = np.sum((E_true_final - E_true_final.mean())**2)\n",
    "    E_r2 = 1 - ss_res_E / ss_tot_E if ss_tot_E > 0 else 0\n",
    "    \n",
    "    F_mae = np.mean(np.abs(F_pred_final - F_true_final))\n",
    "    F_rmse = np.sqrt(np.mean((F_pred_final - F_true_final)**2))\n",
    "    ss_res_F = np.sum((F_pred_final - F_true_final)**2)\n",
    "    ss_tot_F = np.sum((F_true_final - F_true_final.mean())**2)\n",
    "    F_r2 = 1 - ss_res_F / ss_tot_F if ss_tot_F > 0 else 0\n",
    "    \n",
    "    metrics = {\n",
    "        \"E_r2\": float(E_r2),\n",
    "        \"E_mae_Ha\": float(E_mae),\n",
    "        \"E_rmse_Ha\": float(E_rmse),\n",
    "        \"E_mae_eV\": float(E_mae * 27.2114),\n",
    "        \"F_r2\": float(F_r2),\n",
    "        \"F_mae\": float(F_mae),\n",
    "        \"F_rmse\": float(F_rmse),\n",
    "    }\n",
    "    \n",
    "    predictions = {\n",
    "        \"E_pred\": E_pred_final.tolist(),\n",
    "        \"E_true\": E_true_final.tolist(),\n",
    "        \"F_pred\": F_pred_final.tolist(),\n",
    "        \"F_true\": F_true_final.tolist(),\n",
    "    }\n",
    "    \n",
    "    return metrics, predictions\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# K-FOLD CROSS-VALIDATION FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def run_kfold_cv(k_folds, n_epochs, data_dir, output_dir, seed=42):\n",
    "    \"\"\"\n",
    "    Run k-fold cross-validation for all four methods.\n",
    "    \n",
    "    Args:\n",
    "        k_folds: Number of folds for cross-validation\n",
    "        n_epochs: Number of training epochs per fold\n",
    "        data_dir: Directory containing NH₃ dataset\n",
    "        output_dir: Directory to save results\n",
    "        seed: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with all results\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"NH₃ K-FOLD CROSS-VALIDATION COMPARISON\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"K-Folds: {k_folds}\")\n",
    "    print(f\"Epochs per fold: {n_epochs}\")\n",
    "    print(f\"Data directory: {data_dir}\")\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Load data\n",
    "    print(\"Loading NH₃ dataset...\")\n",
    "    positions = np.load(os.path.join(data_dir, \"Positions.npy\"))\n",
    "    energy = np.load(os.path.join(data_dir, \"Energy.npy\"))\n",
    "    forces = np.load(os.path.join(data_dir, \"Forces.npy\"))\n",
    "    \n",
    "    N_samples = len(energy)\n",
    "    print(f\"  Loaded {N_samples} samples\")\n",
    "    print(f\"  Positions shape: {positions.shape}\")\n",
    "    print(f\"  Energy shape: {energy.shape}\")\n",
    "    print(f\"  Forces shape: {forces.shape}\")\n",
    "    \n",
    "    # Prepare scalers (fit on entire dataset for consistency)\n",
    "    energy_scaler = MinMaxScaler((-1, 1))\n",
    "    energy_scaled = energy_scaler.fit_transform(energy.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    forces_H = forces[:, 1:, :]  # H atoms only\n",
    "    force_scaler = MinMaxScaler((-1, 1))\n",
    "    forces_flat = forces_H.reshape(-1, 1)\n",
    "    forces_scaled = force_scaler.fit_transform(forces_flat).reshape(forces_H.shape)\n",
    "    \n",
    "    # Prepare different feature representations\n",
    "    positions_H = positions[:, 1:, :]  # H positions relative to N\n",
    "    \n",
    "    # Non-equivariant: N-H distances\n",
    "    distances_NH = np.array([\n",
    "        [np.linalg.norm(positions[i, 1] - positions[i, 0]),\n",
    "         np.linalg.norm(positions[i, 2] - positions[i, 0]),\n",
    "         np.linalg.norm(positions[i, 3] - positions[i, 0])]\n",
    "        for i in range(N_samples)\n",
    "    ])\n",
    "    \n",
    "    # Graph features: distances + angles\n",
    "    def compute_graph_features(pos):\n",
    "        d1 = np.linalg.norm(pos[1] - pos[0])\n",
    "        d2 = np.linalg.norm(pos[2] - pos[0])\n",
    "        d3 = np.linalg.norm(pos[3] - pos[0])\n",
    "        \n",
    "        def angle_at_center(p1, p2, pc):\n",
    "            v1, v2 = p1 - pc, p2 - pc\n",
    "            cos_a = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2) + 1e-8)\n",
    "            return np.arccos(np.clip(cos_a, -1, 1))\n",
    "        \n",
    "        a1 = angle_at_center(pos[1], pos[2], pos[0])\n",
    "        a2 = angle_at_center(pos[1], pos[3], pos[0])\n",
    "        a3 = angle_at_center(pos[2], pos[3], pos[0])\n",
    "        \n",
    "        return np.array([d1, d2, d3, a1, a2, a3])\n",
    "    \n",
    "    graph_features = np.array([compute_graph_features(positions[i]) for i in range(N_samples)])\n",
    "    \n",
    "    # Force gradients for non-equivariant and graph models\n",
    "    forces_grad_NH = np.zeros((N_samples, 3))\n",
    "    for i in range(N_samples):\n",
    "        for j in range(3):\n",
    "            F_atom = forces_H[i, j]\n",
    "            r_vec = positions[i, j+1] - positions[i, 0]\n",
    "            r_norm = np.linalg.norm(r_vec)\n",
    "            if r_norm > 1e-8:\n",
    "                forces_grad_NH[i, j] = -np.dot(F_atom, r_vec) / r_norm\n",
    "    \n",
    "    forces_grad_NH_scaled = force_scaler.fit_transform(forces_grad_NH.reshape(-1, 1)).reshape(forces_grad_NH.shape)\n",
    "    \n",
    "    # Graph force gradients\n",
    "    forces_grad_graph = np.zeros((N_samples, 6))\n",
    "    forces_grad_graph[:, :3] = forces_grad_NH\n",
    "    forces_grad_graph_scaled = force_scaler.fit_transform(forces_grad_graph.reshape(-1, 1)).reshape(forces_grad_graph.shape)\n",
    "    \n",
    "    # Initialize k-fold\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=seed)\n",
    "    \n",
    "    # Results storage\n",
    "    results = {\n",
    "        \"config\": {\n",
    "            \"k_folds\": k_folds,\n",
    "            \"n_epochs\": n_epochs,\n",
    "            \"n_samples\": N_samples,\n",
    "            \"seed\": seed,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "        },\n",
    "        \"rotationally_equivariant\": {\"folds\": [], \"summary\": {}},\n",
    "        \"non_equivariant\": {\"folds\": [], \"summary\": {}},\n",
    "        \"graph_permutation_equivariant\": {\"folds\": [], \"summary\": {}},\n",
    "        \"classical_equivariant\": {\"folds\": [], \"summary\": {}},\n",
    "    }\n",
    "    \n",
    "    method_names = [\n",
    "        \"rotationally_equivariant\",\n",
    "        \"non_equivariant\", \n",
    "        \"graph_permutation_equivariant\",\n",
    "        \"classical_equivariant\"\n",
    "    ]\n",
    "    \n",
    "    # Run k-fold CV\n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(kf.split(positions)):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"FOLD {fold_idx + 1}/{k_folds}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"  Train samples: {len(train_idx)}, Test samples: {len(test_idx)}\")\n",
    "        \n",
    "        fold_seed = seed + fold_idx * 100\n",
    "        \n",
    "        # Prepare fold data\n",
    "        E_train = energy_scaled[train_idx]\n",
    "        E_test = energy_scaled[test_idx]\n",
    "        F_train = forces_scaled[train_idx]\n",
    "        F_test = forces_scaled[test_idx]\n",
    "        \n",
    "        pos_H_train = jnp.array(positions_H[train_idx])\n",
    "        pos_H_test = jnp.array(positions_H[test_idx])\n",
    "        \n",
    "        pos_full_train = jnp.array(positions[train_idx])\n",
    "        pos_full_test = jnp.array(positions[test_idx])\n",
    "        \n",
    "        dist_train = jnp.array(distances_NH[train_idx])\n",
    "        dist_test = jnp.array(distances_NH[test_idx])\n",
    "        \n",
    "        graph_train = jnp.array(graph_features[train_idx])\n",
    "        graph_test = jnp.array(graph_features[test_idx])\n",
    "        \n",
    "        F_grad_train = jnp.array(forces_grad_NH_scaled[train_idx])\n",
    "        F_grad_test = jnp.array(forces_grad_NH_scaled[test_idx])\n",
    "        \n",
    "        F_grad_graph_train = jnp.array(forces_grad_graph_scaled[train_idx])\n",
    "        F_grad_graph_test = jnp.array(forces_grad_graph_scaled[test_idx])\n",
    "        \n",
    "        # ==================== 1. Rotationally Equivariant ====================\n",
    "        print(f\"\\n  [1/4] Rotationally Equivariant QML...\")\n",
    "        rot_model = RotationallyEquivariantQML(depth=6, rep=2, active_atoms=3, seed=fold_seed)\n",
    "        \n",
    "        rot_history = train_rotationally_equivariant(\n",
    "            rot_model, pos_H_train, jnp.array(E_train), jnp.array(F_train),\n",
    "            n_epochs=n_epochs, lr=3e-3, wE=1.0, wF_max=5.0, warmup_frac=0.4\n",
    "        )\n",
    "        \n",
    "        # Evaluate\n",
    "        E_pred_train = np.array([rot_model.energy(pos_H_train[i], rot_model.params) for i in range(len(train_idx))])\n",
    "        E_pred_test = np.array([rot_model.energy(pos_H_test[i], rot_model.params) for i in range(len(test_idx))])\n",
    "        F_pred_train = np.array([rot_model.force(pos_H_train[i], rot_model.params) for i in range(len(train_idx))])\n",
    "        F_pred_test = np.array([rot_model.force(pos_H_test[i], rot_model.params) for i in range(len(test_idx))])\n",
    "        \n",
    "        rot_metrics, rot_preds = evaluate_fold(\n",
    "            E_pred_train, E_pred_test, F_pred_train, F_pred_test,\n",
    "            E_train, E_test, F_train, F_test,\n",
    "            energy_scaler, force_scaler\n",
    "        )\n",
    "        \n",
    "        results[\"rotationally_equivariant\"][\"folds\"].append({\n",
    "            \"fold\": fold_idx + 1,\n",
    "            \"metrics\": rot_metrics,\n",
    "            \"history\": rot_history,\n",
    "        })\n",
    "        print(f\"    Energy R²: {rot_metrics['E_r2']:.4f}, Force R²: {rot_metrics['F_r2']:.4f}\")\n",
    "        \n",
    "        # ==================== 2. Non-Equivariant ====================\n",
    "        print(f\"\\n  [2/4] Non-Equivariant QML...\")\n",
    "        non_eq_model = NonEquivariantQML(n_qubits=6, depth=4, seed=fold_seed)\n",
    "        \n",
    "        non_eq_history = train_non_equivariant(\n",
    "            non_eq_model, dist_train, jnp.array(E_train), F_grad_train,\n",
    "            n_epochs=n_epochs, lr=1e-3, lambda_E=2.0, lambda_F=1.0\n",
    "        )\n",
    "        \n",
    "        # Evaluate\n",
    "        def non_eq_energy(dists, params):\n",
    "            raw = non_eq_model.circuit(dists, params)\n",
    "            return params[\"head_scale\"] * raw + params[\"head_bias\"]\n",
    "        \n",
    "        def non_eq_force(dists, params):\n",
    "            return -jax.grad(non_eq_energy, argnums=0)(dists, params)\n",
    "        \n",
    "        E_pred_train_ne = np.array([non_eq_energy(dist_train[i], non_eq_model.params) for i in range(len(train_idx))])\n",
    "        E_pred_test_ne = np.array([non_eq_energy(dist_test[i], non_eq_model.params) for i in range(len(test_idx))])\n",
    "        F_pred_train_ne = np.array([non_eq_force(dist_train[i], non_eq_model.params) for i in range(len(train_idx))])\n",
    "        F_pred_test_ne = np.array([non_eq_force(dist_test[i], non_eq_model.params) for i in range(len(test_idx))])\n",
    "        \n",
    "        # Expand forces to match shape\n",
    "        F_pred_train_ne_full = np.zeros((len(train_idx), 3, 3))\n",
    "        F_pred_test_ne_full = np.zeros((len(test_idx), 3, 3))\n",
    "        for i in range(len(train_idx)):\n",
    "            for j in range(3):\n",
    "                r_vec = positions[train_idx[i], j+1] - positions[train_idx[i], 0]\n",
    "                r_norm = np.linalg.norm(r_vec)\n",
    "                if r_norm > 1e-8:\n",
    "                    F_pred_train_ne_full[i, j] = -F_pred_train_ne[i, j] * r_vec / r_norm\n",
    "        for i in range(len(test_idx)):\n",
    "            for j in range(3):\n",
    "                r_vec = positions[test_idx[i], j+1] - positions[test_idx[i], 0]\n",
    "                r_norm = np.linalg.norm(r_vec)\n",
    "                if r_norm > 1e-8:\n",
    "                    F_pred_test_ne_full[i, j] = -F_pred_test_ne[i, j] * r_vec / r_norm\n",
    "        \n",
    "        non_eq_metrics, non_eq_preds = evaluate_fold(\n",
    "            E_pred_train_ne, E_pred_test_ne, F_pred_train_ne_full, F_pred_test_ne_full,\n",
    "            E_train, E_test, F_train, F_test,\n",
    "            energy_scaler, force_scaler\n",
    "        )\n",
    "        \n",
    "        results[\"non_equivariant\"][\"folds\"].append({\n",
    "            \"fold\": fold_idx + 1,\n",
    "            \"metrics\": non_eq_metrics,\n",
    "            \"history\": non_eq_history,\n",
    "        })\n",
    "        print(f\"    Energy R²: {non_eq_metrics['E_r2']:.4f}, Force R²: {non_eq_metrics['F_r2']:.4f}\")\n",
    "        \n",
    "        # ==================== 3. Graph Permutation Equivariant ====================\n",
    "        print(f\"\\n  [3/4] Graph Permutation Equivariant QML...\")\n",
    "        graph_model = GraphPermutationEquivariantQML(n_qubits=6, depth=4, seed=fold_seed)\n",
    "        \n",
    "        # Use raw positions for training (forces on H atoms only)\n",
    "        F_train_H = F_train  # Already (N, 3, 3) for H atoms\n",
    "        \n",
    "        graph_history = train_graph_permutation(\n",
    "            graph_model, pos_full_train, jnp.array(E_train), jnp.array(F_train_H),\n",
    "            n_epochs=n_epochs, lr=0.01\n",
    "        )\n",
    "        \n",
    "        # Evaluate using raw positions\n",
    "        def graph_force_single(coords, params):\n",
    "            return -jax.grad(lambda c, p: graph_model.circuit(c, p), argnums=0)(coords, params)\n",
    "        \n",
    "        vec_force_graph = jax.vmap(graph_force_single, in_axes=(0, None), out_axes=0)\n",
    "        \n",
    "        E_pred_train_g = np.array(graph_model.vec_circuit(pos_full_train, graph_model.params))\n",
    "        E_pred_test_g = np.array(graph_model.vec_circuit(pos_full_test, graph_model.params))\n",
    "        \n",
    "        # Forces directly from positions (H atoms only)\n",
    "        F_pred_train_g_all = np.array(vec_force_graph(pos_full_train, graph_model.params))\n",
    "        F_pred_test_g_all = np.array(vec_force_graph(pos_full_test, graph_model.params))\n",
    "        F_pred_train_g_full = F_pred_train_g_all[:, 1:, :]  # H atoms only\n",
    "        F_pred_test_g_full = F_pred_test_g_all[:, 1:, :]\n",
    "        \n",
    "        graph_metrics, graph_preds = evaluate_fold(\n",
    "            E_pred_train_g, E_pred_test_g, F_pred_train_g_full, F_pred_test_g_full,\n",
    "            E_train, E_test, F_train, F_test,\n",
    "            energy_scaler, force_scaler\n",
    "        )\n",
    "        \n",
    "        results[\"graph_permutation_equivariant\"][\"folds\"].append({\n",
    "            \"fold\": fold_idx + 1,\n",
    "            \"metrics\": graph_metrics,\n",
    "            \"history\": graph_history,\n",
    "        })\n",
    "        print(f\"    Energy R²: {graph_metrics['E_r2']:.4f}, Force R²: {graph_metrics['F_r2']:.4f}\")\n",
    "        \n",
    "        # ==================== 4. Classical Equivariant ====================\n",
    "        print(f\"\\n  [4/4] Classical Rotationally Equivariant NN...\")\n",
    "        classical_model = ClassicalRotationallyEquivariantNN(hidden_dims=[128, 128, 64], seed=fold_seed)\n",
    "        \n",
    "        classical_history = train_classical_equivariant(\n",
    "            classical_model, pos_full_train, jnp.array(E_train), jnp.array(F_train),\n",
    "            n_epochs=n_epochs, lr=0.003, lambda_E=1.0, lambda_F=2.0, warmup_frac=0.3\n",
    "        )\n",
    "        \n",
    "        # Evaluate\n",
    "        E_pred_train_c = np.array(classical_model.vec_energy(pos_full_train, classical_model.params))\n",
    "        E_pred_test_c = np.array(classical_model.vec_energy(pos_full_test, classical_model.params))\n",
    "        F_pred_train_c = np.array(classical_model.vec_force(pos_full_train, classical_model.params))[:, 1:, :]\n",
    "        F_pred_test_c = np.array(classical_model.vec_force(pos_full_test, classical_model.params))[:, 1:, :]\n",
    "        \n",
    "        classical_metrics, classical_preds = evaluate_fold(\n",
    "            E_pred_train_c, E_pred_test_c, F_pred_train_c, F_pred_test_c,\n",
    "            E_train, E_test, F_train, F_test,\n",
    "            energy_scaler, force_scaler\n",
    "        )\n",
    "        \n",
    "        results[\"classical_equivariant\"][\"folds\"].append({\n",
    "            \"fold\": fold_idx + 1,\n",
    "            \"metrics\": classical_metrics,\n",
    "            \"history\": classical_history,\n",
    "        })\n",
    "        print(f\"    Energy R²: {classical_metrics['E_r2']:.4f}, Force R²: {classical_metrics['F_r2']:.4f}\")\n",
    "    \n",
    "    # Compute summary statistics across folds\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"COMPUTING SUMMARY STATISTICS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    metrics_keys = [\"E_r2\", \"E_mae_Ha\", \"E_rmse_Ha\", \"F_r2\", \"F_mae\", \"F_rmse\"]\n",
    "    \n",
    "    for method in method_names:\n",
    "        folds_data = results[method][\"folds\"]\n",
    "        summary = {}\n",
    "        \n",
    "        for metric in metrics_keys:\n",
    "            values = [fold[\"metrics\"][metric] for fold in folds_data]\n",
    "            summary[metric] = {\n",
    "                \"mean\": float(np.mean(values)),\n",
    "                \"std\": float(np.std(values)),\n",
    "                \"min\": float(np.min(values)),\n",
    "                \"max\": float(np.max(values)),\n",
    "                \"values\": values,\n",
    "            }\n",
    "        \n",
    "        # Compute coefficient of variation (CV) for generalizability measure\n",
    "        for metric in [\"E_r2\", \"F_r2\"]:\n",
    "            values = summary[metric][\"values\"]\n",
    "            mean = summary[metric][\"mean\"]\n",
    "            if mean != 0:\n",
    "                summary[f\"{metric}_cv\"] = float(np.std(values) / abs(mean))  # CV\n",
    "            else:\n",
    "                summary[f\"{metric}_cv\"] = float('inf')\n",
    "        \n",
    "        results[method][\"summary\"] = summary\n",
    "    \n",
    "    # Print summary table\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"K-FOLD CROSS-VALIDATION SUMMARY\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    print(f\"\\n{'Method':<35} {'E_R² (mean±std)':<20} {'F_R² (mean±std)':<20} {'E_CV':<10} {'F_CV':<10}\")\n",
    "    print(\"-\"*100)\n",
    "    \n",
    "    method_labels = {\n",
    "        \"rotationally_equivariant\": \"Rot. Equiv. QML\",\n",
    "        \"non_equivariant\": \"Non-Equiv. QML\",\n",
    "        \"graph_permutation_equivariant\": \"Graph Perm. QML\",\n",
    "        \"classical_equivariant\": \"Classical Equiv. NN\",\n",
    "    }\n",
    "    \n",
    "    for method in method_names:\n",
    "        summary = results[method][\"summary\"]\n",
    "        e_r2 = summary[\"E_r2\"]\n",
    "        f_r2 = summary[\"F_r2\"]\n",
    "        e_cv = summary.get(\"E_r2_cv\", 0)\n",
    "        f_cv = summary.get(\"F_r2_cv\", 0)\n",
    "        \n",
    "        print(f\"{method_labels[method]:<35} \"\n",
    "              f\"{e_r2['mean']:.4f}±{e_r2['std']:.4f}       \"\n",
    "              f\"{f_r2['mean']:.4f}±{f_r2['std']:.4f}       \"\n",
    "              f\"{e_cv:.4f}     {f_cv:.4f}\")\n",
    "    \n",
    "    print(\"=\"*100)\n",
    "    print(\"CV = Coefficient of Variation (lower = more consistent across folds)\")\n",
    "    \n",
    "    # Save results\n",
    "    results_path = os.path.join(output_dir, \"kfold_results.json\")\n",
    "    with open(results_path, \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    print(f\"\\nResults saved to: {results_path}\")\n",
    "    \n",
    "    # Save NPZ for easy loading\n",
    "    npz_data = {}\n",
    "    for method in method_names:\n",
    "        for metric in metrics_keys:\n",
    "            key = f\"{method}_{metric}\"\n",
    "            npz_data[key] = np.array(results[method][\"summary\"][metric][\"values\"])\n",
    "    \n",
    "    npz_path = os.path.join(output_dir, \"kfold_metrics.npz\")\n",
    "    np.savez(npz_path, **npz_data)\n",
    "    print(f\"Metrics saved to: {npz_path}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def main(k_folds=5, n_epochs=300, output_dir=\"kfold_results\", \n",
    "         data_dir=\"eqnn_force_field_data_nh3_new\", seed=42):\n",
    "    \"\"\"\n",
    "    Main function for k-fold cross-validation.\n",
    "    \n",
    "    Args:\n",
    "        k_folds: Number of folds (default: 5)\n",
    "        n_epochs: Epochs per fold (default: 300)\n",
    "        output_dir: Output directory\n",
    "        data_dir: Data directory\n",
    "        seed: Random seed\n",
    "    \n",
    "    Returns:\n",
    "        Results dictionary\n",
    "    \"\"\"\n",
    "    return run_kfold_cv(k_folds, n_epochs, data_dir, output_dir, seed)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    \n",
    "    if 'ipykernel' in sys.modules:\n",
    "        print(\"Running in Jupyter. Call main() with parameters:\")\n",
    "        print(\"  results = main(k_folds=5, n_epochs=300, output_dir='kfold_results')\")\n",
    "    else:\n",
    "        parser = argparse.ArgumentParser(description=\"NH₃ K-Fold Cross-Validation Comparison\")\n",
    "        parser.add_argument(\"--k_folds\", type=int, default=5, help=\"Number of folds\")\n",
    "        parser.add_argument(\"--n_epochs\", type=int, default=300, help=\"Epochs per fold\")\n",
    "        parser.add_argument(\"--output_dir\", type=str, default=\"kfold_results\", help=\"Output directory\")\n",
    "        parser.add_argument(\"--data_dir\", type=str, default=\"eqnn_force_field_data_nh3_new\", help=\"Data directory\")\n",
    "        parser.add_argument(\"--seed\", type=int, default=42, help=\"Random seed\")\n",
    "        \n",
    "        args = parser.parse_args()\n",
    "        \n",
    "        main(\n",
    "            k_folds=args.k_folds,\n",
    "            n_epochs=args.n_epochs,\n",
    "            output_dir=args.output_dir,\n",
    "            data_dir=args.data_dir,\n",
    "            seed=args.seed\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daa5dba6-2100-4147-9acc-a04a32a4fad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:2025-12-03 04:08:46,125:jax._src.xla_bridge:850: An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "NH₃ K-FOLD CROSS-VALIDATION COMPARISON\n",
      "================================================================================\n",
      "K-Folds: 5\n",
      "Epochs per fold: 300\n",
      "Data directory: eqnn_force_field_data_nh3_new\n",
      "Output directory: kfold_results\n",
      "================================================================================\n",
      "\n",
      "Loading NH₃ dataset...\n",
      "  Loaded 2400 samples\n",
      "  Positions shape: (2400, 4, 3)\n",
      "  Energy shape: (2400,)\n",
      "  Forces shape: (2400, 4, 3)\n",
      "\n",
      "============================================================\n",
      "FOLD 1/5\n",
      "============================================================\n",
      "  Train samples: 1920, Test samples: 480\n",
      "\n",
      "  [1/4] Rotationally Equivariant QML...\n",
      "    Energy R²: 0.9728, Force R²: 0.8772\n",
      "\n",
      "  [2/4] Non-Equivariant QML...\n",
      "    Energy R²: 0.0600, Force R²: 0.2080\n",
      "\n",
      "  [3/4] Graph Permutation Equivariant QML...\n",
      "    Energy R²: 0.9363, Force R²: 0.9384\n",
      "\n",
      "  [4/4] Classical Rotationally Equivariant NN...\n",
      "    Energy R²: 0.9764, Force R²: 0.9522\n",
      "\n",
      "============================================================\n",
      "FOLD 2/5\n",
      "============================================================\n",
      "  Train samples: 1920, Test samples: 480\n",
      "\n",
      "  [1/4] Rotationally Equivariant QML...\n",
      "    Energy R²: 0.9609, Force R²: 0.8121\n",
      "\n",
      "  [2/4] Non-Equivariant QML...\n",
      "    Energy R²: 0.1453, Force R²: 0.1908\n",
      "\n",
      "  [3/4] Graph Permutation Equivariant QML...\n",
      "    Energy R²: 0.9316, Force R²: 0.9420\n",
      "\n",
      "  [4/4] Classical Rotationally Equivariant NN...\n",
      "    Energy R²: 0.9734, Force R²: 0.9575\n",
      "\n",
      "============================================================\n",
      "FOLD 3/5\n",
      "============================================================\n",
      "  Train samples: 1920, Test samples: 480\n",
      "\n",
      "  [1/4] Rotationally Equivariant QML...\n",
      "    Energy R²: 0.8762, Force R²: 0.8363\n",
      "\n",
      "  [2/4] Non-Equivariant QML...\n",
      "    Energy R²: 0.1524, Force R²: 0.1411\n",
      "\n",
      "  [3/4] Graph Permutation Equivariant QML...\n",
      "    Energy R²: 0.9319, Force R²: 0.9345\n",
      "\n",
      "  [4/4] Classical Rotationally Equivariant NN...\n",
      "    Energy R²: 0.9695, Force R²: 0.9472\n",
      "\n",
      "============================================================\n",
      "FOLD 4/5\n",
      "============================================================\n",
      "  Train samples: 1920, Test samples: 480\n",
      "\n",
      "  [1/4] Rotationally Equivariant QML...\n",
      "    Energy R²: 0.9300, Force R²: 0.9059\n",
      "\n",
      "  [2/4] Non-Equivariant QML...\n",
      "    Energy R²: 0.1786, Force R²: 0.1925\n",
      "\n",
      "  [3/4] Graph Permutation Equivariant QML...\n",
      "    Energy R²: 0.9372, Force R²: 0.9553\n",
      "\n",
      "  [4/4] Classical Rotationally Equivariant NN...\n",
      "    Energy R²: 0.9713, Force R²: 0.9551\n",
      "\n",
      "============================================================\n",
      "FOLD 5/5\n",
      "============================================================\n",
      "  Train samples: 1920, Test samples: 480\n",
      "\n",
      "  [1/4] Rotationally Equivariant QML...\n",
      "    Energy R²: 0.7700, Force R²: 0.8162\n",
      "\n",
      "  [2/4] Non-Equivariant QML...\n",
      "    Energy R²: 0.1373, Force R²: 0.1602\n",
      "\n",
      "  [3/4] Graph Permutation Equivariant QML...\n",
      "    Energy R²: 0.9130, Force R²: 0.9346\n",
      "\n",
      "  [4/4] Classical Rotationally Equivariant NN...\n",
      "    Energy R²: 0.9600, Force R²: 0.9432\n",
      "\n",
      "================================================================================\n",
      "COMPUTING SUMMARY STATISTICS\n",
      "================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "K-FOLD CROSS-VALIDATION SUMMARY\n",
      "====================================================================================================\n",
      "\n",
      "Method                              E_R² (mean±std)      F_R² (mean±std)      E_CV       F_CV      \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Rot. Equiv. QML                     0.9020±0.0740       0.8495±0.0364       0.0820     0.0429\n",
      "Non-Equiv. QML                      0.1347±0.0399       0.1785±0.0243       0.2959     0.1361\n",
      "Graph Perm. QML                     0.9300±0.0088       0.9410±0.0077       0.0095     0.0082\n",
      "Classical Equiv. NN                 0.9701±0.0056       0.9510±0.0052       0.0057     0.0055\n",
      "====================================================================================================\n",
      "CV = Coefficient of Variation (lower = more consistent across folds)\n",
      "\n",
      "Results saved to: kfold_results/kfold_results.json\n",
      "Metrics saved to: kfold_results/kfold_metrics.npz\n"
     ]
    }
   ],
   "source": [
    "results = main(k_folds=5, n_epochs=300, output_dir='kfold_results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaf483e-6766-408a-918d-79638b183d8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
